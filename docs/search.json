[
  {
    "objectID": "resources.html#r-programming",
    "href": "resources.html#r-programming",
    "title": "Resources",
    "section": "R Programming",
    "text": "R Programming\n\nTidyverse style guide\nTidy design principles\nr-spatial\nSpatial Statistics for Data Science: Theory and Practice with R. Paula Moraga, 2023.\nA Guide to Reproducible Code in Ecology and Evolution. British Ecological Society, 2017"
  },
  {
    "objectID": "resources.html#coordinate-systems-and-projections",
    "href": "resources.html#coordinate-systems-and-projections",
    "title": "Resources",
    "section": "Coordinate Systems and Projections",
    "text": "Coordinate Systems and Projections\n\nGeographic vs projected coordinate systems (Esri)\nCoordinate Reference System and Spatial Projection (Earth Lab, CU Boulder)\nGuide to map projections (Axis Maps)\nChoosing a projection (Penn State)\nA list of Coordinate Reference Systems (Spatial Reference)\nCoordinate Systems Worldwide (MapTiler Team)\nExploring Projection Distortions (Leventhal Map and Education Center)"
  },
  {
    "objectID": "resources.html#mapmaking",
    "href": "resources.html#mapmaking",
    "title": "Resources",
    "section": "Mapmaking",
    "text": "Mapmaking\n\nPlotting Geospatial Data in R: comparing across packages\nColor palette finder with paletteer\nColorBrewer 2.0\nIntro to Color Visualization (NASA)\nGIS icons\nGuide to common errors in map production (Journal of Maps)"
  },
  {
    "objectID": "course-materials/week5.html#class-materials",
    "href": "course-materials/week5.html#class-materials",
    "title": "Intro to remote sensing & electromagnetism",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nFundamentals of electromagnetic radiation\n\n\n Lab materials to download\nVector and raster interactions\n\n\n Lab solution\nVector and raster interactions\n\n\n Discussion\nPractice raster operations with vectors"
  },
  {
    "objectID": "course-materials/week5.html#assignment-reminders",
    "href": "course-materials/week5.html#assignment-reminders",
    "title": "Intro to remote sensing & electromagnetism",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nSR\nSelf Reflection #2\n10/28/2025\n11/01/2025\n\n\nCheck-In\nCheck-In (week 5)\n10/30/2025\n10/30/2025\n\n\nHW\nHomework Assignment #3\n10/21/2025\n11/08/2025"
  },
  {
    "objectID": "course-materials/week5.html#background-reading",
    "href": "course-materials/week5.html#background-reading",
    "title": "Intro to remote sensing & electromagnetism",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2"
  },
  {
    "objectID": "course-materials/week6.html#class-materials",
    "href": "course-materials/week6.html#class-materials",
    "title": "Remote sensing data collection",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nResolutions: spatial, temporal, spectral, radiometric\n\n\n Lab\nVector and raster interactions cont.; False color images\n\n\n Lab materials to download\nVector and raster interactions cont.; False color images\n\n\n Discussion\nWorking with false color images"
  },
  {
    "objectID": "course-materials/week6.html#assignment-reminders",
    "href": "course-materials/week6.html#assignment-reminders",
    "title": "Remote sensing data collection",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 6)\n11/06/2025\n11/06/2025\n\n\nHW\nHomework Assignment #3\n10/21/2025\n11/10/2025"
  },
  {
    "objectID": "course-materials/week6.html#background-reading",
    "href": "course-materials/week6.html#background-reading",
    "title": "Remote sensing data collection",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2\nHow to Interpret a False-Color Satellite Image"
  },
  {
    "objectID": "course-materials/week6.html#additional-resources",
    "href": "course-materials/week6.html#additional-resources",
    "title": "Remote sensing data collection",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nHow raster functions map to stars functions\nSpatiotemporal raster data handling with stars\nA comparison of terra and stars packages"
  },
  {
    "objectID": "course-materials/week3.html#class-materials",
    "href": "course-materials/week3.html#class-materials",
    "title": "Spatial and geometry operations with vector data",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nIntro to basic spatial data and geometry operations with vector data\n\n\n Lab materials to download\nSpatial joins, topological relationships, and distance relationships\n\n\n Lab solution\nSpatial joins, topological relationships, and distance relationships\n\n\n Discussion\nPractice vector operations"
  },
  {
    "objectID": "course-materials/week3.html#assignment-reminders",
    "href": "course-materials/week3.html#assignment-reminders",
    "title": "Spatial and geometry operations with vector data",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 3)\n10/16/2025\n10/16/2025\n\n\nHW\nHomework Assignment #2\n10/07/2025\n10/18/2025"
  },
  {
    "objectID": "course-materials/week3.html#background-reading",
    "href": "course-materials/week3.html#background-reading",
    "title": "Spatial and geometry operations with vector data",
    "section": " Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 4\nGeocomputation with R, Chapter 5\nGIS Fundamentals, Chapter 9 Part 1\nDouglas–Peucker Algorithm (Cartography Playground)\nLine Simplification with Visvalingam–Whyatt Algorithm (Mike Bostok)"
  },
  {
    "objectID": "course-materials/week3.html#technical-background",
    "href": "course-materials/week3.html#technical-background",
    "title": "Spatial and geometry operations with vector data",
    "section": " Technical Background",
    "text": "Technical Background\n\nsf overview\nsf cheatsheet\nIntroduction to sf and stars"
  },
  {
    "objectID": "course-materials/template.html",
    "href": "course-materials/template.html",
    "title": "template {add topic}",
    "section": "",
    "text": "Session\nMaterials\n\n\n\n\n Lecture\ndescription\n\n\n Lab\ndescription\n\n\n Discussion\ndescription"
  },
  {
    "objectID": "course-materials/template.html#class-materials",
    "href": "course-materials/template.html#class-materials",
    "title": "template {add topic}",
    "section": "",
    "text": "Session\nMaterials\n\n\n\n\n Lecture\ndescription\n\n\n Lab\ndescription\n\n\n Discussion\ndescription"
  },
  {
    "objectID": "course-materials/template.html#assignment-reminders",
    "href": "course-materials/template.html#assignment-reminders",
    "title": "template {add topic}",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (Week 1)\nMon 01/08/2024\nMon 01/08/2024, 11:55pm PT\n\n\nSR\nSelf reflection (SR #1)\nMon 01/08/2024\nSat 01/13/2024, 11:59pm PT\n\n\nHW\nHomework Assignment #1\nMon 01/08/2024\nSat 01/20/2024, 11:59pm PT"
  },
  {
    "objectID": "course-materials/template.html#background-reading",
    "href": "course-materials/template.html#background-reading",
    "title": "template {add topic}",
    "section": " Background Reading",
    "text": "Background Reading"
  },
  {
    "objectID": "course-materials/template.html#additional-resources",
    "href": "course-materials/template.html#additional-resources",
    "title": "template {add topic}",
    "section": " Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/week1.html#class-materials",
    "href": "course-materials/week1.html#class-materials",
    "title": "Intro to EDS 223 and map making",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nWhy spatial?, and fundamentals of good maps\n\n\n Lab materials to download\nMaking maps in R with tmap\n\n\n Lab solution\nMaking maps in R with tmap\n\n\n Discussion\nMaking maps with tmap and beyond"
  },
  {
    "objectID": "course-materials/week1.html#assignment-reminders",
    "href": "course-materials/week1.html#assignment-reminders",
    "title": "Intro to EDS 223 and map making",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 1)\n10/02/2025\n10/02/2025\n\n\nSR\nPre-course Self Reflection (SR#1)\n09/30/2025\n10/04/2025\n\n\nHW\nHomework Assignment #1\n09/30/2025\n10/06/2025"
  },
  {
    "objectID": "course-materials/week1.html#background-reading",
    "href": "course-materials/week1.html#background-reading",
    "title": "Intro to EDS 223 and map making",
    "section": " Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 9\nGIS Fundamentals, Chapter 2\nGIS Fundamentals, Chapter 3\nA Gentle Introduction to GIS, Chapter 8\nGeographic vs projected coordinate systems (Esri)"
  },
  {
    "objectID": "course-materials/week1.html#technical-background",
    "href": "course-materials/week1.html#technical-background",
    "title": "Intro to EDS 223 and map making",
    "section": " Technical Background",
    "text": "Technical Background\n\ntmap: thematic maps in R documentation\ntmap overview\nCreating thematic maps in R"
  },
  {
    "objectID": "course-materials/week1.html#additional-resources",
    "href": "course-materials/week1.html#additional-resources",
    "title": "Intro to EDS 223 and map making",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nJohn Snow: A Legacy of Disease Detectives (CDC)\nHow the north ended up on top of the map (Al Jazeera)\nWhy maps point North on top? (Geospatial World)\nWhy all world maps are wrong (Vox)"
  },
  {
    "objectID": "course-materials/resources/good-bad-example/good_example.html",
    "href": "course-materials/resources/good-bad-example/good_example.html",
    "title": "Professional Output Example",
    "section": "",
    "text": "Professional documents should be easy to follow and get the point across to the reader in a concise manner.\nHelpful components include:\n\ndocument header with title, name, and date\n\nideally, the date reflects the day the document is most recently rendered\n\nall packages are loaded together at the top of the document\ninclude code comments when appropriate\nclean code indentation for lists, parameters, functions within functions, etc.\ndata visualizations such as maps and plots contain all necessary components such as a legend, axes labels, units, and appropriate palette (divergent, colorblind friendly, etc.)\nfolding code or sourcing separate scripts when appropriate to direct reader’s attention to the important output or to condense code\nhiding unnecessary output such as warning messages or loaded packages\nsuccinct documentation between steps, such as section headers, descriptions for an analysis step, and map/plot interpretation\ncomplete and detailed data citations\nfunction definitions should include warnings and errors that ensure the intended operations are executed\n\n\n\nElephants are intelligent and gregarious animals that tend to travel as a herd and seek out resources such as shrubs and water. Elephants in Krugar National Park were collared with GPS tracking devices from August 2007 through August 2009 (Slotow et al. 2019).\nObjective: Use GPS point locations of individual elephants to visualize their movement throughout Krugar National Park in South Africa between 2007 and 2009.\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nShow the code\nlibrary(here)\n\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nShow the code\nlibrary(sf)\n\n\nWarning: package 'sf' was built under R version 4.3.3\n\n\nShow the code\nlibrary(tmap)\n\n\nWarning: package 'tmap' was built under R version 4.3.3\n\n\nShow the code\nlibrary(rnaturalearth)\nlibrary(osmdata)\n\n# source external script that defines custom functions\nsource(list.files(path = here(\"course-materials\"),\n                  pattern = \"functions.R\",\n                  recursive = TRUE,\n                  full.names = TRUE))\n\n\n\n\n\n\nImport the elephant movement data and configure its spatial components\n\n\nmetadata_df &lt;- load_csv(filename = \"elephants_metadata.csv\",\n                        directory = \"course-materials\",\n                        communicate = FALSE)\n\n\nAttaching package: 'tools'\n\n\nThe following object is masked from 'package:tmap':\n\n    toTitleCase\n\nelephants_df &lt;- load_csv(filename = \"elephants.csv\",\n                         directory = \"course-materials\",\n                         communicate = FALSE)\n\n# convert the elephant observations to sf objects\nelephants &lt;- to_spatial_points(data = elephants_df,\n                               latitude_col = \"location-lat\",\n                               longitude_col = \"location-long\")\n\n\n\n\n\nImport the spatial boundaries for Kruger National Park to serve as a basemap for the elephant tracks\n\n\n# set bbox to approximate boundaries of Kruger NP\nkruger &lt;- osmdata::opq(bbox = c(16, -35, 33, -22)) %&gt;%\n          add_osm_feature(key = \"boundary\", \n                          value = \"protected_area\") %&gt;%\n          add_osm_feature(key = \"name\", \n                          value = \"Kruger National Park\") %&gt;%\n          osmdata_sf()\n\n# subset imported list of elements to just the geodataframe\nkruger_bounds &lt;- kruger$osm_multipolygons\n\n\nProduce a map of elephant observations to visualize the individual movement of elephants\n\n\n\nShow the code\ntitle = \"Elephant Observations\\nKruger National Park\\n2007-2009\"\n\n# basemap: Kruger National Park geometry\ntm_shape(kruger_bounds) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"black\") +\n  # overlay all elephant GPS locations \n  tm_shape(elephants) +\n  tm_dots(col = \"individual-local-identifier\",\n          palette = \"Set3\",\n          # small dots to distinguish between individuals\n          size = 0.001,\n          border.col = \"black\",\n          title = \"Individual\") +\n  tm_layout(bg.color = \"darkgrey\",\n            legend.bg.color = \"white\",\n            title = title,\n            frame = TRUE,\n            title.position = c(\"center\", \"top\"),\n            title.size = 1.2,\n            title.fontface = \"bold\",\n            legend.frame = TRUE,\n            legend.outside = TRUE,\n            legend.position = c(0.08, 0.3),\n            legend.title.size = 0.8,\n            legend.text.size = 0.6) +\n  # add compass with only North arrow\n  tm_compass(position = c(0.0, 0.01),\n             show.labels = 1)\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_dots()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'palette' (rename to 'values') to fill.scale =\ntm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[tm_dots()] Argument `title` unknown.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Set3\" is named\n\"brewer.set3\"\nMultiple palettes called \"set3\" found: \"brewer.set3\", \"hcl.set3\". The first one, \"brewer.set3\", is returned.\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\nElephants in Kruger National Park fitted with GPS collars are primarily located in the southern part of the park. The clustered tracks imply that this species is highly gregarious. Tracks that expand beyond park boundaries, notably on the western side, imply that the elephants could be seeking resources such as food or water outside of the park. Alternatively, error in the GPS location for these points could explain this perceived movement.\nFuther analysis steps could include:\n\nzooming in further to the realized niche\npartitioning the data temporally\nusing metadata to advise how to subset the data to view fewer elephant locations at a time\noverlaying other vector or raster data layers such as surface water and vegetation to understand if the elephants’ movement is driven by these resources\n\n\n\n\n\n\n\nData\nCitation\nLink\n\n\n\n\nElephant observations\nSlotow R, Thaker M, Vanak AT. 2019. Data from: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water. Movebank Data Repository. https://doi.org/10.5441/001/1.403h24q5\nMoveBank Repository: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water"
  },
  {
    "objectID": "course-materials/resources/good-bad-example/good_example.html#elephant-tracking-in-krugar-national-park",
    "href": "course-materials/resources/good-bad-example/good_example.html#elephant-tracking-in-krugar-national-park",
    "title": "Professional Output Example",
    "section": "",
    "text": "Elephants are intelligent and gregarious animals that tend to travel as a herd and seek out resources such as shrubs and water. Elephants in Krugar National Park were collared with GPS tracking devices from August 2007 through August 2009 (Slotow et al. 2019).\nObjective: Use GPS point locations of individual elephants to visualize their movement throughout Krugar National Park in South Africa between 2007 and 2009.\n\n\n\n\nShow the code\nlibrary(tidyverse)\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nShow the code\nlibrary(here)\n\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nShow the code\nlibrary(sf)\n\n\nWarning: package 'sf' was built under R version 4.3.3\n\n\nShow the code\nlibrary(tmap)\n\n\nWarning: package 'tmap' was built under R version 4.3.3\n\n\nShow the code\nlibrary(rnaturalearth)\nlibrary(osmdata)\n\n# source external script that defines custom functions\nsource(list.files(path = here(\"course-materials\"),\n                  pattern = \"functions.R\",\n                  recursive = TRUE,\n                  full.names = TRUE))\n\n\n\n\n\n\nImport the elephant movement data and configure its spatial components\n\n\nmetadata_df &lt;- load_csv(filename = \"elephants_metadata.csv\",\n                        directory = \"course-materials\",\n                        communicate = FALSE)\n\n\nAttaching package: 'tools'\n\n\nThe following object is masked from 'package:tmap':\n\n    toTitleCase\n\nelephants_df &lt;- load_csv(filename = \"elephants.csv\",\n                         directory = \"course-materials\",\n                         communicate = FALSE)\n\n# convert the elephant observations to sf objects\nelephants &lt;- to_spatial_points(data = elephants_df,\n                               latitude_col = \"location-lat\",\n                               longitude_col = \"location-long\")\n\n\n\n\n\nImport the spatial boundaries for Kruger National Park to serve as a basemap for the elephant tracks\n\n\n# set bbox to approximate boundaries of Kruger NP\nkruger &lt;- osmdata::opq(bbox = c(16, -35, 33, -22)) %&gt;%\n          add_osm_feature(key = \"boundary\", \n                          value = \"protected_area\") %&gt;%\n          add_osm_feature(key = \"name\", \n                          value = \"Kruger National Park\") %&gt;%\n          osmdata_sf()\n\n# subset imported list of elements to just the geodataframe\nkruger_bounds &lt;- kruger$osm_multipolygons\n\n\nProduce a map of elephant observations to visualize the individual movement of elephants\n\n\n\nShow the code\ntitle = \"Elephant Observations\\nKruger National Park\\n2007-2009\"\n\n# basemap: Kruger National Park geometry\ntm_shape(kruger_bounds) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"black\") +\n  # overlay all elephant GPS locations \n  tm_shape(elephants) +\n  tm_dots(col = \"individual-local-identifier\",\n          palette = \"Set3\",\n          # small dots to distinguish between individuals\n          size = 0.001,\n          border.col = \"black\",\n          title = \"Individual\") +\n  tm_layout(bg.color = \"darkgrey\",\n            legend.bg.color = \"white\",\n            title = title,\n            frame = TRUE,\n            title.position = c(\"center\", \"top\"),\n            title.size = 1.2,\n            title.fontface = \"bold\",\n            legend.frame = TRUE,\n            legend.outside = TRUE,\n            legend.position = c(0.08, 0.3),\n            legend.title.size = 0.8,\n            legend.text.size = 0.6) +\n  # add compass with only North arrow\n  tm_compass(position = c(0.0, 0.01),\n             show.labels = 1)\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_dots()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'palette' (rename to 'values') to fill.scale =\ntm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[tm_dots()] Argument `title` unknown.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Set3\" is named\n\"brewer.set3\"\nMultiple palettes called \"set3\" found: \"brewer.set3\", \"hcl.set3\". The first one, \"brewer.set3\", is returned.\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n\nElephants in Kruger National Park fitted with GPS collars are primarily located in the southern part of the park. The clustered tracks imply that this species is highly gregarious. Tracks that expand beyond park boundaries, notably on the western side, imply that the elephants could be seeking resources such as food or water outside of the park. Alternatively, error in the GPS location for these points could explain this perceived movement.\nFuther analysis steps could include:\n\nzooming in further to the realized niche\npartitioning the data temporally\nusing metadata to advise how to subset the data to view fewer elephant locations at a time\noverlaying other vector or raster data layers such as surface water and vegetation to understand if the elephants’ movement is driven by these resources\n\n\n\n\n\n\n\nData\nCitation\nLink\n\n\n\n\nElephant observations\nSlotow R, Thaker M, Vanak AT. 2019. Data from: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water. Movebank Data Repository. https://doi.org/10.5441/001/1.403h24q5\nMoveBank Repository: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water"
  },
  {
    "objectID": "course-materials/week9.html#class-materials",
    "href": "course-materials/week9.html#class-materials",
    "title": "Land Cover Classification",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nImage classifiction: unsupervised and supervised approaches\n\n\n Lab\nLandcover classification with decision trees\n\n\n Lab solution\n\n\n\n Discussion"
  },
  {
    "objectID": "course-materials/week9.html#assignment-reminders",
    "href": "course-materials/week9.html#assignment-reminders",
    "title": "Land Cover Classification",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nHW\nHomework Assignment #4\n11/13/2025\n11/26/2025\n\n\nPR\nPortfolio Repository\n11/13/2025\n12/06/2025"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(tmaptools)\n# Set directory for folder\npre_fire_dir &lt;- here::here(\"data\", \"LC80340322016189-SC20170128091153\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npre_fire_bands &lt;- list.files(pre_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npre_fire_rast &lt;- rast(pre_fire_bands)\n\n# Read mask raster\npre_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016189LGN00_cfmask_crop.tif\"))\n# Set directory for folder\npost_fire_dir &lt;- here::here(\"data\", \"LC80340322016205-SC20170127160728\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npost_fire_bands &lt;- list.files(post_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npost_fire_rast &lt;- rast(post_fire_bands)\n\n# Read mask raster\npost_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016205LGN00_cfmask_crop.tif\"))\nnbr_fun &lt;- function(nir, swir2){\n    (nir - swir2)/(nir + swir2)\n}"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#rename-bands",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#rename-bands",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Rename Bands",
    "text": "Rename Bands\n\nbands &lt;- c(\"Aerosol\", \"Blue\", \"Green\", \"Red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(pre_fire_rast) &lt;- bands\nnames(post_fire_rast) &lt;- bands"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#mask-clouds-and-shadows",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#mask-clouds-and-shadows",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Mask Clouds and Shadows",
    "text": "Mask Clouds and Shadows\n\n# Set all cells with values greater than 0 to NA\npre_mask[pre_mask &gt; 0] &lt;- NA\n\n# Subset raster based on mask\npre_fire_rast &lt;- mask(pre_fire_rast, mask = pre_mask)\n\n# View raster\nplot(pre_fire_rast)\n\n\n# Set all cells with values greater than 0 to NA\npost_mask[post_mask &gt; 0] &lt;- NA\n\n# Subset raster based on mask\npost_fire_rast &lt;- mask(post_fire_rast, mask = post_mask)\n\n# View raster\nplot(post_fire_rast)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-true-color-composite",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-true-color-composite",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Plot True Color Composite",
    "text": "Plot True Color Composite\n\nplotRGB(pre_fire_rast, r = 4, g = 3, b = 2, stretch = \"lin\", colNA = \"black\")\n\n\nplotRGB(post_fire_rast, r = 4, g = 3, b = 2, stretch = \"lin\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-false-color-composite",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-false-color-composite",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Plot False Color Composite",
    "text": "Plot False Color Composite\n\nplotRGB(pre_fire_rast, r = 7, g = 5, b = 3, stretch = \"lin\", colNA = \"black\")\n\n\nplotRGB(post_fire_rast, r = 7, g = 5, b = 3, stretch = \"lin\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-nbr-and-dnbr",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-nbr-and-dnbr",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Calculate NBR and dNBR",
    "text": "Calculate NBR and dNBR\n\npre_nbr_rast &lt;- terra::lapp(pre_fire_rast[[c(5, 7)]], fun = nbr_fun)\n\nplot(pre_nbr_rast, main = \"Cold Springs Pre-Fire NBR\", colNA = \"black\")\n\n\npost_nbr_rast &lt;- terra::lapp(post_fire_rast[[c(5, 7)]], fun = nbr_fun)\n\nplot(post_nbr_rast, main = \"Cold Springs Post-Fire NBR\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-and-plot-dnbr",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-and-plot-dnbr",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Calculate and Plot dNBR",
    "text": "Calculate and Plot dNBR\n\ndiff_nbr &lt;- pre_nbr_rast - post_nbr_rast\n\ntm_shape(diff_nbr) +\n  tm_raster(style = \"equal\", n = 6, \n            palette = get_brewer_pal(\"YlOrRd\", n = 6, plot = FALSE),\n            title = \"Difference NBR (dNBR)\", colorNA = \"black\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#reclassification",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#reclassification",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Reclassification",
    "text": "Reclassification\n\n# Set categories for severity levels\ncategories &lt;- c(\"Enhanced Regrowth\", \"Unburned\", \"Low Severity\", \"Moderate Severity\", \"High Severity\")\n\n# Create reclassification matrix\nrcl &lt;- matrix(c(-Inf, -0.1, 1, # group 1 ranges for Enhanced Regrowth\n                -0.1, 0.1, 2, # group 2 ranges for Unburned\n                0.1, 0.27, 3, # group 3 ranges for Low Severity\n                0.27, 0.66, 4, # group 4 ranges for Moderity Severity\n                0.66, Inf, 5), # group 5 ranges for High Severity\n                ncol = 3, byrow = TRUE)\n\n# Use reclassification matrix to reclassify dNBR raster\nreclassified &lt;- classify(diff_nbr, rcl = rcl)\n\nreclassified[is.nan(reclassified)] &lt;- NA\n\n\ntm_shape(reclassified) +\n  tm_raster(style = \"cat\",\n            labels = c(categories, \"Missing\"),\n            palette = get_brewer_pal(\"YlOrRd\", n = 5, plot = FALSE),\n            title = \"Severity Level\", colorNA = \"black\")+\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n# Read Santa Barbara PA vectors\nsb_protected_areas &lt;- read_sf(here::here(\"data\", \"cpad_super_units_sb.shp\")) %&gt;% \n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's city boundaries vector\nsb_city_boundaries &lt;- read_sf(here::here(\"data\", \"sb_city_boundaries_2003.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's county boundaries vector\nsb_county_boundary &lt;- read_sf(here::here(\"data\", \"sb_county_boundary_2020.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read iNaturalist bird observations from 2020-2024\naves &lt;- read_sf(here::here(\"data\", \"aves_observations_2020_2024.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\nApproach with spatial subset:\n\nSpatially subset the PA geometries to only those with bird observations\n\n\naves_PA_subset &lt;- sb_protected_areas[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_subset) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_subset)\n\n[1] 35\n\n\nApproach with a spatial join:\n\nAppend the Protected Area geometries to the bird observation geometries\n\n\naves_PA_join &lt;- st_join(aves, sb_protected_areas)\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_join) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_join)\n\n[1] 500\n\n\nAnd try adding a 5 km buffer around the protected areas:\n\n# Check if units are in meters\nst_crs(sb_protected_areas)$units\n\n[1] \"m\"\n\n# Create 5000m buffer around PAs\nPA_buffer_5km &lt;- st_buffer(sb_protected_areas, dist = 5000)\n\n# Subset the buffered PA geoms to only those with bird observations\naves_buffer_subset &lt;- PA_buffer_5km[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_buffer_subset) +\n  tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\n\n\n\n\n\n\n\nnrow(aves_buffer_subset)\n\n[1] 327"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-bird-observations-within-santa-barbaras-pas",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-bird-observations-within-santa-barbaras-pas",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n# Read Santa Barbara PA vectors\nsb_protected_areas &lt;- read_sf(here::here(\"data\", \"cpad_super_units_sb.shp\")) %&gt;% \n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's city boundaries vector\nsb_city_boundaries &lt;- read_sf(here::here(\"data\", \"sb_city_boundaries_2003.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's county boundaries vector\nsb_county_boundary &lt;- read_sf(here::here(\"data\", \"sb_county_boundary_2020.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read iNaturalist bird observations from 2020-2024\naves &lt;- read_sf(here::here(\"data\", \"aves_observations_2020_2024.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\nApproach with spatial subset:\n\nSpatially subset the PA geometries to only those with bird observations\n\n\naves_PA_subset &lt;- sb_protected_areas[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_subset) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_subset)\n\n[1] 35\n\n\nApproach with a spatial join:\n\nAppend the Protected Area geometries to the bird observation geometries\n\n\naves_PA_join &lt;- st_join(aves, sb_protected_areas)\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_join) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_join)\n\n[1] 500\n\n\nAnd try adding a 5 km buffer around the protected areas:\n\n# Check if units are in meters\nst_crs(sb_protected_areas)$units\n\n[1] \"m\"\n\n# Create 5000m buffer around PAs\nPA_buffer_5km &lt;- st_buffer(sb_protected_areas, dist = 5000)\n\n# Subset the buffered PA geoms to only those with bird observations\naves_buffer_subset &lt;- PA_buffer_5km[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_buffer_subset) +\n  tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\n\n\n\n\n\n\n\nnrow(aves_buffer_subset)\n\n[1] 327"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-pas-within-15-km-of-goleta",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-pas-within-15-km-of-goleta",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "Find PAs within 15 km of Goleta",
    "text": "Find PAs within 15 km of Goleta\n\n# Subset SB county to Goleta\ngoleta &lt;- sb_city_boundaries %&gt;%\n  dplyr::filter(NAME == \"Goleta\")\n\n# Create 15km buffer around Goleta\nst_crs(goleta)$units\n\n[1] \"m\"\n\ngoleta_buffer_15km &lt;- st_buffer(goleta, dist = 15000)\n\n# Explore the different outputs with different spatial operations\ngoleta_PAs_within &lt;- st_within(sb_protected_areas, goleta_buffer_15km)\ngoleta_PAs_intersect &lt;- st_intersects(sb_protected_areas, goleta_buffer_15km)\ngoleta_PAs_intersection &lt;- st_intersection(sb_protected_areas, goleta_buffer_15km)\n\n# Check class\nclass(goleta_PAs_intersect) == class(goleta_PAs_intersection)\n\n[1] FALSE FALSE FALSE FALSE\n\n# Compute distance-based join\ngoleta_PAs_join &lt;- st_join(sb_protected_areas, goleta, st_is_within_distance, dist = 15000)\n\n# Print the number of observations included in outputs\nnrow(goleta_PAs_intersection)\n\n[1] 185\n\nnrow(goleta_PAs_join)\n\n[1] 369"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-distance-between-goleta-and-dangermond-preserve",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-distance-between-goleta-and-dangermond-preserve",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "Find Distance between Goleta and Dangermond Preserve",
    "text": "Find Distance between Goleta and Dangermond Preserve\n\n# Subset PA to Dangermond Preserve\ndangermond &lt;- sb_protected_areas %&gt;%\n  dplyr::filter(UNIT_NAME == \"Jack and Laura Dangermond Preserve\")\n\n# Compute the distance between geometries edges, output as a matrix\ndanger_dist &lt;- st_distance(goleta, dangermond)\n\n# Calculate the geometric center\ndangermond_centroid &lt;- st_centroid(dangermond)\ngoleta_centroid &lt;- st_centroid(goleta)\n\ndanger_dist_centroid &lt;- st_distance(goleta_centroid, dangermond_centroid)\n\n# Check if the distance matrices are equal\ndanger_dist == danger_dist_centroid\n\n      [,1]\n[1,] FALSE"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\n# import all data files for Easter Island (3 vectors, 1 raster)\nei_points &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_points.gpkg\"))\nvolcanoes &lt;- subset(ei_points, type == \"volcano\")\nei_elev &lt;- stars::read_stars(here::here(\"data\", \"easter_island\", \"ei_elev.tif\"))\nei_borders &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\nei_roads &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))\n\n\n# first plot the elevation raster\ntm_shape(ei_elev) +\n  tm_raster(style = \"cont\",                # continuous values for this layer\n            palette = \"-RdYlGn\",           # reversed red-yellow-green continuous palette\n            title = \"Elevation (m asl)\") + # legend title with units\n  # add vector: Easter Island vector in default gray\n  tm_shape(ei_borders) +\n    tm_borders() +\n  # add vector: road lines in default black\n  tm_shape(ei_roads) +\n    tm_lines(lwd = \"strokelwd\", # line width depends on attribute value \n           legend.lwd.show = FALSE) +\n  # add vector: volcano points in default gray\n  tm_shape(volcanoes) +\n    tm_symbols(shape = 24,                       # triangle\n             size = \"elevation\",                 # symbol size depends on attribute value \n             title.size = \"Volcanoes (m asl)\") + # legend title with units\n  # general map layout options\n  tm_layout(main.title = \"Easter Island\",\n            bg.color = \"lightblue\") +            # background color for map (ocean)\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(show.labels = 1)                    # only show North label on compass"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-tmap",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-tmap",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\n# import all data files for Easter Island (3 vectors, 1 raster)\nei_points &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_points.gpkg\"))\nvolcanoes &lt;- subset(ei_points, type == \"volcano\")\nei_elev &lt;- stars::read_stars(here::here(\"data\", \"easter_island\", \"ei_elev.tif\"))\nei_borders &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\nei_roads &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))\n\n\n# first plot the elevation raster\ntm_shape(ei_elev) +\n  tm_raster(style = \"cont\",                # continuous values for this layer\n            palette = \"-RdYlGn\",           # reversed red-yellow-green continuous palette\n            title = \"Elevation (m asl)\") + # legend title with units\n  # add vector: Easter Island vector in default gray\n  tm_shape(ei_borders) +\n    tm_borders() +\n  # add vector: road lines in default black\n  tm_shape(ei_roads) +\n    tm_lines(lwd = \"strokelwd\", # line width depends on attribute value \n           legend.lwd.show = FALSE) +\n  # add vector: volcano points in default gray\n  tm_shape(volcanoes) +\n    tm_symbols(shape = 24,                       # triangle\n             size = \"elevation\",                 # symbol size depends on attribute value \n             title.size = \"Volcanoes (m asl)\") + # legend title with units\n  # general map layout options\n  tm_layout(main.title = \"Easter Island\",\n            bg.color = \"lightblue\") +            # background color for map (ocean)\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(show.labels = 1)                    # only show North label on compass"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-an-interactive-map-of-easter-island-with-tmap",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-an-interactive-map-of-easter-island-with-tmap",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "Create an Interactive Map of Easter Island with tmap",
    "text": "Create an Interactive Map of Easter Island with tmap\n\n# same code as static map above\nei_map &lt;- tm_shape(ei_elev) +\n  tm_raster(style = \"cont\",\n            palette = \"-RdYlGn\",\n            title = \"Elevation (m asl)\") +\n  tm_shape(ei_borders) + \n  tm_borders() +\n  tm_shape(ei_roads) + \n  tm_lines(lwd = \"strokelwd\", \n           legend.lwd.show = FALSE) +\n  tm_shape(volcanoes) +\n  tm_symbols(shape = 24, \n             size = \"elevation\",\n             title.size = \"Volcanoes (m asl)\") +\n  tm_layout(main.title = \"Easter Island\",\n            bg.color = \"lightblue\") +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(show.labels = 1)\n\n# convert default static map to interactive map \ntmap_mode(\"view\")\n\nei_map"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-ggplot2",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-ggplot2",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "Create a Map of Easter Island with ggplot2",
    "text": "Create a Map of Easter Island with ggplot2\n\n# extract lat & long from geom column \nvolcanoes_point &lt;- volcanoes %&gt;%\n  mutate(lon = unlist(map(volcanoes$geom,1)), # longitude = first component (x)\n         lat = unlist(map(volcanoes$geom,2))) # latitude = second component (y)\n\nggplot() +\n  # first add Easter Isand borders\n  geom_sf(data = ei_borders, color = \"#212529\") +\n  # add elevation raster\n  geom_stars(data = ei_elev) +\n  scale_fill_distiller(name = \"Elevation (m asl)\", # legend title with units\n                       palette = \"RdYlGn\",         # red-yellow-green palette (no need to reverse)\n                       na.value = \"lightblue\") +  # set NA color (background ocean)\n  # add road vector\n  geom_sf(data = ei_roads, color = \"#343a40\") +\n  # add volcano vector\n  geom_point(data = volcanoes_point, \n             aes(x = lon, y = lat, \n                 size = elevation), # point size depends on attribute value\n             shape = 17,            # triangle\n             color = \"#22577a\") +\n  scale_size_continuous(name = \"Volcanoes (m asl)\") +       # legend title with units\n  ggspatial::annotation_north_arrow(location = \"br\",        # bottom right\n                                    which_north = \"true\") + # point to north pole\n  ggspatial::annotation_scale(location = \"bl\",              # bottom left\n                              width_hint = 0.5) +           # proportion of map area the scalebar should occupy\n  labs(title = \"Easter Island\") +\n  theme_minimal()"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(kableExtra)\n\n\ndem &lt;- terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nperu &lt;- stars::read_stars(here::here(\"data\", \"week4-discussion\", \"PER_elv.tif\"))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(kableExtra)\n\n\ndem &lt;- terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nperu &lt;- stars::read_stars(here::here(\"data\", \"week4-discussion\", \"PER_elv.tif\"))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem-1",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem-1",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Plot a histogram and boxplot of dem",
    "text": "Plot a histogram and boxplot of dem\n\nhist(dem,\n     main = \"Digital Elevation Model Raster Value Distribution\",\n     xlab = \"Value\")\n\nboxplot(dem,\n        main = \"Digital Elevation Model Raster Value Distribution\",\n        ylab = \"Value\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#reclassify-elevation-and-find-mean",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#reclassify-elevation-and-find-mean",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Reclassify Elevation and Find Mean",
    "text": "Reclassify Elevation and Find Mean\n\n# define a reclassification matrix\nrcl &lt;- matrix(c(-Inf, 300, 0, # values -Inf to 300 = 0\n                300, 500, 1,  # values 300 to 500 = 1\n                500, Inf, 2), # values 500 to Inf = 2\n              ncol = 3, byrow = TRUE)\n\n# apply the matrix to reclassify the raster, making all cells 0 or 1 or 2\ndem_rcl &lt;- terra::classify(dem, rcl = rcl)\n\n# assign labels to the numerical categories\nlevels(dem_rcl) &lt;- tibble::tibble(id = 0:2, \n                                  cats = c(\"low\", \"medium\", \"high\"))\n\n# calculate mean elevation for each category using original DEM values\nelevation_mean &lt;- terra::zonal(dem, dem_rcl, fun = \"mean\")\nelevation_mean"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-correlation-between-ndwi-and-ndvi",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-correlation-between-ndwi-and-ndvi",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Find Correlation Between NDWI and NDVI",
    "text": "Find Correlation Between NDWI and NDVI\nDefine functions for calculating NDWI and NDVI\n\nndwi_fun &lt;- function(green, nir){\n    (green - nir)/(green + nir)\n}\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red)/(nir + red)\n}\n\nApply the functions to the appropriate Landsat 8 bands. Landsat 8 bands 2-5 correspond to bands 1-4 for this raster. Bands are as follows:\n\n\n\nBand\n\nColor\n\n\n\n\n1\nblue\n30 meter\n\n\n2\ngreen\n30 meter\n\n\n3\nred\n30 meter\n\n\n4\nnear-infrared\n30 meter\n\n\n\n\nndwi_rast &lt;- terra::lapp(landsat[[c(2, 4)]],\n                         fun = ndwi_fun)\nplot(ndwi_rast,\n     main = \"Zion National Park NDWI\")\n\nndvi_rast &lt;- terra::lapp(landsat[[c(4, 3)]],\n                         fun = ndvi_fun)\n\n# stack rasters\ncombine &lt;- c(ndvi_rast, ndwi_rast)\n\nplot(combine, main = c(\"NDVI\", \"NDWI\"))\n\n# calculate the correlation between raster layers \nterra::layerCor(combine, fun = cor)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-distances-across-all-peru-cells",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-distances-across-all-peru-cells",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Find Distances Across All peru Cells",
    "text": "Find Distances Across All peru Cells\n\n# Aggregate by a factor of 20 to reduce resolution and create new raster\nperu_agg &lt;- terra::aggregate(rast(peru), fact = 20)\nplot(peru_agg)\n\n# Create mask of ocean (NA values)\nwater_mask &lt;- is.na(peru_agg) # returns TRUE value for NA\n# Set all FALSE values to NA\nwater_mask[water_mask == 0] &lt;- NA\nplot(water_mask)\n\n# Find distance from each cell to ocean/coastline (default is unit = \"m\")\ndistance_to_coast &lt;- terra::distance(water_mask)\n# Convert from meters to kilometers \ndistance_to_coast_km &lt;- distance_to_coast/1000\n\nplot(distance_to_coast_km, main = \"Distance to the coast (km)\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#change-resolution-of-srtm",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#change-resolution-of-srtm",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Change Resolution of srtm",
    "text": "Change Resolution of srtm\n\nplot(srtm)\n\nrast_template &lt;- terra::rast(terra::ext(srtm), res = 0.01)\n\nsrtm_resampl1 &lt;- terra::resample(srtm, y = rast_template, method = \"bilinear\")\nsrtm_resampl2 &lt;- terra::resample(srtm, y = rast_template, method = \"near\")\nsrtm_resampl3 &lt;- terra::resample(srtm, y = rast_template, method = \"cubic\")\nsrtm_resampl4 &lt;- terra::resample(srtm, y = rast_template, method = \"cubicspline\")\nsrtm_resampl5 &lt;- terra::resample(srtm, y = rast_template, method = \"lanczos\")\n\nsrtm_resampl_all &lt;- c(srtm_resampl1, srtm_resampl2, srtm_resampl3, srtm_resampl4, srtm_resampl5)\nlabs &lt;- c(\"Bilinear\", \"Near\", \"Cubic\", \"Cubic Spline\", \"Lanczos\")\nplot(srtm_resampl_all, main = labs)"
  },
  {
    "objectID": "course-materials/discussions/week8-discussion.html#background",
    "href": "course-materials/discussions/week8-discussion.html#background",
    "title": "Week 8: Discussion Section",
    "section": "1. Background",
    "text": "1. Background\nThis week, you will practice function making with rasters and create a generalizable workflow to determine the climate envelope (or the climate where a species currently lives) of two endemic species to California:\n\nTule elk (Cervus canadensis nannodes)\nYellow-billed magpie (Pica nutalli)"
  },
  {
    "objectID": "course-materials/discussions/week8-discussion.html#get-started",
    "href": "course-materials/discussions/week8-discussion.html#get-started",
    "title": "Week 8: Discussion Section",
    "section": "2. Get Started",
    "text": "2. Get Started\n\nCreate an .Rproj as your version controlled project for Week 8\nCreate a Quarto document inside your project\nDownload this data folder from Google Drive and move it inside your project\nLoad all necessary packages and read spatial objects\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t have dismo installed, make sure to run: \n\ninstall.packages(\"dismo\")\n\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(dismo)\nlibrary(tmap)\n\n\nmagpie &lt;- read_csv(here::here(\"data\", \"magpie_obvs.csv\"))\ntule_elk &lt;- read_csv(here::here(\"data\", \"tule_elk_obvs.csv\"))\n\nbioclim_dir &lt;- here::here(\"data\", \"climate\", \"wc2.1_2.5m\")\nbioclim &lt;- list.files(bioclim_dir, pattern = glob2rx(\"*.tif$\"), full.names = TRUE)\nbioclim_sort &lt;- bioclim[\n  # Sort filepaths based on numeric suffix\n  order(\n  # Extract numeric suffix of filenames and convert to numeric\n  as.numeric(gsub(\".*_(\\\\d+)\\\\.tif$\", \"\\\\1\", bioclim)))]\nbioclim_rast &lt;- rast(bioclim_sort)"
  },
  {
    "objectID": "course-materials/discussions/week8-discussion.html#your-task",
    "href": "course-materials/discussions/week8-discussion.html#your-task",
    "title": "Week 8: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nUpdate the bioclim_rast layer names to match the Bioclimatic variables using names()\nUse st_as_sf to convert and find the geographic extent of species occurrence data magpie using st_bbox()\nCrop bioclim_rast to match the extent of species occurrence data magpie\nExtract points from the cropped bioclim_rast for each occurrence in magpie\nCreate “background values” by using dismo::randomPoints() to generate random sample points from the cropped bioclim_rast\nExtract points from bioclim_rast for each random sample point generated in Step #5\nPlot species climate niche and background climate (temperature vs. precipitation)\nModify steps 2-7 into a generalizable workflow for other species occurrence data\nTry your new function with species occurrence data tule_elk"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html",
    "href": "course-materials/discussions/week1-discussion.html",
    "title": "Week 1: Discussion Section",
    "section": "",
    "text": "In this section, we’ll kick-off our mapmaking skills using the tmap package. Your task will be to make a map of Rapa Nui (Easter Island or Isla de Pascua). Rapa Nui is a volcanic island in Polynesia and territory of Chile. It is widely known for monumental statues called moai, but it is also a place of geoscientific inquiry. Several research programs about the island’s volcanic history, petrology, and human-environment interactions exist."
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week1-discussion.html#learning-objectives",
    "title": "Week 1: Discussion Section",
    "section": "1. Learning Objectives",
    "text": "1. Learning Objectives\n\nRead in spatial objects\nCreate map with single spatial object\nCreate map with multiple spatial objects\nUse different types of tmap plotting formats (e.g. tm_polygons(), tm_fill(), tm_dots(), etc.)\nAdjust color palettes\nInclude essential map elements (e.g. scale bar & north arrow or graticules)\nCreate an interactive map\nBonus Challenge: Reproduce map using ggplot2 instead of tmap\n\n\n\n\n\n\n\nTipMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#get-started",
    "href": "course-materials/discussions/week1-discussion.html#get-started",
    "title": "Week 1: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nAdd (at least) a subfolder to your R project: data\nCreate a Quarto document\n\nLet’s load all necessary packages:\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(tmap)\n\nNext, let’s download some data from Rapa Nui. Unzip and move this to your version-controlled R Project’s data folder.\n\nRead in the spatial objects for Rapa Nui:\n\nei_points: file contains several points on the island\n\nUse the pipe operator to filter() output for points representing volcanoes\n\nei_elev: raster with elevation data\nei_borders: polygon with the island outline\nei_roads: lines contains a road network for the island\n\n\n\nei_points &lt;- st_read(here(\"data\", \"easter_island\", \"ei_points.gpkg\")) |&gt; \n  filter(type == \"volcano\")\nei_elev &lt;- read_stars(here(\"data\", \"easter_island\", \"ei_elev.tif\"))\nei_borders &lt;- st_read(here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\nei_roads &lt;- st_read(here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#your-task",
    "href": "course-materials/discussions/week1-discussion.html#your-task",
    "title": "Week 1: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nCreate a map of Easter Island\nCreate a map of Easter Island and…\n\n\n…denote the island’s borders and continuous elevation\n…denote the island’s volcanoes and roads\n…play with the color palette and essential map elements\n\n\nCreate an interactive map of Easter Island"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html",
    "href": "course-materials/discussions/week4-discussion.html",
    "title": "Week 4: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from Geocomputation with R by Robin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#get-started",
    "href": "course-materials/discussions/week4-discussion.html#get-started",
    "title": "Week 4: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nCreate a Quarto document\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t have spDataLarge installed, make sure to run:  install.packages('spDataLarge', repos='https://nowosad.github.io/drat/', type='source')\n\n\nLet’s load all necessary packages:\n\nlibrary(tidyverse)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\n\nYou will be working with the following datasets:\n\nDigital elevation model (DEM) of Mt. Mongón, Perú, obtained from spDataLarge\nLandsat image of Zion National Park, obtained from spDataLarge\nSRTM elevation of Zion National Park, obtained from spDataLarge\n\n\n\ndem &lt;- terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#your-task",
    "href": "course-materials/discussions/week4-discussion.html#your-task",
    "title": "Week 4: Discussion Section",
    "section": "2. Your Task",
    "text": "2. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nPlot a histogram and boxplot of dem\nReclassify dem and compute the mean for the three classes:\n\n\nLow, where elevation is less than 300\nMedium\nHigh, where elevation is greater than 500\n\n\nCalculate the Normalized Difference Vegetation Index (NDVI) and Normalized Difference Water Index (NDWI) of landsat and find a correlation between NDVI and NDWI\n\n\nNote: \\(NDVI = (NIR - red)/(NIR + red)\\)\nNote: \\(NDWI = (green - NIR)/(green + NIR)\\)\n\nApply the functions to the appropriate Landsat 8 bands. Landsat 8 bands 2-5 correspond to bands 1-4 for this raster. Bands are as follows:\n\n\n\nBand\n\nColor\n\n\n\n\n1\nblue\n30 meter\n\n\n2\ngreen\n30 meter\n\n\n3\nred\n30 meter\n\n\n4\nnear-infrared\n30 meter\n\n\n\n\nFind the distance across all cells in peru to its nearest coastline\n\n\nHint: Use terra::distance() to find geographic distance for all cells\nNote: terra::distance() will calculate distance for all cells that are NA to the nearest cell that are not NA\nWeigh the distance raster with peru and visualize the difference between the raster created using the Euclidean distance (E7) and the raster weighted by elevation\n\nEvery 100 altitudinal meters should increase the distance to the coast by 10 km\n\n\n\nChange the resolution of srtm to 0.01 by 0.01 degrees\n\n\nUse all of the method available in the terra package\nNote: The srtm raster has a resolution of 0.00083 x 0.00083 degrees"
  },
  {
    "objectID": "course-materials/discussions/extra-discussion.html",
    "href": "course-materials/discussions/extra-discussion.html",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggspatial)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the ghm.tif files. ghm.tif Transform both to EPSG:4326.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")"
  },
  {
    "objectID": "course-materials/discussions/extra-discussion.html#prerequsites",
    "href": "course-materials/discussions/extra-discussion.html#prerequsites",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggspatial)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the ghm.tif files. ghm.tif Transform both to EPSG:4326.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")"
  },
  {
    "objectID": "course-materials/discussions/extra-discussion.html#exercises",
    "href": "course-materials/discussions/extra-discussion.html#exercises",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s make nice plots of the California Protected areas by access level\n\n\np1 &lt;- ggplot2::ggplot(data = cpad_super) +\n  geom_sf(aes(color = access_typ, fill = access_typ)) +\n  theme_bw() +\n  labs(\n    color = \"Access Type\",\n    fill = \"Access Type\"\n  ) +\n  annotation_scale(plot_unit = \"km\") +\n  annotation_north_arrow(\n    location = \"tr\",\n    pad_x = unit(0.2, \"in\"),\n    pad_y = unit(0.2, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill =\n        c(\"grey40\", \"white\"),\n      line_col = \"grey20\"\n    )\n  ) +\n  coord_sf() +\n  scale_color_viridis_d() +\n  scale_fill_viridis_d()\n\n\np1 +\n  facet_wrap(~access_typ) +\n  theme(strip.background = element_rect(fill = \"transparent\"))\n\n\nLet’s try plotting the ghm layers nicely too!\n\n\nggplot() +\n    geom_stars(data = st_as_stars(ghm)) +\n  coord_equal() +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"\",\n    fill = \"Global Human Modification\"\n  ) +\n  scale_fill_viridis_c() +\n  annotation_scale(plot_unit = \"km\") +\n  annotation_north_arrow(\n    location = \"tr\",\n    pad_x = unit(0.2, \"in\"),\n    pad_y = unit(0.2, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill =\n        c(\"grey40\", \"white\"),\n      line_col = \"grey20\"\n    )\n  )\n\n\nCreate a function to take 2 data sets (1 polygon and 1 raster) and create a boxplot of the values based on a specific layer\n\n\nsummary_boxplot &lt;- function(polygon, raster, my_layer, my_label) {\n  \n  # rasterize polygon by layer\n  id_rast &lt;- rasterize(polygon, raster, field = \"suid_nma\")\n  \n  #do mean zonal statistics\n  zonal_layer &lt;- zonal(raster, id_rast, fun = \"mean\", na.rm = TRUE)\n  \n  #join with polygon database\n  poly_join &lt;- full_join(polygon, zonal_layer) %&gt;% \n    select(suid_nma, gHM, my_layer)\n  \n  #create boxplot based on your layer\n  p1 &lt;- ggplot(poly_join) +\n    geom_boxplot(aes(gHM, .data[[my_layer]])) +\n    theme_bw() +\n    labs(x = \"Human Modification Index\", \n         y = my_label)\n  \n  return(p1)\n}\n\n\nLet’s select some layers and use our new function!\n\n\nnames(cpad_super)\n\n\naccess &lt;- summary_boxplot(cpad_super, ghm, \"access_typ\", \"Access Type\")\n\naccess\n\n\nlayer &lt;- summary_boxplot(cpad_super, ghm, \"layer\", \"Management Agency Type\")\n\nlayer"
  },
  {
    "objectID": "course-materials/labs/week10.html#data",
    "href": "course-materials/labs/week10.html#data",
    "title": "Week 10: Lab",
    "section": "1. Data",
    "text": "1. Data\n\nLidar data\n\ndigital surface models (DSM) represent the elevation of the top of all objects\ndigital terrain model (DTM) represent the elevation of the ground (or terrain)\n\nData files:\n\nSJER2013_DSM.tif\nSJER2013_DTM.tif\n\n\n\nVegetation plot geometries\n\nContains locations of vegetation surveys\nPolygons representing 20m buffer around plot centroids\n\nData file: SJERPlotCentroids_Buffer.shp\n\n\nVegetation surveys\n\nMeasurements for individual trees in each plot\n\nData files: - D17_2013_vegStr.csv - Metadata available in D17_2013_vegStr_metadata_desc.csv"
  },
  {
    "objectID": "course-materials/labs/week10.html#set-up",
    "href": "course-materials/labs/week10.html#set-up",
    "title": "Week 10: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nLet’s load all necessary packages:\n\nlibrary(terra)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(here)"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-lidar-data",
    "href": "course-materials/labs/week10.html#load-lidar-data",
    "title": "Week 10: Lab",
    "section": "2. Load Lidar data",
    "text": "2. Load Lidar data\n\n# digital surface model (DSM)\ndsm &lt;- rast(here::here(\"data\", \"SJER2013_DSM.tif\"))\n\n# digital terrain model (DTM)\ndtm &lt;- rast(here::here(\"data\", \"SJER2013_DTM.tif\"))\n\nLet’s check if the DSM and DTM have the same resolution, position, and extent by creating a raster stack:\n\ntest_raster &lt;- c(dsm, dtm)\n\nCreate the canopy height model (CHM) or the height of all objects by finding the difference between the DSM and DTM:\n\nchm &lt;- dsm - dtm"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-vegetation-plot-geometries",
    "href": "course-materials/labs/week10.html#load-vegetation-plot-geometries",
    "title": "Week 10: Lab",
    "section": "3. Load vegetation plot geometries",
    "text": "3. Load vegetation plot geometries\nThis includes the locations of study plots and the surveys of individual trees in each plot.\n\n# read in plot centroids\nplot_centroids &lt;- st_read(here::here(\"data\", \"PlotCentroids\", \"SJERPlotCentroids_Buffer.shp\"))\n\n\n# test if the plot CRS matches the Lidar CRS\nif(st_crs(plot_centroids) == st_crs(chm)) {\n  print(\"coordinate reference systems match\")\n} else{\n  plot_centroids &lt;- st_transform(plot_centroids, crs = st_crs(chm))\n}\n\n[1] \"coordinate reference systems match\"\n\n\n\n\nCode\ntm_shape(chm) +\n  tm_raster(col.legend = tm_legend(\"Digital surface model (m)\")) +\n  tm_shape(plot_centroids) +\n  tm_polygons()"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-vegetation-survey-data",
    "href": "course-materials/labs/week10.html#load-vegetation-survey-data",
    "title": "Week 10: Lab",
    "section": "4. Load vegetation survey data",
    "text": "4. Load vegetation survey data\nLet’s find the maximum tree height in each plot:\n\n# read in survey data and find the maximum tree height in each plot\nveg_surveys &lt;- read.csv(here::here(\"course-materials\", \"data\", \"week10\", \"VegetationData\", \"D17_2013_vegStr.csv\")) %&gt;%\n  group_by(plotid) %&gt;%\n  summarise(\"survey_height\" = max(stemheight, na.rm = TRUE))\n\nNow find the maximum tree height in each plot as determined by the CHM:\n\nextract_chm_height &lt;- terra::extract(chm, plot_centroids, fun = max) %&gt;%\n  rename(chm_height = SJER2013_DSM) %&gt;%\n  select(chm_height)\n\nCombine tree height estimates from the Lidar and plot surveys:\n\nplot_centroids &lt;- cbind(plot_centroids, extract_chm_height) %&gt;%\n  left_join(.,veg_surveys, by = c(\"Plot_ID\" = \"plotid\"))"
  },
  {
    "objectID": "course-materials/labs/week10.html#plot-results",
    "href": "course-materials/labs/week10.html#plot-results",
    "title": "Week 10: Lab",
    "section": "5. Plot results",
    "text": "5. Plot results\nLet’s compare the estimates between the two methods: Lidar and on-the-ground surveys\n\nTo make the comparison, we’ll add a 1:1 line\n\nIf all the points fall along this line it means that both methods give the same answer\n\nLet’s also add a regression line with confidence intervals to compare how the overall fit between methods compares to the 1:1 line\n\n\n\nCode\nggplot(plot_centroids, aes(y=chm_height, x= survey_height)) +\n  geom_abline(slope=1, intercept=0, alpha=.5, lty=2) + #plotting our \"1:1\" line\n  geom_point() +\n  geom_smooth(method = lm) + # add regression line and confidence interval\n  ggtitle(\"Validating Lidar measurements\") +\n  xlab(\"Maximum Measured Height (m)\") +\n  ylab(\"Maximum Lidar Height (m)\")\n\n\n\n\n\n\n\n\n\nWe’ve now compared Lidar estimates of tree height to on-the-ground measurements!\n\n\n\n\n\n\nWarningInterpreting results\n\n\n\nIt looks like the Lidar estimates tend to underestimate tree height for shorter trees and overestimates tree height for taller trees. Or maybe human observers underestimate the height of tall trees because they’re challenging to measure? Or maybe the digital terrain model misjudged the elevation of the ground? There could be many reasons that the answers don’t line up! It’s then up to the researcher to figure out if the mismatch is important for their problem."
  },
  {
    "objectID": "course-materials/labs/week4.html",
    "href": "course-materials/labs/week4.html",
    "title": "Week 4: Lab",
    "section": "",
    "text": "In this lab we’ll be exploring the basics of raster data, including spatial data and geometry operations. Raster data represents continuous surfaces, as opposed to the discrete features represented in the vector data model. We’ll primarily be working with data from Zion National Park in Utah."
  },
  {
    "objectID": "course-materials/labs/week4.html#set-up",
    "href": "course-materials/labs/week4.html#set-up",
    "title": "Week 4: Lab",
    "section": "1. Set Up",
    "text": "1. Set Up\nFirst, let’s install the {geoData} package which we’ll use later in the lab to get access to example datasets.\n\ninstall.packages(\"geodata\")\n\nNow let’s load all the necessary packages. R has several packages for handling raster data. In this lab, we’ll use the {terra} package.\n\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(kableExtra) # table formatting\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data\nlibrary(geodata) # spatial data"
  },
  {
    "objectID": "course-materials/labs/week4.html#raster-objects",
    "href": "course-materials/labs/week4.html#raster-objects",
    "title": "Week 4: Lab",
    "section": "2. Raster objects",
    "text": "2. Raster objects\nIn this section we’ll learn how to create raster data objects by reading in data and how to do basic data manipulations.\n\nCreating raster objects\nThe {terra} package represents raster objects using the SpatRaster class. The easiest way to create SpatRaster objects is to read them in using the rast() function. Raster objects can handle both continuous and categorical data.\nWe’ll start with an example of two datasets for Zion National Park from the spDataLarge package:\n\nsrtm.tif: remotely sensed elevation estimates (continuous data)\nnlcd.tif: simplified version of the National Land Cover Database 2011 product (categorical data)\n\n\n# create raster objects\nzion_elevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion_land &lt;- rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\n\n# test class of raster object\nclass(zion_elevation)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(\"Elevation (m)\"))+   \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n  \n\nmap2 &lt;- tm_shape(zion_land) +\n  tm_raster(col.legend = tm_legend(\"Land cover\"))+\n      tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nThe SpatRaster class can also handle multiple “layers”. Layers can store different variables for the same region in one object. This is similar to attributes (or columns) in data.frames. Later in the course when we discuss multispectral data, we’ll learn more about why remotely-sensed data will often contain multiple “bands” or layers.\nAs an example, we’ll load a dataset from spDataLarge containing the four bands of the Landsat 8 image for Zion National Park.\n\nlandsat &lt;- rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\n\nnlyr(landsat) # test number of layers in raster object\n\n[1] 4\n\n\n\n\nCode\ntm_shape(landsat) +\n  tm_raster(col.legend = tm_legend(\"Unscaled reflectance\"),\n            col.free = FALSE )\n\n\n\n\n\n\n\n\n\nWe can subset layers using either the layer number or name:\n\nnames(landsat)\n\n[1] \"landsat_1\" \"landsat_2\" \"landsat_3\" \"landsat_4\"\n\nlandsat3 &lt;- subset(landsat, 3)\nlandsat4 &lt;- subset(landsat, \"landsat_4\")\n\nWe can combine SpatRaster objects into one, using c():\n\nlandsat34 &lt;- c(landsat3, landsat4)\n\n\n\nMerging Rasters\nIn some cases, data for a region will be stored in multiple, contiguous files. To use them as a single raster, we need to merge them.\nIn this example, we download elevation data for Austria and Switzerland and merge the two rasters into one.\n\naustria &lt;- geodata::elevation_30s(country = \"AUT\", path = tempdir())\nswitzerland &lt;- geodata::elevation_30s(country = \"CHE\", path = tempdir())\n\nmerged &lt;- merge(austria, switzerland)\n\n\n\nCode\nmap1 &lt;- tm_shape(austria) +\n  tm_raster(col.legend = tm_legend( \"Elevation (m)\")) +\n  tm_title(text = \"Austria\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n  \nmap2 &lt;- tm_shape(switzerland) +\n  tm_raster(col.legend = tm_legend( \"Elevation (m)\")) +\n  tm_title(text = \"Switzerland\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap3 &lt;- tm_shape(merged) +\n tm_raster(col.legend = tm_legend( \"Elevation (m)\")) +\n  tm_title(text = \"Merged\") +\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n  \ntmap_arrange(map1, map2, map3, nrow = 1)  \n\n\n\n\n\n\n\n\n\n\n\nInspecting raster objects\nWe can get info on raster values just by typing the name or using the summary function.\n\nsummary(zion_elevation)\n\nWarning: [summary] used a sample\n\n\n      srtm     \n Min.   :1024  \n 1st Qu.:1535  \n Median :1836  \n Mean   :1843  \n 3rd Qu.:2114  \n Max.   :2892  \n\n\nWe can get global summaries, such as standard deviation.\n\nglobal(zion_elevation, sd)\n\n           sd\nsrtm 416.6776\n\n\nOr we can use freq() to get the counts with categories.\n\nfreq(zion_land)\n\n  layer      value  count\n1     1      Water   1209\n2     1  Developed  17517\n3     1     Barren 106070\n4     1     Forest 767537\n5     1  Shrubland 545771\n6     1 Herbaceous   4878\n7     1 Cultivated   8728\n8     1   Wetlands   6497\n\n\n\n\nIndexing\nWe can index rasters using row-column indexing or cell IDs.\n\n# row 1, column 1\nzion_elevation[1, 1]\n\n  srtm\n1 1728\n\n# cell ID 1\nzion_elevation[1]\n\n  srtm\n1 1728\n\n\nFor multi-layer rasters, subsetting returns the values in both layers.\n\nlandsat[1]\n\n  landsat_1 landsat_2 landsat_3 landsat_4\n1      9833      9579      9861     14114\n\n\nWe can also modify/overwrite cell values.\n\nzion_elevation[1, 1] &lt;- 0\nzion_elevation[1, 1]\n\n  srtm\n1    0\n\n\nReplacing values in multi-layer rasters requires a matrix with as many columns as layers and rows as replaceable cells.\n\nlandsat[1] &lt;- cbind(c(0), c(0),c(0), c(0))\nlandsat[1]\n\n  landsat_1 landsat_2 landsat_3 landsat_4\n1         0         0         0         0\n\n\nWe can also use a similar approach to replace values that we suspect are incorrect.\n\ntest_raster &lt;- zion_elevation\ntest_raster[test_raster &lt; 20] &lt;- NA"
  },
  {
    "objectID": "course-materials/labs/week4.html#spatial-subsetting",
    "href": "course-materials/labs/week4.html#spatial-subsetting",
    "title": "Week 4: Lab",
    "section": "3. Spatial subsetting",
    "text": "3. Spatial subsetting\nWe can move from subsetting based on specific cell IDs to extract info based on spatial objects.\nTo use coordinates for subsetting, we can “translate” coordinates into a cell ID with the functions terra::cellFromXY() or terra::extract().\n\n# create point within area covered by raster\npoint &lt;- matrix(c(-113, 37.5), ncol = 2)\n\n# approach 1\n# find cell ID for point\nid &lt;- cellFromXY(zion_elevation, xy = point)\n# index to cell\nzion_elevation[id]\n\n  srtm\n1 2398\n\n# approach 2\n# extract raster values at point\nterra::extract(zion_elevation, point)\n\n  srtm\n1 2398\n\n\nWe can also subset raster objects based on the extent another raster object. Here we extract the values of our elevation raster that fall within the extent of a clipping raster that we create.\n\n# create a raster with a smaller extent\nclip &lt;- rast(xmin = -113.3, xmax = -113, ymin = 37.2, ymax = 37.9,\n            resolution = 0.3,\n            vals = 1)\n\n# select values that fall within smaller extent\nzion_elevation_clip &lt;- zion_elevation[clip]\n\n# verify that output has fewer values than original\nif(ncell(zion_elevation) == nrow(zion_elevation_clip)) {\n  warning(\"clipping did not remove cells\")\n} else {\n  print(\"clipping removed cells\")\n}\n\n[1] \"clipping removed cells\"\n\n\nIn the previous example, we just got the values of the raster back (and lost the raster format). In some cases, we might want the output to be the raster cells themselves.\nWe can do this use the “[” operator and setting “drop = FALSE”.\n\nzion_elevation_clip &lt;- zion_elevation[clip, drop = FALSE]\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(zion_elevation_clip) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"clipped\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\n\ntmap_arrange(map1, map2, nrow = 1)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningWoops, is the clipped version really smaller?\n\n\n\nPlotting side-by-side, the clipped version appears to take up more space – does this mean the clipping didn’t work? How can we tell?\n\nVisually: If we look at the features represented, we can see that the clipped version doesn’t represent all the features present in the original version.\nQuantitatively: We can directly check whether the extents match using the ext() function!\n\n\nif(ext(zion_elevation) == ext(zion_elevation_clip)){\n  print(\"extents match\")\n} else{\n  print(\"extents do not match\")\n}\n\n[1] \"extents do not match\"\n\n\n\n\nIn the previous example, we subsetted the extent of the raster (removed cells). Another common use of spatial subsetting is to select cells based on their values. In this case we create a “masking” raster comprised of logicals or NAs that dictates the cells we would like to preserve.\n\n# create raster mask of the same resolution and extent\nrmask &lt;- zion_elevation\n\n# set all cells with elevation less than 2000 meters to NA\nrmask[rmask &lt; 2000] &lt;- NA\n \n# subset elevation raster based on mask\n\n# approach 1: bracket subsetting\nmasked1 &lt;- zion_elevation[rmask, drop = FALSE]   \n# approach 2: mask() function\nmasked2 &lt;- mask(zion_elevation, rmask)           \n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(masked1) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"bracket subsetting\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(masked2) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"mask()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week4.html#map-algebra",
    "href": "course-materials/labs/week4.html#map-algebra",
    "title": "Week 4: Lab",
    "section": "4. Map algebra",
    "text": "4. Map algebra\n“Map algebra” is the set of operations that modify or summarize raster cell values with reference to surrounding cells, zones, or statistical functions that apply to every cell. Map algebra is typically categorized into local, focal, and zonal operations.\n\nLocal operations\nLocal operations are computed on each cell individually. For example, we can use ordinary arithmetic or logical statements.\n\nzion_elevation + zion_elevation # doubles each cells' value\nzion_elevation^2 # raises each cells' value to the power of 2\nlog(zion_elevation) # takes the log of each cells' value\nzion_elevation &gt; 5 # determines whether each cell has a value greater than 5\n\nWe can also classify intervals of values into groups. For example, we could classify elevation into low, middle, and high elevation cells.\nFirst, we need to construct a reclassification matrix:\n\nThe first column corresponds to the lower end of the class\nThe second column corresponds to the upper end of the class\nThe third column corresponds to the new value for the specified ranges in columns 1 and 2\n\n\n# create reclassification matrix\nrcl &lt;- matrix(c(1000, 1500, 1, # group 1 ranges from 1000 - 1500 m\n                1500, 2000, 2, # group 2 ranges from 1500 - 2000 m\n                2000, 2500, 3, # group 3 ranges from 2000 - 2500 m\n                2500, 3000, 4), # group 4 ranges from 2500 - 3000 m\n                ncol = 3, byrow = TRUE)\n\n# use reclassification matrix to reclassify elevation raster\nreclassified &lt;- classify(zion_elevation, rcl = rcl)\n\n# change reclassified values into factors\nvalues(reclassified) &lt;- as.factor(values(reclassified))\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(reclassified) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text= \"reclassified\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\n\nFor more efficient processing, we can use a set of map algebra functions:\n\napp() applies a function to each cell of a raster to summarize the values of multiple layers into one layer\ntapp() is an extension of app() that allows us to apply an operation on a subset of layers\nlapp() allows us to apply a function to each cell using layers as arguments\n\nWe can use the lapp()function to compute the Normalized Difference Vegetation Index (NDVI). (More on this later in the quarter!) Let’s calculate NDVI for Zion National Park using multispectral satellite data.\nFirst, we need to define a function to calculate NDVI. Then, we can use lapp() to calculate NDVI in each raster cell. To do so, we just need the NIR and red bands.\n\n# define NDVI as the normalized difference between NIR and red bands\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\n# apply NDVI function to Landsat bands 3 & 4\nndvi_rast &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n\n\n\nCode\ntm_shape(ndvi_rast) +\n  tm_raster(col.legend = tm_legend(title = \"NDVI\"))\n\n\n\n\n\n\n\n\n\n\n\nFocal Operations\nLocal operations operate on one cell, though from multiple layers. Focal operations take into account a central (focal) cell and its neighbors. The neighborhood (or kernel, moving window, filter) can take any size or shape. A focal operation applies an aggregation function to all cells in the neighborhood and updates the value of the central cell before moving on to the next central cell.\nThe image below provides an example of using a moving window filter. The large orange square highlights the 8 cells that are considered “neighbors” to the central cell (value = 8). Using this approach, the value of the central cell will be updated to the minimum value of its neighboring cells (in this case 0). This process then repeats for each cell.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\n\n\nWhy are the cells on the border in the filtered raster now have values of NA?\nThe cells along the border do not have a complete set of “neighbors”, therefore the filtering operation returns a NA.\n\n\n\nWe can use the focal() function to perform spatial filtering. We define the size, shape, and weights of the moving window using a matrix. In the following example we’ll find the minimum value in 9x9 cell neighborhoods.\n\nelevation_focal &lt;- focal(zion_elevation, \n                         w = matrix(1, nrow = 9, ncol = 9), # create moving window\n                         fun = min) # function to map new values\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(elevation_focal) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"aggregated\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\n\n\nWhat should we expect to observe in the output for this spatial filtering example?\n\nOverall, we see more lower values because we are finding the minimum value in each neighborhood\nThe output looks “grainier” because many cells have the same values as their neighbors\n\n\n\n\n\n\nZonal Operations\nSimilar to focal operations, zonal operations apply an aggregation function to multiple cells. However, instead of applying operations to neighbors, zonal operations aggregate based on “zones”. Zones can are defined using a categorical raster and do not necessarily have to be neighbors\nFor example, we could find the average elevation within the elevations zones we created.\n\nzonal(zion_elevation, reclassified, fun = \"mean\") %&gt;%\n  kable(col.names = c(\"Elevation zone\", \"Mean elevation (m)\")) %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nElevation zone\nMean elevation (m)\n\n\n\n\n0\n0.000\n\n\n1\n1292.281\n\n\n2\n1771.967\n\n\n3\n2222.776\n\n\n4\n2640.340"
  },
  {
    "objectID": "course-materials/labs/week4.html#changing-extent-origin-and-resolution",
    "href": "course-materials/labs/week4.html#changing-extent-origin-and-resolution",
    "title": "Week 4: Lab",
    "section": "1. Changing extent, origin, and resolution",
    "text": "1. Changing extent, origin, and resolution\n\nExtent\nIn the simplest case, two images differ only in their extent. Let’s start by increasing the extent of a elevation raster.\n\nelev_2 &lt;- extend(zion_elevation, c(1, 200)) # add one row and two columns\n\nPerforming algebraic operations on objects with different extents doesn’t work.\n\nelev + elev_2\n\nWe can align the extent of the 2 rasters using the extend() function. Here we extend the zion_elevation object to the extent of elev_2 by adding NAs.\n\nelev_3 &lt;- extend(zion_elevation, elev_2)\n\n\n\nOrigin\nThe origin function returns the coordinates of the cell corner closes to the coordinates (0,0).\n\norigin(zion_elevation)\n\n[1] -0.0004165537 -0.0004165677\n\n\n\n\nResolution\nRaster datasets can also differ in their resolution. To match resolutions we can decrease (or coarsen) the resolution by aggregating or increase (or sharpen) the resolution by disaggregating.\n\nAggregating\nWhen decreasing the resolution of rasters, we are effectively combining multiple cells into a single cell. Let’s start by coarsening the resolution of the Zion elevation data by a factor of 5, by taking the mean value of cells.\n\nzion_elevation_coarse &lt;-  aggregate(zion_elevation, fact = 5, fun = mean)\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(zion_elevation_coarse) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"aggregated\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\n\n\nWhat should we observe in the output?\nThe aggregated raster appears “grainier” because the cells are now larger. This is the same concept as having a more pixelated image.\n\n\n\n\n\nDisaggregating\nTo increase the resolution of a raster, we need to break a single cell into multiple cells. There are many ways to do this and the appropriate method will often depend on our specific purpose. However, most approaches define the values of the new (smaller) cells based on not only the value of the original cell they came from, but also neighboring cells.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\nIn the example below, we use the bilinear method to disaggregate the elevation raster we aggregated in the previous example.\n\n\n\n\n\n\nTipCritical thinking\n\n\n\n\n\nDoes disaggregating the aggregated version get us back to the original raster?\nNo! There is no way for us to exactly recover the original data from the aggregated version.\n\n\n\n\n# disaggregate the aggregated raster\nzion_elevation_disagg &lt;- disagg(zion_elevation_coarse, fact = 5, method = \"bilinear\")\n\n# check whether the disaggregated version matches the original\nif(identical(zion_elevation, zion_elevation_disagg)){\n  print(\"disaggregated data matches original\")\n} else {\n  warning(\"disaggregated data does not match original\")\n}\n\nWarning: disaggregated data does not match original\n\n\n\n\nCode\nmap3 &lt;- tm_shape(zion_elevation_disagg) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"disaggregated\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, map3, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week4.html#resampling",
    "href": "course-materials/labs/week4.html#resampling",
    "title": "Week 4: Lab",
    "section": "2. Resampling",
    "text": "2. Resampling\nAggregation/disaggregation work when both rasters have the same origins.\nBut what do we do in the case where we have two or more rasters with different origins and resolutions? Resampling computes values for new pixel locations based on custom resolutions and origins.\nThe images below show that we are trying to find the values of the original raster within the cells defined by the new “target” raster.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\nIn most cases, the target raster would be an object you are already working with, but here we define a target raster.\n\ntarget_rast &lt;- rast(xmin = -113.2, xmax = -112.9,\n                   ymin = 37.14, ymax = 37.5,\n                   nrow = 450, ncol = 460, \n                   crs = crs(zion_elevation))\n\nzion_elevation_resample &lt;- resample(zion_elevation, y = target_rast, method = \"bilinear\")\n\n\n\nCode\nmap4 &lt;- tm_shape(zion_elevation_resample) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"resampled\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map4, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week3.html",
    "href": "course-materials/labs/week3.html",
    "title": "Week 3: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of spatial and geometry operations on vector data in R using the sf package. We’ll be working with data representing the heigh points of New Zealand."
  },
  {
    "objectID": "course-materials/labs/week3.html#set-up",
    "href": "course-materials/labs/week3.html#set-up",
    "title": "Week 3: Lab",
    "section": "1. Set Up",
    "text": "1. Set Up\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(spData)"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-subsetting-filtering",
    "href": "course-materials/labs/week3.html#spatial-subsetting-filtering",
    "title": "Week 3: Lab",
    "section": "2. Spatial subsetting (filtering)",
    "text": "2. Spatial subsetting (filtering)\nWhen working with tabular data, we have frequently found it useful to subset the data.frame we are working with based on some condition using dplyr::filter().\n\n\n\n\n\n\n\n\n\n\nArtwork by Allison Horst\n\nFor example, last week we saw how we could filter to countries whose average life expectancy is greater than 80 years old using the following code:\n\nworld %&gt;%\n  filter(lifeExp &gt;= 80)\n\nSimilarly, we might want to filter data based on its spatial relationships. In this case, we use spatial subsetting which is the process of converting a spatial object into a new object containing only the spatial features that relate in space to another object. This is analogous the attribute subsetting that we covered last week (example above).\n\nTopological relationships\nWhen filtering based on attributes, we use conditions (for example, lifeExp &gt;= 80). In spatial subsetting, we use the relationships of objects to each other in space (topological relationships). These relationships are based on mathematical relationships, but can be more easily understood from visualizing them. The figure below shows how each relationship is satisfied.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\n\n\n\n\n\n\nTipst_intersects() and st_disjoint()\n\n\n\nNote that st_intersects()is a “catch-all” that contains the following relationships:\n\nst_touches()\nst_overlaps()\nst_contains() and st_contains_properly()\nst_covers() and st_covered_by()\nst_within()\n\nst_disjoint() is the opposite of st_intersects()\n\n\n\n\nExamples\nThere are many ways to spatially subset in R, so we will explore a few.\nAs an example we’ll work with the following two datasets from the spData package:\n\nnz: polygons representing the 16 regions of New Zealand\nnz_height: top 101 highest points in New Zealand\n\nWe’ll explore by trying to find all the high points in the region of Canterbury (shown in dark grey).\n\n\nCode\ntm_shape(nz) +\n  tm_polygons() +\ntm_shape(canterbury) +\n  tm_polygons(fill = \"darkgrey\") +\ntm_shape(nz_height) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\n\n\nBracket subsetting\nLike attribute subsetting, the command x[y, ] (equivalent to nz_height[canterbury, ]) subsets features of a target x using the contents of a source object y. Instead of y being a vector of class logical or integer, however, for spatial subsetting both x and y must be geographic objects. Specifically, objects used for spatial subsetting in this way must have the class sf or sfc: both nz and nz_height are geographic vector data frames and have the class sf, and the result of the operation returns another sf object representing the features in the target nz_height object that intersect with (in this case high points that are located within) the Canterbury region.\n\n# subset nz_heights to just the features that intersect Canterbury\nc_height1 &lt;- nz_height[canterbury, ]\n\nBy default bracket subsetting will filter to features in x that intersect features in y. However, we can use other topological relationships by changing options.\n\nnz_height[canterbury, , op = st_disjoint]\n\n\n\nst_filter()\nThe sf package also includes the function st_filter() which is analogous to dplyr::filter(). Using st_filter() we can perform spatial subsetting in the same format as using dplyr commands. The .predicate = argument allows us to define which topological relationship we would like to filter by (e.g. st_intersects(), st_disjoint()).\nThe results from this method are the identical to the method above.\n\n# subset to the features in Cantebury\nc_height2 &lt;- nz_height %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects) # define the topological relationship\n\n\n\nTopological operators (st_intersects())\nThe previous two methods either by default or explicitly use the argument st_intersects. All topological relationships have their own topological operators which are functions that evaluate whether or not features meet the specified condition (e.g. st_intersects()). These operators can be used for spatial subsetting, but are more complicated to use.\nThe output of st_intersects() and other topological operators is a sparse geometry binary predicate list (yikes!) that’s a list that defines whether or not each feature in x intersects y.\nThis can be converted into logical vector of TRUE and FALSE values which can then be used for filtering.\n\n# sparse binary predicate list\nnz_height_sgbp &lt;- st_intersects(x = nz_height, y = canterbury)\nnz_height_sgbp\n\nSparse geometry binary predicate list of length 101, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 1\n 6: 1\n 7: 1\n 8: 1\n 9: 1\n 10: 1\n\n# convert to logical vector\nnz_height_logical &lt;- lengths(nz_height_sgbp) &gt; 0\n\n# filter based on logical vector\nc_height3 = nz_height[nz_height_logical, ]\n\nNow let’s plot results from all three methods to confirm they gave the same results.\n\n\nCode\nmap1 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height1) +\n  tm_dots(fill = \"red\") +\n  tm_title(text = \"Bracket subsetting\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height2) +\n  tm_dots(fill = \"red\") +\n  tm_title(text = \"st_filter()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height3) +\n  tm_dots(fill = \"red\") +\n  tm_title(text = \"st_intersects()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nDistance relationships\nThe topological relationships we have been discussing are all binary (features either intersect or don’t). In some cases, it might be helpful to subset based on a distance to a feature. In these cases we can use the st_is_within_distance() to filter features. By default st_is_within_distance() will return a sparse geometry binary predicate list as in st_intersects() above. Instead, we can return a logical by setting sparse = FALSE.\n\n# find heights within 1000 km of Canterbury\nnz_height_logical &lt;- st_is_within_distance(nz_height, canterbury,\n                      dist = units::set_units(1000, \"km\"), # set distance\n                      sparse = FALSE) # return logical vector instead\n\nc_height4 &lt;- nz_height[nz_height_logical, ] # filter based on logical\n\nNow, we should see points appear that do not intersect Caterbury, but are within 1000 km.\n\n\nCode\n# additional high points should appear\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height4) +\n  tm_dots(fill = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-joins",
    "href": "course-materials/labs/week3.html#spatial-joins",
    "title": "Week 3: Lab",
    "section": "3. Spatial joins",
    "text": "3. Spatial joins\nJoins are a common way to link different data sources. Up until now, we have been performing joins using common attributes between data.frames. Last week, we saw that the same joins can be used on sf objects. However, we can also perform joins by using the spatial relationship of datasets.\nFirst, let’s remind ourselves of the different types of joins.\n\n\n\n\n\n\n\n\n\n\nSoftware Carpentry\n\n\nToplogical relationships\nWith spatial data, we can join based on the geometry columns using topological relationships using the st_join() function. By default st_join() will join based on geometries that intersect, but can accommodate other topological relationships by changing the join = argument. By default st_join() performs left joins, but can perform inner joins by setting left = FALSE.\n\n# specify join based on geometries x within y\nst_join(x, y, join = st_within)\n\n# specify inner join\nst_join(x, y, left = FALSE)\n\nLet’s consider the scenario where we would like to know which region each of the highest points is located in. We can left join the nz dataset (polygons of NZ’s regions) onto the nz_height dataset (points of highest points in the county).\n\nnz_height_left_join &lt;- st_join(nz_height, nz) %&gt;%\n  select(id = t50_fid, elevation, region = Name)\n\nhead(nz_height_left_join)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1204143 ymin: 5048309 xmax: 1389460 ymax: 5168749\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n       id elevation     region                geometry\n1 2353944      2723  Southland POINT (1204143 5049971)\n2 2354404      2820      Otago POINT (1234725 5048309)\n3 2354405      2830      Otago POINT (1235915 5048745)\n4 2369113      3033 West Coast POINT (1259702 5076570)\n5 2362630      2749 Canterbury POINT (1378170 5158491)\n6 2362814      2822 Canterbury POINT (1389460 5168749)\n\n\nNow we could use this data to summarize the number of highest point in each region!\n\nnz_height_left_join %&gt;%\n  group_by(region) %&gt;%\n  summarise(n_points = n()) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 7 × 2\n  region            n_points\n* &lt;chr&gt;                &lt;int&gt;\n1 Canterbury              70\n2 Manawatu-Wanganui        2\n3 Marlborough              1\n4 Otago                    2\n5 Southland                1\n6 Waikato                  3\n7 West Coast              22\n\n\n\n\nDistance-based joins\nSimilar to filtering, in some cases we may want to join datasets based on their proximity. Let’s see an example!\nWe’ll use the following two datasets from the spData package:\n\ncycle_hire: points representing cycle hire points across London with information on number of bikes available\ncycle_hire_osm: dataset downloaded from OpenStreetMaps representing cycle hire points across London with information on the capacity of the hire point\n\nIn this example, we would like join the capacity attribute from the cycle_hire_osm dataset to the cycle_hire dataset. Unfortunately it appears that the points from the two datasets do not perfectly align.\n\n# check whether or not points overlap\nif(any(st_intersects(cycle_hire, cycle_hire_osm, sparse = FALSE)) == TRUE){\n  print(\"points overlap\")\n} else{\n  warning(\"points don't overlap\")\n}\n\nWarning: points don't overlap\n\n\n\n\nCode\ntmap_mode(\"view\")\n\ntm_shape(cycle_hire) +\n  tm_symbols(fill = \"red\" , fill_alpha = 0.2)+\ntm_shape(cycle_hire_osm) +\n  tm_symbols(fill = \"blue\", fill_alpha = 0.2)\n\n\n\n\n\n\nWe can join by again using st_join(), but this time including a distance threshold using st_is_within_distance.\n\ncycle_hire_join &lt;- st_join(cycle_hire, cycle_hire_osm,\n                           st_is_within_distance,\n                           dist = units::set_units(20, \"m\")) %&gt;%\n                   select(id, capacity)\n\nhead(cycle_hire_join)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1975742 ymin: 51.49313 xmax: -0.08460569 ymax: 51.53006\nGeodetic CRS:  WGS 84\n  id capacity                     geometry\n1  1        9  POINT (-0.1099705 51.52916)\n2  2       27  POINT (-0.1975742 51.49961)\n3  3       NA POINT (-0.08460569 51.52128)\n4  4       NA  POINT (-0.1209737 51.53006)\n5  5       NA   POINT (-0.156876 51.49313)\n6  6        8  POINT (-0.1442289 51.51812)\n\n\nLet’s build some checks to diagnose the output.\n\nif(nrow(cycle_hire) == nrow(cycle_hire_join)){\n  print(\"join matches original data dimensions\")\n} else {\n  warning(\"join does not match orginal data dimensions\")\n  print(paste(\"cycle_hire has\", nrow(cycle_hire), \"rows\"))\n  print(paste(\"cycle_hire_join has\", nrow(cycle_hire_join), \"rows\"))\n}\n\nWarning: join does not match orginal data dimensions\n\n\n[1] \"cycle_hire has 742 rows\"\n[1] \"cycle_hire_join has 762 rows\"\n\n\nNote that the joined result has more rows than the target data. This is because some of the cycle hire stations in cycle_hire have multiple matches in cycle_hire_osm. Depending on your project, you would need to think about how to resolve this. In this case, we can aggregate the values for the overlapping points by taking the mean.\n\n# aggregate values for single points in cycle_hire\ncycle_hire_join &lt;- cycle_hire_join %&gt;%\n  group_by(id) %&gt;%\n  summarise(capacity = mean(capacity))\n\n# check results\nif(nrow(cycle_hire) == nrow(cycle_hire_join)){\n  print(\"join matches original data dimensions\")\n} else {\n  warning(\"join does not match orginal data dimensions\")\n  print(paste(\"cycle_hire has\", nrow(cycle_hire), \"rows\"))\n  print(paste(\"cycle_hire_join has\", nrow(cycle_hire_join), \"rows\"))\n}\n\n[1] \"join matches original data dimensions\"\n\n\n\n\n4. Spatial aggregation\nAs with aggregating attribute data, spatial data aggregation condenses data: outputs have few rows than inputs. Think about our friend group_by() %&gt;% summarise()! Spatial aggregation is the same.\nLet’s consider the example where we would like to make a map of the mean elevation of high points within each region of NZ. There are several ways to do this, but the first thing we should be thinking is that we will need to retain the geometry column of the NZ regions in order to make a map.\nThe first approach is by leveraging st_join() again. But in this example, we want the nz object to be the target to maintain the geometries we need.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nnz_elevation &lt;- st_join(x = nz, y = nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarise(elevation = mean(elevation, na.rm = TRUE))\n\nThe second approah uses the aggregate() function. Although it doesn’t follow the dplyr piping convention we’re used to, aggregate() will come in handy later, so it’s nice to see how it works.\nThe syntax looks slightly different, in this case the argument x is the data we would like to aggregate (nz_height) and the by argument specifies the geometry that you would like to group by. The FUN argument defines the function that you would like to use to aggregate, in this case mean.\n\nnz_elevation &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\n\n\n\nCode\nmap1 &lt;- tm_shape(nz_elevation) +\n  tm_polygons(fill = \"elevation\",\n              fill.legend= tm_legend(title = \"Mean elevation (meters)\")) +\n  tm_title(text = \"group_by()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(nz_elevation) +\n  tm_polygons(fill = \"elevation\",\n              fill.legend = tm_legend(title = \"Mean elevation (meters)\")) +\n  tm_title(text = \"aggregate()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteJoining incongruent layers\n\n\n\nAn important consideration when aggregating spatial objects is that geometries are congruent, meaning aggregating zones align with the units being aggregating. This is often the case with administrative boundaries where units are sub-units of one another (e.g. countries &gt; states &gt; counties).\nHowever, in some cases the aggregating zones do not share common borders with the target. This is an issue because it’s not clear how to aggregate underlying data. Areal interpolation overcomes this issue by transferring values to another using algorithms, including simple area weighted approaches.\n\nincongruent &lt;- spData::incongruent\naggregating_zones &lt;- spData::aggregating_zones\n\ntm_shape(incongruent) +\n  tm_polygons(fill = \"lightblue\",\n              col = \"blue\") +\n  tm_shape(aggregating_zones) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n\n\n\nThe simplest useful method for this is area weighted spatial interpolation, which transfers values from the incongruent object to a new column in aggregating_zones in proportion with the area of overlap: the larger the spatial intersection between input and output features, the larger the corresponding value. This is implemented in st_interpolate_aw(), as demonstrated in the code chunk below.\n\n# select just the value to be aggregated\nincongruent_2 &lt;- incongruent %&gt;%\n  select(value)\n\n# use area-weighted interpolation to aggregrate the \"value\" attribute\naggregating_zones_area_weighted &lt;- st_interpolate_aw(incongruent_2, aggregating_zones, extensive = TRUE)\n\nWarning in st_interpolate_aw.sf(incongruent_2, aggregating_zones, extensive =\nTRUE): st_interpolate_aw assumes attributes are constant or uniform over areas\nof x\n\naggregating_zones_area_weighted$value\n\n[1] 19.61613 25.66872"
  },
  {
    "objectID": "course-materials/labs/week3.html#aggregating",
    "href": "course-materials/labs/week3.html#aggregating",
    "title": "Week 3: Lab",
    "section": "1. Aggregating",
    "text": "1. Aggregating\n\nGeometry unions\nWe may come across situations where we would like to summarize data across several spatial units. For example, summarizing the population across states within regions of the US. Based on our experience with tabular data, we can summarize attributes by using group_by() %&gt;% summarize(). Alternatively we can use the aggregate() function. So far this should look familiar, but if plot the outputs we notice that these functions have also aggregated the underlying geometries.\n\n# load US states\nus_states &lt;- spData::us_states\n\n# summarize total population within each region\nregions1 &lt;- us_states %&gt;%\n  group_by(REGION) %&gt;%\n  summarise(population = sum(total_pop_15, na.rm = TRUE))\n\n# alternative approach\nregions2 &lt;- aggregate(x = us_states[, \"total_pop_15\"], # data and attribute to be aggregated\n                      by = list(us_states$REGION), # attribute to aggregate by\n                      FUN = sum, na.rm = TRUE) # aggregating function\n\n\n\nCode\nmap1 &lt;- tm_shape(us_states) +\n  tm_polygons(fill = \"total_pop_15\",\n              fill.legend  = tm_legend(\"Total population\")) +\n  tm_title(text = \"US States\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(regions1) +\n  tm_polygons(fill = \"population\",\n              fill.legend  = tm_legend(\"Total population\")) +\n  tm_title(text = \"group_by()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(regions2) +\n  tm_polygons(fill = \"total_pop_15\",\n              fill.legend  = tm_legend(\"Total population\")) +\n  tm_title(text = \"aggregate()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nWhat’s going on here? Behind the scenes, R is using st_union() to combine geometries within each group. We can also use st_union() to combine any pair of spatial objects.\n\n# combine geometries of western states\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nus_west_union &lt;- st_union(us_west)\n\n# combine geometries of Texas and western states\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\n\n\n\nCode\nmap1 &lt;- tm_shape(us_west) +\n  tm_polygons() +\n  tm_title(text = \"western states\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(us_west_union) +\n  tm_polygons() +\n  tm_title(text = \"western states union\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(texas) +\n  tm_polygons() +\n  tm_title(text = \"TX\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap4 &lt;- tm_shape(texas_union) +\n  tm_polygons() +\n  tm_title(text = \"TX + western states union\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, map4, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week3.html#filtering",
    "href": "course-materials/labs/week3.html#filtering",
    "title": "Week 3: Lab",
    "section": "2. Filtering",
    "text": "2. Filtering\n\nBuffers\nIn the previous section, we saw that we can filter spatial objects based on their proximity using st_is_within_distance(). An alternative approach to finding items that are within a set distance would be to expand the geometry and then intersect with objects of interest. We can change the size of geometries by creating a “buffer” using st_buffer()\nIn this example, let’s create 5 km and 50 km buffers around the Seine.\n\nseine_buffer_5km &lt;- st_buffer(seine, dist = 5000)\nseine_buffer_50km = st_buffer(seine, dist = 50000)\n\n\n\nCode\nmap1 &lt;- tm_shape(seine_buffer_5km) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_title(text = \"5km buffer\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(seine_buffer_50km) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_title(text = \"50km buffer\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nThe Seine is actually comprised of multiple geometries. To make things simpler, and more look better on maps, we can combine geometries using our new friend st_union!\n\nseine_union &lt;- st_union(seine_buffer_50km)\n\n\n\nCode\ntm_shape(seine_union) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_title(text= \"50km buffer\")\n\n\n\n\n\n\n\n\n\nNow let’s see an example of using a buffer to find objects within a set distance. Here, we’ll repeat our previous example of finding points within 100 km of Canterbury. And check to see if the results match our previous approach!\n\n# create buffer around high points\nnz_height_buffer &lt;- st_buffer(nz_height, dist = 1000000)\n\n# filter buffered points with those that intersect Canterbury\nc_height5 &lt;- nz_height_buffer %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects)\n\n# check to see if results match previous approach\nif(nrow(c_height4) == nrow(c_height5)){\n  print(\"results from buffer approach match st_is_within_distance() approach\")\n} else{\n  warning(\"approaches giving different results\")\n}\n\n[1] \"results from buffer approach match st_is_within_distance() approach\"\n\n\n\n\nClipping\nBeyond filtering observations based on their spatial proximity, in some cases we might want to filter (or remove) portions of geometries. Spatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features.\nClipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents.\nThere are several options for clipping geometries:\n\nst_intersection(x, y) - portion of x intersecting y\nst_difference(x, y) - portion of x not intersecting y\nst_difference(y, x) - portion of y not intersecting x\nst_union(x, y) - portion either in x or y\nst_sym_difference(x, y) - portions of x and y that do not intersect\n\nTo illustrate the concept, we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one.\n\nx &lt;- st_sfc(st_point(c(0, 1))) %&gt;%\n  st_buffer(dist = 1) %&gt;%\n  st_sf() \n\ny &lt;- st_sfc(st_point(c(1, 1))) %&gt;%\n  st_buffer(dist = 1) %&gt;%\n  st_sf()\n\nintersection &lt;- st_intersection(x, y) \ndifference_x_y &lt;- st_difference(x, y) \ndifference_y_x &lt;- st_difference(y, x)\nunion &lt;- st_union(x, y)\nsym_difference &lt;- st_sym_difference(x, y) \n\n\n\nCode\nx &lt;- x %&gt;% st_set_crs(4326)\ny &lt;- y %&gt;% st_set_crs(4326)\nbbox &lt;- st_union(x, y)\n\nmap1 &lt;- tm_shape(x, bbox = bbox) +\n  tm_borders(col = \"red\") +\n  tm_shape(y) +\n  tm_borders(col = \"blue\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- map1 +\n  tm_shape(intersection, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_intersection()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- map1 +\n  tm_shape(difference_x_y, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_difference(x,y)\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap4 &lt;- map1 +\n  tm_shape(difference_y_x, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_difference(y,x)\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap5 &lt;- map1 +\n  tm_shape(union, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_union()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap6 &lt;- map1 +\n  tm_shape(sym_difference, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_sym_diffference()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, map4, map5, map6, nrow = 2)\n\n\n\n\n\n\n\n\n\nNow let’s see how we could use these updated geometries for filtering. Extending this simple example, we’ll create 100 random points. We want to find the points that intersect both x and y. We have a few different approaches that all produce the same results.\n\n# create random points\nbb &lt;- st_bbox(bbox) # create bounding box of x and y\nbox &lt;- st_as_sfc(bb)\np &lt;- st_sample(x = box, size = 100) %&gt;% # randomly sample the bounding box\n  st_as_sf()\n\n# find intersection of x and y\nx_and_y &lt;- st_intersection(x, y)\nx_and_y &lt;- st_make_valid(x_and_y)\n\n\n# filter points\n# first approach: bracket subsetting\np_xy1 = p[x_and_y, ]\n\n# second approach: st_filter()\np_xy2 &lt;- p %&gt;%\n  st_filter(., x_and_y)\n\n# third approach: st_intersection()\np_xy3 = st_intersection(p, x_and_y)\n\n\n\nCode\nmap2 &lt;- map1 +\n  tm_shape(p) +\n  tm_dots(alpha = 0.5) +\n  tm_layout(main.title = \"original\")\n\nmap3 &lt;- map2 +\n  tm_shape(p_xy1) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"bracket subsetting\")\n\nmap4 &lt;- map2 +\n  tm_shape(p_xy2) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"st_filter()\")\n\nmap5 &lt;- map2 +\n  tm_shape(p_xy3) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"st_intersection()\")\n\ntmap_arrange(map2, map3, map4, map5, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week3.html#making-life-easier",
    "href": "course-materials/labs/week3.html#making-life-easier",
    "title": "Week 3: Lab",
    "section": "3. Making life easier!",
    "text": "3. Making life easier!\nWorking with, and especially plotting, complex spatial objects can become quite cumbersome. This section we’ll see a few ways to create and manipulate geometries to make them easier to work with.\n\nCentroids\nCentroids are basically the center of spatial objects. They can be a handy way to display summary statistics (or we might actually use them for analysis – e.g. to find the distance between polygons).\nThere are many ways to that we might want to define the “center” of an object, but the most common is the geographic centroid which is the center of mass of a spatial object. The geographic centroid can be found using st_centroid().\nSometimes the geographic centroid may fall outside of the boundaries of the object (picture the centroid of a doughnut!). While correct, it might be confusing on a map, so we can use st_point_on_surface() to ensure that the centroid is placed onto the object.\nLet’s inspect a few examples!\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\n\n\nCode\nmap1 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(nz_centroid) +\n  tm_symbols(fill = \"red\", fill_alpha = 0.5) +\n  tm_shape(nz_pos) +\n  tm_symbols(fill = \"blue\", fill_alpha = 0.5)+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(seine) +\n  tm_lines() +\n  tm_shape(seine_centroid) +\n  tm_symbols(fill = \"red\", fill_alpha = 0.5) +\n  tm_shape(seine_pos) +\n  tm_symbols(fill = \"blue\", fill_alpha = 0.5)+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nSimplification\nWe may also want to simplify a geometry to make it easier to plot or take up less storage. There are several different algorithms for simplifying geometries, check out Geocomputation with R for more examples. The sf packages uses the Douglas-Peucker algorithm within the st_simplify() function.\nLet’s see an example!\n\nseine_simple &lt;- st_simplify(seine, dTolerance = 2000)  # 2000 m\n\n\n\nCode\nmap1 &lt;- tm_shape(seine) +\n  tm_lines() +\n  tm_title(\"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(seine_simple) +\n  tm_lines() +\n  tm_title(\"st_simplify()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week2.html",
    "href": "course-materials/labs/week2.html",
    "title": "Week 2 Lab",
    "section": "",
    "text": "NoteSource Materials\n\n\n\nThe following materials are modified from Chapter 3 of Geocomputation with R and the tmap book.\nIn this lab, we’ll explore the basics of manipulating vector data in R using the sf package."
  },
  {
    "objectID": "course-materials/labs/week2.html#set-up",
    "href": "course-materials/labs/week2.html#set-up",
    "title": "Week 2 Lab",
    "section": "1. Set up",
    "text": "1. Set up\nInstall a new package to take advantage of some preloaded data.\n\ninstall.packages(\"spData\")\n\nLet’s load all necessary packages:\n\nrm(list = ls())\nlibrary(sf) # for handling vector data\nlibrary(tmap) # for making maps\nlibrary(tidyverse) # because we love the tidyverse\nlibrary(spData) # preloaded spatial data"
  },
  {
    "objectID": "course-materials/labs/week2.html#simple-features-in-sf",
    "href": "course-materials/labs/week2.html#simple-features-in-sf",
    "title": "Week 2 Lab",
    "section": "2. Simple features in sf",
    "text": "2. Simple features in sf\nSimple features is a hierarchical data model that represents a wide range of geometry types. The sf package can represent all common vector geometry types:\n\npoints\nlines\npolygons\nand their respective ‘multi’ versions\n\nsfprovides the same functionality that the sp, rgdal, and rgeos packages provided, but is more intuitive because it builds on the tidy data model and works well with the tidyverse. sf represents spatial objects as “simple feature” objects by storing them as a data frame with the geographic data stored in a special column (usually named geom or geometry).\n\nSimple features from scratch\nLet’s start by looking at how we can construct a sf object. Typically we will load sf objects by reading in data. However, it can be helpful to see how sf objects are created from scratch.\nFirst, we create a geometry for London by supplying a point and coordinate reference system.\n\n# create st_point with longitude and latitude for London\n# simple feature geometry\nlondon_point &lt;- st_point(c(0.1, 51.5))\n\n# add coordinate reference system\n# simple feature collection\nlondon_geom &lt;- st_sfc(london_point, crs = 4326)\n\nThen, we supply some non-geographic attributes by creating a data frame with attributes about London.\n\n# create data frame of attributes about London\nlondon_attrib &lt;- data.frame(\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nAnd we attach the simple feature collection and data frame to create a sf object. Check out the class of the new object we created.\n\n# combine geometry and data frame\n# simple feature object\nlondon_sf &lt;- st_sf(london_attrib, geometry = london_geom)\n\n# check class\nclass(london_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can also check out what the CRS looks like:\n\nst_crs(london_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nst_crs(london_sf)$IsGeographic\n\n[1] TRUE\n\nst_crs(london_sf)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n\n\nExisting sf object\nNow let’s look at an existing sf object representing countries of the world:\n\nworld &lt;- spData::world\nclass(world)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(world)\n\n[1] 177  11\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nWe can see that this object contains both spatial data (geom column) and attributes about those geometries. We can perform operations on the attribute data, just like we would with a normal data frame.\n\nsummary(world$lifeExp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  50.62   64.96   72.87   70.85   76.78   83.59      10 \n\n\nThe geometry column is “sticky”, meaning it will stick around unless we explicitly get rid of it. For example, dplyr’s select() function won’t get rid of it.\n\nworld_df &lt;- world %&gt;%\n  select(-geom) #doesn't actually remove the geom column\n\ncolnames(world_df) # geom still shows up as a column\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nTo drop the geom column and convert this sf object into a data frame, we need to drop the geometry column using the st_drop_geometry().\n\nworld_df &lt;- st_drop_geometry(world)\nclass(world_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_df)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\"\n\nncol(world)\n\n[1] 11\n\nncol(world_df)\n\n[1] 10\n\n\n\n\n\n\n\n\nTipsf syntax\n\n\n\nNote that all functions in the sf package start with the prefix st_ NOT sf_. Why? st_ stands for “spatiotemporal” as in data that varies in space and time."
  },
  {
    "objectID": "course-materials/labs/week2.html#coordinate-reference-systems-and-projections",
    "href": "course-materials/labs/week2.html#coordinate-reference-systems-and-projections",
    "title": "Week 2 Lab",
    "section": "3. Coordinate reference systems and projections",
    "text": "3. Coordinate reference systems and projections\nR handles coordinate reference systems using multiple formats:\n\nan identifying string specifying the authority and code such as EPSG:4325\n\nthese need to be passed as strings\nsf will accept the four digit code as an integer\n\nproj4strings are now outdated, but you might see them around\n\nfor example, +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\n\nReprojecting data\nIn some cases we will be working with data which is represented with different coordinate reference systems (CRS). Whenever we work with multiple spatial data objects, we need to check that the CRSs match.\nLet’s create another sf object for London, but now represented with a project coordinate system.\n\nlondon_proj = data.frame(x = 530000, y = 180000) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:27700\")\n\nWe can check the CRS of any data using the st_crs() function.\n\nst_crs(london_proj)\n\nCoordinate Reference System:\n  User input: EPSG:27700 \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThis is a lot of information to read, so if we wanted to use this point with our other London point, we need to check to see if they are using the same CRS.\n\nst_crs(london_proj) == st_crs(london_sf)\n\n[1] FALSE\n\n\nTo transform the CRS of a dataset, we use the st_transform() function. In the crs argument, we need to specify the coordinate reference system. We can do this by either supplying a CRS code or specifying the CRS of another dataset using the st_crs() function.\n\nlondon_sf_transform &lt;- st_transform(london_sf, crs = st_crs(london_proj))\n\nNow if we check, the CRS between the two datasets should match\n\nif(st_crs(london_sf_transform) == st_crs(london_proj)){\n  print(\"it's a match!\")\n} else {\n  print(\"still not a match\")\n}\n\n[1] \"it's a match!\"\n\n\n\n\n\n\n\n\nTipBuilding beautiful workflows\n\n\n\nHopefully we’re already thinking about how we could build checking coordinate reference systems into our workflows.\nFor example, we could add code like the following that transforms the CRS of dataset2 to match dataset1 and prints out a warning message.\n\nif(st_crs(dataset1) != st_crs(dataset2)){\n  warning(\"coordinate refrence systems do not match\")\n  dataset2 &lt;- st_transform(dataset1, crs = st_crs(dataset1))\n}\n\n\n\n\n\nChanging map projections\nRemember that whenever we make a map we are trying to display three dimensional data with only two dimensions. To display 3D data in 2D, we use projections. Which projection you use can have big implications for how you display information.\nTo the projection of our data, we could:\n\nreproject the underlying data\nor in tmap we can specify the projection we want the map to use\n\nLet’s compare global maps using two different projections:\n\nEqual Earth is an equal-area pseudocylindrical projection (EPSG 8857)\nMercator is a conformal cylindrical map that preserves angles (EPSG 3395)\n\n\ntm_shape(world, crs = 8857) +\n  tm_fill(fill = \"area_km2\")\n\n\n\n\n\n\n\ntm_shape(world, crs = 3395) +\n  tm_fill(fill = \"area_km2\")"
  },
  {
    "objectID": "course-materials/labs/week2.html#vector-attribute-subsetting",
    "href": "course-materials/labs/week2.html#vector-attribute-subsetting",
    "title": "Week 2 Lab",
    "section": "4. Vector attribute subsetting",
    "text": "4. Vector attribute subsetting\nOften we’ll want to manipulate sf objects in the same ways as we might with tabular data in data frames. The great thing about the simple features data model, is we can largely treat spatial objects the same as data frames.\n\ndplyr functions!\nThis means that we can use all of our favorite dplyr functions on sf objects – yay!\nWe can select columns…\n\nworld %&gt;%\n  select(name_long, pop)\n\nOr remove columns…\n\nworld %&gt;%\n  select(-subregion, -area_km2)\n\nOr select AND rename columns\n\nworld %&gt;%\n  select(name = name_long, population = pop)\n\nOr filter observations based on variables\n\nworld1 &lt;- world %&gt;%\n  filter(area_km2 &lt; 10000)\n\nsummary(world1$area_km2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2417    4412    6207    5986    7614    9225 \n\nworld2 &lt;- world %&gt;%\n  filter(lifeExp &gt;= 80)\n\nnrow(world2)\n\n[1] 24\n\n\n\n\nChaining commands with pipes\nBecause we can use dplyr functions with sf objects, we can chain together commands using the pipe operator.\nLet’s try to find the country in Asia with the highest life expectancy\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;%\n  select(name_long, continent, lifeExp) %&gt;%\n  slice_max(lifeExp) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 1 × 3\n  name_long continent lifeExp\n* &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 Japan     Asia         83.6\n\n\n\n\nVector attribute aggregation\nAggregation is the process of summarizing data with one or more ‘grouping’ variables. For example, using the ‘world’ which provides information on countries of the world, we might want to aggregate to the level of continents. It is important to note that aggregating data attributes is a different process from aggregating geographic data, which we will cover later.\nLet’s try to find the total population within each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE)) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 8 × 2\n  continent               population\n* &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811\n\n\nLet’s also find the total area and number of countries in each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 8 × 4\n  continent               population  area_km2 n_countries\n* &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;\n1 Africa                  1154946633 29946198.          51\n2 Antarctica                       0 12335956.           1\n3 Asia                    4311408059 31252459.          47\n4 Europe                   669036256 23065219.          39\n5 North America            565028684 24484309.          18\n6 Oceania                   37757833  8504489.           7\n7 Seven seas (open ocean)          0    11603.           1\n8 South America            412060811 17762592.          13\n\n\nBuilding on this, let’s find the population density of each continent, find the continents with highest density and arrange by the number of countries. We’ll drop the geometry column to speed things up.\n\nworld %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  mutate(density = round(population/area_km2)) %&gt;%\n  slice_max(density, n = 3) %&gt;%\n  arrange(desc(n_countries))\n\n# A tibble: 3 × 5\n  continent population  area_km2 n_countries density\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;   &lt;dbl&gt;\n1 Africa    1154946633 29946198.          51      39\n2 Asia      4311408059 31252459.          47     138\n3 Europe     669036256 23065219.          39      29"
  },
  {
    "objectID": "course-materials/labs/week2.html#joins-with-vector-attributes",
    "href": "course-materials/labs/week2.html#joins-with-vector-attributes",
    "title": "Week 2 Lab",
    "section": "5. Joins with vector attributes",
    "text": "5. Joins with vector attributes\nA critical part of many data science workflows is combining data sets based on common attributes. In R, we do this using multiple join functions, which follow SQL conventions.\nLet’s start by looking a data set on national coffee production from the spData package:\n\ncoffee_data &lt;- spData::coffee_data\nhead(coffee_data)\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\nIt appears that coffee_data contains information on the amount of coffee produced in 2016 and 2017 from a subset of countries.\n\nnrow(coffee_data)\n\n[1] 47\n\nnrow(world)\n\n[1] 177\n\n\nThe coffee production dataset does not include any spatial information, so If we wanted to make a map of coffee production, we would need to combine coffee_data with the world dataset. We do this by joining based on countries’ names.\n\nworld_coffee &lt;- left_join(world, coffee_data, by = \"name_long\")\n\nnames(world_coffee)\n\n [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n [4] \"region_un\"              \"subregion\"              \"type\"                  \n [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n[10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n[13] \"coffee_production_2017\"\n\n\nAnd plot what this looks like…\n\ntm_shape(world_coffee) +\n  tm_fill(\n    \"coffee_production_2017\",\n    fill.legend = tm_legend(title = \"Coffee production (2017)\")\n  )\n\n[tip] Consider a suitable map projection, e.g. by adding `+ tm_crs(\"auto\")`.\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nBy using a left join, our previous result added the coffee production information onto all countries of the world. If we just wanted to keep countries that do have coffee data, we could use an inner join:\n\nworld_coffee_inner &lt;- inner_join(world, coffee_data, by = \"name_long\")\n\nLet’s build ourselves a warning message to make sure we don’t lose any data because of incomplete matches.\n\nif (nrow(world_coffee_inner) != nrow(coffee_data)) {\n  warning(\"inner join does not match original data. potential data loss during join\")\n}\n\nWarning: inner join does not match original data. potential data loss during\njoin\n\n\nIt looks like we lost some countries with coffee data, so let’s figure out what’s going on. We can find rows that didn’t match using the setdiff() function.\n\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\"             \n\n\nWe see that one of the issues is that the two data sets use different naming conventions for the Democratic Republic of the Congo. We can use a string matching function to figure out what the DRC is called in the world data set.\n\n# search for the DRC in the world dataset\ndrc &lt;- stringr::str_subset(world$name_long, \"Dem*.+Congo\")\n\nNow we can update the coffee data set with the matching name for the DRC:\n\ncoffee_data$name_long[stringr::str_detect(coffee_data$name_long, \"Congo\")] &lt;- drc\n\nAnd we can try the inner join again and hopefully the DRC now matches:\n\nworld_coffee_inner &lt;- inner_join(world, coffee_data , by = \"name_long\")\n\n# update warning message conditional to include the mismatch for \"others\"\nif (nrow(world_coffee_inner) != nrow(coffee_data) & setdiff(coffee_data$name_long, world_coffee_inner$name_long) != \"Others\") {\n  warning(\"inner join does not match original data. potential data loss during join\")\n}\n\nLet’s visualize what a the inner join did to our spatial object.\n\ntm_shape(world_coffee_inner) +\n  tm_polygons(fill = \"coffee_production_2017\",\n              fill.legend = tm_legend(title = \"Coffee production (2017)\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCritical thinking question\n\n\n\nWhat happens if we left join a sf object onto a data frame?\n\ncoffee_world &lt;- left_join(coffee_data, world, by = \"name_long\")\nclass(coffee_world)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(coffee_world)\n\n [1] \"name_long\"              \"coffee_production_2016\" \"coffee_production_2017\"\n [4] \"iso_a2\"                 \"continent\"              \"region_un\"             \n [7] \"subregion\"              \"type\"                   \"area_km2\"              \n[10] \"pop\"                    \"lifeExp\"                \"gdpPercap\"             \n[13] \"geom\"                  \n\n\nWe end up with a data frame!"
  },
  {
    "objectID": "course-materials/labs/week9.html#data",
    "href": "course-materials/labs/week9.html#data",
    "title": "Week 9: Lab",
    "section": "1. Data",
    "text": "1. Data\n\nLandsat 5 Thematic Mapper\n\nLandsat 5\n1 scene from September 25, 2007\nBands: 1, 2, 3, 4, 5, 7\nCollection 2 surface reflectance product\n\nData files:\n\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B1.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B2.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B3.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B4.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B5.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B7.tif\n\n\n\nStudy area\nPolygon representing southern Santa Barbara county\nData file: SB_county_south.shp\n\n\nTraining data\nPolygons representing sites with training data - type: character string with land cover type\nData file: trainingdata.shp"
  },
  {
    "objectID": "course-materials/labs/week9.html#set-up",
    "href": "course-materials/labs/week9.html#set-up",
    "title": "Week 9: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nTo train our classification algorithm and plot the results, we’ll use the rpart and rpart.plot packages.\n\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\n\nLet’s load all necessary packages:\n\nlibrary(sf) # vector data\nlibrary(terra) # raster data\nlibrary(here) # file path management\nlibrary(tidyverse)\nlibrary(rpart) # recursive partitioning and regression trees\nlibrary(rpart.plot) # plotting for rpart\nlibrary(tmap) # map making"
  },
  {
    "objectID": "course-materials/labs/week9.html#load-landsat-data",
    "href": "course-materials/labs/week9.html#load-landsat-data",
    "title": "Week 9: Lab",
    "section": "2. Load Landsat data",
    "text": "2. Load Landsat data\nLet’s create a raster stack. Each file name ends with the band number (e.g. B1.tif).\n\nNotice that we are missing a file for band 6\nBand 6 corresponds to thermal data, which we will not be working with for this lab\n\nTo create a raster stack, we will create a list of the files that we would like to work with and read them all in at once using the terra::rast() function. We’ll then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n# list files for each band, including the full file path\nfilelist &lt;- list.files(here::here(\"data\", \"landsat-data\"), full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat &lt;- rast(filelist)\n\n# update layer names to match band\nnames(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")"
  },
  {
    "objectID": "course-materials/labs/week9.html#load-study-area",
    "href": "course-materials/labs/week9.html#load-study-area",
    "title": "Week 9: Lab",
    "section": "3. Load study area",
    "text": "3. Load study area\nWe want to constrain our analysis to the southern portion of the county where we have training data, so we’ll read in a file that defines the area we would like to study.\n\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(here::here(\"data\", \"SB_county_south.shp\")) %&gt;%\n      st_transform(SB_county_south, crs = crs(landsat))\n\n\n\nCode\ntm_shape(SB_county_south) +\n  tm_borders()"
  },
  {
    "objectID": "course-materials/labs/week9.html#crop-and-mask-landsat-data-to-study-area",
    "href": "course-materials/labs/week9.html#crop-and-mask-landsat-data-to-study-area",
    "title": "Week 9: Lab",
    "section": "4. Crop and mask Landsat data to study area",
    "text": "4. Crop and mask Landsat data to study area\nNow, we can crop and mask the Landsat data to our study area.\n\nWhy? This reduces the amount of data we’ll be working with and therefore saves computational time\nBonus: We can also remove any objects we’re no longer working with to save space\n\n\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, SB_county_south, landsat_cropped)\n\nplotRGB(landsat_masked, r = 3, g = 2, b = 1, stretch = \"lin\")"
  },
  {
    "objectID": "course-materials/labs/week9.html#convert-landsat-values-to-reflectance",
    "href": "course-materials/labs/week9.html#convert-landsat-values-to-reflectance",
    "title": "Week 9: Lab",
    "section": "5. Convert Landsat values to reflectance",
    "text": "5. Convert Landsat values to reflectance\nNow we need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\nIn this case, we are working with Landsat Collection 2.\n\nThe valid range of pixel values for this collection goes from 7,273 to 43,636…\n\nwith a multiplicative scale factor of 0.0000275\nwith an additive scale factor of -0.2\n\n\nLet’s reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%!\n\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat &lt;- (landsat * 0.0000275 - 0.2) * 100\n\n# check values are 0 - 100\nsummary(landsat)\n\n      blue           green            red             NIR       \n Min.   : 1.11   Min.   : 0.74   Min.   : 0.00   Min.   : 0.23  \n 1st Qu.: 2.49   1st Qu.: 2.17   1st Qu.: 1.08   1st Qu.: 0.75  \n Median : 3.06   Median : 4.59   Median : 4.45   Median :14.39  \n Mean   : 3.83   Mean   : 5.02   Mean   : 4.92   Mean   :11.52  \n 3rd Qu.: 4.63   3rd Qu.: 6.76   3rd Qu.: 7.40   3rd Qu.:19.34  \n Max.   :39.42   Max.   :53.32   Max.   :56.68   Max.   :57.08  \n NA's   :39856   NA's   :39855   NA's   :39855   NA's   :39856  \n     SWIR1           SWIR2      \n Min.   : 0.10   Min.   : 0.20  \n 1st Qu.: 0.41   1st Qu.: 0.60  \n Median :13.43   Median : 8.15  \n Mean   :11.88   Mean   : 8.52  \n 3rd Qu.:18.70   3rd Qu.:13.07  \n Max.   :49.13   Max.   :48.07  \n NA's   :42892   NA's   :46809"
  },
  {
    "objectID": "course-materials/labs/week9.html#training-classifier",
    "href": "course-materials/labs/week9.html#training-classifier",
    "title": "Week 9: Lab",
    "section": "6. Training classifier",
    "text": "6. Training classifier\nLet’s begin by extracting reflectance values for training data!\nWe will load the shapefile identifying locations within our study area as containing one of our 4 land cover types.\n\n# read in and transform training data\ntraining_data &lt;- st_read(here::here( \"data\", \"trainingdata.shp\")) %&gt;%\n  st_transform(., crs = crs(landsat))\n\nNow, we can extract the spectral reflectance values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n# extract reflectance values at training sites\ntraining_data_values &lt;- terra::extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %&gt;%\n                    mutate(type = as.factor(type)) # convert landcover type to factor\n\nNext, let’s train the decision tree classifier!\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are).\n\nThe rpart() function implements the CART algorithm\nThe rpart() function needs to know the model formula and training data you would like to use\nBecause we are performing a classification, we set method = \"class\"\nWe also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\n\n# establish model formula\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n                          data = SB_training_data,\n                          method = \"class\",\n                          na.action = na.omit)\n\nTo understand how our decision tree will classify pixels, we can plot the results!\n\n\n\n\n\n\nTipInterpreting decision trees\n\n\n\nNote: The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\n\n\nCode\n# plot decision tree\nprp(SB_decision_tree)"
  },
  {
    "objectID": "course-materials/labs/week9.html#classify-image",
    "href": "course-materials/labs/week9.html#classify-image",
    "title": "Week 9: Lab",
    "section": "7. Classify image",
    "text": "7. Classify image\nNow that we have a rule set for classifying spectral reflectance values into landcover types, we can apply the classifier to identify the landcover type in each pixel.\nThe terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n# classify image based on decision tree\nSB_classification &lt;- terra::predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)\n\n[1] \"green_vegetation\" \"soil_dead_grass\"  \"urban\"            \"water\""
  },
  {
    "objectID": "course-materials/labs/week9.html#plot-results",
    "href": "course-materials/labs/week9.html#plot-results",
    "title": "Week 9: Lab",
    "section": "8. Plot results",
    "text": "8. Plot results\nNow we can plot the results and check out our land cover map!\n\n\nCode\n# plot results\ntm_shape(SB_classification) +\n  tm_raster(\n    col.scale = tm_scale(\n      values = c(\"#8DB580\", \"#F2DDA4\", \"#7E8987\", \"#6A8EAE\"),\n      labels = c(\"green vegetation\", \"soil/dead grass\", \"urban\", \"water\")\n    ),\n    col.legend = tm_legend(\"Landcover type\")\n  ) +\n  tm_title(text = \"Santa Barbara Landcover\") + \n  tm_layout( legend.position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\nDo these results make sense?"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the spatial modeling and analytic techniques of geographic information science to data science students. The emphasis is on deep understanding of spatial data models and the analytic operations they enable. Recognizing remotely sensed data as a key data type within environmental data science, this course will also introduce fundamental concepts and applications of remote sensing. In addition to this theoretical background, students will become familiar with libraries, packages, and APIs that support spatial analysis in R."
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nLecture: Tuesday/Thursday 8- 9:15 AM at Bren Hall 4016\nDiscussion Section: Friday 9:00 - 10:20 AM in Bren Hall 3022"
  },
  {
    "objectID": "index.html#readings-and-references",
    "href": "index.html#readings-and-references",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Readings and References",
    "text": "Readings and References\n\nGeocompuation with R\nSpatial Data Science with Applications in R\nA Gentle Introduction to GIS"
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nMeet minimum MEDS device requirements\nInstall or update to R version 4.40\nInstall or update RStudio\nCreate a GitHub account"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nAnnie Adams\nEmail: aradams@ucsb.edu\nStudent Hours: Thursday 11 am - 12 pm @ BH 3418\nLearn more: annieradams.github.io\n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\n\n\nAlessandra Vidal Meza\nEmail: avidalmeza@ucsb.edu\nStudent Hours: Tuesday 3 pm - 4 pm @ BH 1001\nLearn more: avidalmeza.com"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis website was designed by Sam Csik."
  },
  {
    "objectID": "assignments/HW4.html#learning-outcomes",
    "href": "assignments/HW4.html#learning-outcomes",
    "title": "Homework Assignment 4",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThis assignment will reinforce key concepts in geospatial analysis by practicing the following:\n\ncombining vector/raster data\nresampling raster data\nmasking raster data\nmap algebra"
  },
  {
    "objectID": "assignments/HW4.html#instructions",
    "href": "assignments/HW4.html#instructions",
    "title": "Homework Assignment 4",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a new GitHub repository. Give your repository a descriptive title that reflects your analysis (not “EDS223-HW4”). Be sure to include a README that follows these guidelines.\nDownload data from here\nUnzip data and place in repository\nAdd the data to your gitignore! Run git add .gitignore to avoid attempting to push any large data to your repo.\nCreate a Quarto document with responses\nTake a screenshot of your README and add it to the beginning of your qmd.\nRender your quarto document as a pdf and submit on Gradescope\n\nMake sure code-fold is not set to True! Your pdf should contain all your code and outputs!\n\nOnce you have fully completed this assignment (including revising and resubmitting), make sure to delete the image of the README from your .qmd file. The image of your README is solely being used for grading purposes.\n\nYour repository should have the following structure:\n\nEDS223-HW3\n└─── README.md\n└─── qmd/Rmd/Proj files\n└─── aquaculture_analysis.qmd # Name your file a title that is representative of your analysis!\n|   .gitignore\n    └───data\n        └─── wc_regions_clean.shp\n        └─── depth.tif\n        └─── average_annual_sst_2008.tif\n        └─── average_annual_sst_2009.tif\n        └─── average_annual_sst_2010.tif\n        └─── average_annual_sst_2011.tif\n        └─── average_annual_sst_2012.tif"
  },
  {
    "objectID": "assignments/HW4.html#background",
    "href": "assignments/HW4.html#background",
    "title": "Homework Assignment 4",
    "section": "Background",
    "text": "Background\nMarine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production.1 Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, and bottom depth. They found that global seafood demand could be met using less than 0.015% of the global ocean area2"
  },
  {
    "objectID": "assignments/HW4.html#description",
    "href": "assignments/HW4.html#description",
    "title": "Homework Assignment 4",
    "section": "Description",
    "text": "Description\nFor this assignment, you are tasked with determining which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture for several species of oysters and a species of your choice. Suitable locations will be determined based on range of suitable sea surface temperature (SST) and depth values for the species.\nTo make your workflow generalizable, you must create a function that has the following characteristics:\n\narguments:\n\nminimum and maximum sea surface temperature\nminimum and maximum depth\nspecies name\n\noutputs:\n\nmap of EEZ regions colored by amount of suitable area\n\nspecies name should be included in the map’s title\n\n\n\n\n\n\n\n\n\nTipTip: Building a generalizable workflow\n\n\n\nRemember, functions can be hard to debug! So it’s highly recommended that you build the workflow for a single species (not in a function). Then you can start translating your workflow into a function by identifying the pieces that need to be generalized.\nQuestions to ask yourself: which variables can stay the same for each species and which ones would need to change?"
  },
  {
    "objectID": "assignments/HW4.html#data-details",
    "href": "assignments/HW4.html#data-details",
    "title": "Homework Assignment 4",
    "section": "Data details",
    "text": "Data details\n\nSuitable growing conditions\n\nOysters\nResearch has shown that oysters need the following conditions for optimal growth:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\n\n\nChoose-your-own species\nYou can find information on species depth and temperature requirements on SeaLifeBase. Remember, we are thinking about the potential for marine aquaculture, so these species should have some reasonable potential for commercial consumption.\n\n\n\nSea Surface Temperature\nWe will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\nData files:\n\naverage_annual_sst_2008.tif\naverage_annual_sst_2009.tif\naverage_annual_sst_2010.tif\naverage_annual_sst_2011.tif\naverage_annual_sst_2012.tif\n\n\n\nBathymetry\nTo characterize the depth of the ocean we will use the General Bathymetric Chart of the Oceans (GEBCO).3\nData file: depth.tif\n\n\nExclusive Economic Zones\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org.\nData file: wc_regions_clean.shp"
  },
  {
    "objectID": "assignments/HW4.html#workflow-outline",
    "href": "assignments/HW4.html#workflow-outline",
    "title": "Homework Assignment 4",
    "section": "Workflow outline",
    "text": "Workflow outline\nBelow is an outline of the steps you should consider taking to achieve the assignment tasks.\n\nPrepare data\nTo start, we need to load all necessary data and make sure it has the coordinate reference system.\n\nshapefile for the West Coast EEZ\nbathymetry raster\nSST rasters\n\ncombine SST rasters into a raster stack\n\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure to check that these datasets have the same coordinate reference systems! If not, transform them to match.\n\n\n\n\nProcess data\nNext, we need to process the SST and depth data so that they can be combined. In this case the SST and depth data have slightly different resolutions, extents, and positions.\n\nfind the mean SST from 2008-2012 (e.g. create single raster of average SST)\nconvert average SST from Kelvin to Celsius\n\nhint: subtract by 273.15\n\ncrop depth raster to match the extent of the SST raster\nnote: the resolutions of the SST and depth data do not match\n\nresample the depth data to match the resolution of the SST data using the nearest neighbor approach\n\ncheck that the depth and SST match in resolution, extent, and coordinate reference system\n\nhint: can the rasters be stacked?\n\n\n\n\nFind suitable locations\nTo find suitable locations for marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth.\n\nreclassify SST and depth data into locations that are suitable for oysters\n\nhint: set suitable values to 1 and unsuitable values to 0\n\nfind locations that satisfy both SST and depth conditions\n\n\n\n\n\n\n\nTipTip: finding suitable locations\n\n\n\nThe SST and depth rasters should now identify the suitability of locations as 0 or 1. To find locations that have both suitable temperature and depth, you can use map algebra. One idea is to multiply the values of the cells, using the lapp() function.\n\n\n\n\nDetermine the most suitable EEZ\nWe want to determine the total suitable area within each EEZ in order to rank zones by priority. To do so, we need to find the total area of suitable locations within each EEZ.\n\nselect suitable cells within West Coast EEZs\nfind area of grid cells\nfind the total suitable area within each EEZ\n\nhint: it might be helpful to rasterize the EEZ data"
  },
  {
    "objectID": "assignments/HW4.html#rubric-specifications",
    "href": "assignments/HW4.html#rubric-specifications",
    "title": "Homework Assignment 4",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nYour output should serve as a stand-alone item that someone unfamiliar with the assignment would be able to understand your analysis, including the decisions made in selecting your approach and interpretation of the results.\nAssignments will be deemed “Satisfactory” based on the following criteria:\n\nData analysis\n\nCode must produce expected output based on correct data manipulation\nMultiple approaches can be used to reach the correct output. Analysis must demonstrate critical interrogation of the approach used by showing justification and verification of intermediate steps. “Correct” answers are not sufficient. Justification and verification of approach should be demonstrated using the following:\n\ncustom warning and error message (e.g. warning() and stop(); resources from EDS 221)\ninformative comments (resource from EDS 220)\nBONUS: unit tests (e.g. using {testthat}; resources from EDS 221)\n\n\n\n\nPlots, tables, and maps\n\nAll plots, tables, and maps must be clear, accurate, and effectively convey the intended information\nAll plots and maps must include the following elements:\n\nan informative title\nlegends with legible titles, including units\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\nfor maps: indication of scale and orientation (i.e. graticules/gridlines or scale bar and compass)\nfor figures: axes with legible labels and titles, including units\n\nAll tables should be rendered with legible titles and stypling (e.g. {kableExtra})\n\n\n\nWritten reflections\n\nReflections must be clear, accurate, and demonstrate a deep understanding of the analysis performed.\n\n\n\nProfessional output\n\nQuarto document must be rendered to pdf\nThe rendered output must include the following elements:\n\ndocument header with title, name, and date (ideally the date rendered)\nall packages are loaded together at the top of the document\nall unnecessary/distracting warnings and messages are suppressed\ninclude informative code comments when appropriate (resource from EDS 220)\nfolding code or sourcing separate scripts when appropriate to direct reader’s attention\nsuccinct documentation between steps, such as section headers, descriptions for an analysis and map/plot interpretation\ncomplete and detailed data citations\nall defined variables are used in analysis (no variables are defined that are not used)\n\n\n\n\n\n\n\n\nTipGuide to professional output\n\n\n\nSee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW4.html#footnotes",
    "href": "assignments/HW4.html#footnotes",
    "title": "Homework Assignment 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).↩︎\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).↩︎\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).↩︎"
  },
  {
    "objectID": "assignments/HW2.html#learning-outcomes",
    "href": "assignments/HW2.html#learning-outcomes",
    "title": "Homework Assignment 2",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nbuild effective, responsible, accessible and aesthetically-pleasing maps\npractice manipulating vector and raster data to build multi-layer maps\npractice making maps in R, specifically using tmap"
  },
  {
    "objectID": "assignments/HW2.html#instructions",
    "href": "assignments/HW2.html#instructions",
    "title": "Homework Assignment 2",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a new GitHub repository.Be sure to include a README that follows these guidelines\nDownload data from here\nUnzip data and place in repository\nAdd the data to your gitignore!\nCreate a Quarto document with responses\nRender your quarto document as a pdf and submit on Gradescope\n\nMake sure code-fold is not set to True! Your pdf should contain all your code and outputs!\n\nFill out this google form to submit your Github repo link\n\nYour repository should have the following structure:\n\nEDS223-HW2\n│   README.md\n│   HW2.qmd\n│   Rmd/Proj files    \n│\n└───.gitignore\n     └───data\n         └───ejscreen\n         └───gbif-birds-LA\n         └───mapping-inequality"
  },
  {
    "objectID": "assignments/HW2.html#background",
    "href": "assignments/HW2.html#background",
    "title": "Homework Assignment 2",
    "section": "Background",
    "text": "Background\nPresent-day environmental justice may reflect legacies of injustice in the past. The United States has a long history of racial segregation which is still visible. During the 1930’s the Home Owners’ Loan Corporation (HOLC), as part of the New Deal, rated neighborhoods based on their perceived safety for real estate investment. Their ranking system, (A (green), B (blue), C (yellow), D (red)) was then used to block access to loans for home ownership. Colloquially known as “redlining”, this practice has had widely-documented consequences not only for community wealth, but also health.1 Redlined neighborhoods have less greenery2 and are hotter than other neighborhoods.3\nCheck out coverage by the New York Times.\nA recent study found that redlining has not only affected the environments communities are exposed to, it has also shaped our observations of biodiversity.4 Community or citizen science, whereby individuals share observations of species, is generating an enormous volume of data. Ellis-Soto and co-authors found that redlined neighborhoods remain the most undersampled areas across 195 US cities. This gap is highly concerning, because conservation decisions are made based on these data.\nCheck out coverage by EOS."
  },
  {
    "objectID": "assignments/HW2.html#data-details",
    "href": "assignments/HW2.html#data-details",
    "title": "Homework Assignment 2",
    "section": "Data details",
    "text": "Data details\n\nEJScreen\nData file: ejscreen/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\nWe will be working with data from the United States Environmental Protection Agency’s former EJScreen: Environmental Justice Screening and Mapping Tool. This tool is no longer operating, but you can find an unofficial version of it here\nAccording to the former US EPA website:\nThe tool, when in existence, was intended to support research and policy goals. It was shared with the public to achieve the following:\n\n\nto be more transparent about how we consider environmental justice in our work,\n\nto assist our stakeholders in making informed decisions about pursuing environmental justice and,\n\nto create a common starting point between the agency and the public when looking at issues related to environmental justice.\n\n\nEJScreen provided on environmental and demographic information for the US at the Census tract and block group levels. You will be working with data at the block group level that has been downloaded from the EPA site. To understand the associated data columns, you will need to explore the following in the data folder:\n\nTechnical documentation: ejscreen-tech-doc-version-2-2.pdf\nColumn descriptions: EJSCREEN_2023_BG_Columns.xlsx\n\n\n\nHOLC Redlining\nData file: mapping-inequality/mapping-inequality-los-angeles.json\nA team of researchers, led by the Digital Scholarship Lab at the University of Richmond have digitized maps and information from the HOLC as part of the Mapping Inequality project.\nWe will be working with maps of HOLC grade designations for Los Angeles. Information on the data can be found here.5\n\n\nBiodiversity observations\nData file: gbif-birds-LA.shp\nThe Global Biodiversity Information Facility is the largest aggregator of biodiversity observations in the world. Observations typically include a location and date that a species was observed.\nWe will be working observations of birds from 2021 onward.\n\n\n\n\n\n\nTip\n\n\n\nMake sure to check that these datasets have the same coordinate reference systems! If not, transform them to match."
  },
  {
    "objectID": "assignments/HW2.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "href": "assignments/HW2.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "title": "Homework Assignment 2",
    "section": "Part 1: Legacy of redlining in current environmental (in)justice",
    "text": "Part 1: Legacy of redlining in current environmental (in)justice\nYour first task is to explore historical redlining in Los Angeles and its legacy on present-day environmental justice.\n\nDescription\nFor this assignment, you must complete the following:\n\nCreate a map of historical redlining neighborhoods, including:\n\n\nneighborhoods colored by HOLC grade\nan appropriate base map\na clear title and legend\n\n\nCreate a table summarizing:\n\n\nthe percentage of census block groups that fall within each HOLC grade\nAlso include the percent of census black groups that don’t fall within a HOLC grade\nHint: The HOLC data contains the grades and the EJScreen data contains the census blocks, so you will need to combine the data spatially before doing summary statistics. Once you combine and no longer need the geometries, you can use st_drop_geometry().\n\n\nCreate at least two visualizations summarizing current conditions (from the EJScreen data) within HOLC grades using the mean of the following variables (you may combine variables or create separate plots):\n\n\n% low income\npercentile for Particulate Matter 2.5\npercentile for low life expectancy\nUse ggplot for your visualizations! You will first need to calculate mean of each variable grouped by HOLC grade.\n\n\nWrite a brief paragraph reflecting on these results\n\n\nInterpret the patterns you observe in your results\nDiscuss potential relationships between historical redlining grades and current environmental/socioeconomic conditions"
  },
  {
    "objectID": "assignments/HW2.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "href": "assignments/HW2.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "title": "Homework Assignment 2",
    "section": "Part 2: Legacy of redlining in biodiversity observations",
    "text": "Part 2: Legacy of redlining in biodiversity observations\nYour second task is to explore the legacy of historical redlining in Los Angeles on the collection of bird observations.\n\nDescription\nFor this assignment, you must produce the following based on observations from 2021-2023:\n\nA figure summarizing the percent of observations within redlined neighborhoods within each HOLC grade\nCreate a visualizations that shows:\n\nThe percentage of bird observations within each HOLC grade\nInclude an appropiate title, axis labels, and legend\nHints: Ensure the bird observations and HOLC dataset have matching CRS’, then perform a spatial join to assign each bird observations to a corresponding HOLC grade.\n\nSpolier alert!! Our results don’t match the findings from Ellis-Soto et al. 2023! Read the abstract of the study. Why might we have obtained different results in our analysis? What did the paper consider that we did not?"
  },
  {
    "objectID": "assignments/HW2.html#rubric-specifications",
    "href": "assignments/HW2.html#rubric-specifications",
    "title": "Homework Assignment 2",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nAssignments will be deemed “Satisfactory” based on the following criteria:\n\nData analysis\n\nCode must produce expected output based on correct data manipulation\nSpatial operations must be performed correctly with appropriate verification (e.g., CRS matching, appropriate join methods, checking results)\nMultiple approaches can be used to reach the correct output. Analysis must demonstrate critical interrogation of the approach used by showing justification and verification of intermediate steps. “Correct” answers are not sufficient. Justification and verification of approach should be demonstrated using the following:\n\ncustom warning and error message (e.g. warning() and stop(); resources from EDS 221)\ninformative comments (resource from EDS 220)\nBONUS: unit tests (e.g. using {testthat}; resources from EDS 221)\n\n\n\n\nPlots, tables, and maps\n\nAll plots, tables, and maps must be clear, accurate, and effectively convey the intended information\nAll plots and maps must include the following elements:\n\nan informative title\nlegends with legible titles, including units\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\nfor maps: indication of scale and orientation (i.e. graticules/gridlines or scale bar and compass), and an appropiate basemap included where necessary\n\nAll tables should be rendered with legible titles and styling (e.g. {gt})\n\n\n\nWritten reflections\n\nReflections must be clear, accurate, and demonstrate a deep understanding of the analysis performed.\n\n\n\nProfessional Output\n\nThe rendered Quarto doc must show all required elements in a professional style with explanation of each step of analysis\n\nsee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW2.html#footnotes",
    "href": "assignments/HW2.html#footnotes",
    "title": "Homework Assignment 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGee, G. C. (2008). A multilevel analysis of the relationship between institutional and individual racial discrimination and health status. American journal of public health, 98(Supplement_1), S48-S56.↩︎\nNardone, A., Rudolph, K. E., Morello-Frosch, R., & Casey, J. A. (2021). Redlines and greenspace: the relationship between historical redlining and 2010 greenspace across the United States. Environmental health perspectives, 129(1), 017006.↩︎\nHoffman, J. S., Shandas, V., & Pendleton, N. (2020). The effects of historical housing policies on resident exposure to intra-urban heat: a study of 108 US urban areas. Climate, 8(1), 12.↩︎\nEllis-Soto, D., Chapman, M., & Locke, D. H. (2023). Historical redlining is associated with increasing geographical disparities in bird biodiversity sampling in the United States. Nature Human Behaviour, 1-9.↩︎\nRobert K. Nelson, LaDale Winling, Richard Marciano, Nathan Connolly, et al., “Mapping Inequality,” American Panorama, ed. Robert K. Nelson and Edward L. Ayers, accessed October 17, 2023, https://dsl.richmond.edu/panorama/redlining/↩︎"
  },
  {
    "objectID": "assignments/SR3.html",
    "href": "assignments/SR3.html",
    "title": "Final Self Reflection (SR #3)",
    "section": "",
    "text": "In this assignment you’ll reflect on your learning through the course. Reread your response to the Pre-Course Self Reflection (SR #1) and Mid-Course Self Reflection (SR #2) and answer the following questions:\n\n\n\nSince SR #2, have you improved any of your existing skills or learned new ones?\nHave you accomplished the learning goals you outlined for this course in SR #1 (and potentially expanded on SR #2)? If you did, how did you do it? If you didn’t why do you think you didn’t?\nDid you learn new strategies for being accomplishing your goals? Will you revise your plan for future quarters?\nWhat are you proudest of accomplishing in this course?\n\n\n\n\n\nWhat are some transferable skills that you developed in this course? How might you apply them to other courses or deliverables in your degree or to jobs in the future?\nWhat is one thing you really liked about this course, and why? What is one thing you think could be improved about this course, and how?\nIs there anything else you’d like the instructors to know about your experience in this course?"
  },
  {
    "objectID": "assignments/SR3.html#description",
    "href": "assignments/SR3.html#description",
    "title": "Final Self Reflection (SR #3)",
    "section": "",
    "text": "In this assignment you’ll reflect on your learning through the course. Reread your response to the Pre-Course Self Reflection (SR #1) and Mid-Course Self Reflection (SR #2) and answer the following questions:\n\n\n\nSince SR #2, have you improved any of your existing skills or learned new ones?\nHave you accomplished the learning goals you outlined for this course in SR #1 (and potentially expanded on SR #2)? If you did, how did you do it? If you didn’t why do you think you didn’t?\nDid you learn new strategies for being accomplishing your goals? Will you revise your plan for future quarters?\nWhat are you proudest of accomplishing in this course?\n\n\n\n\n\nWhat are some transferable skills that you developed in this course? How might you apply them to other courses or deliverables in your degree or to jobs in the future?\nWhat is one thing you really liked about this course, and why? What is one thing you think could be improved about this course, and how?\nIs there anything else you’d like the instructors to know about your experience in this course?"
  },
  {
    "objectID": "assignments/SR3.html#rubric-specifications",
    "href": "assignments/SR3.html#rubric-specifications",
    "title": "Final Self Reflection (SR #3)",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nTo receive a “Satisfactory” score, you must adhere to the following:\n\nSubmit your response here by 11:59 PM on the due date. Extensions can be requested by redeeming tokens.\nYour response must include answers to each question listed in the Description section.\nYou should aim to answer each question in 2-4 sentences.\nYour responses must demonstrate genuine reflection. Obviously low effort responses will receive a “Not Yet”.\n\n\n\n\n\n\n\nTipTips for a “Satisfactory”\n\n\n\nBy clicking the boxes above, you can keep track of which items you’ve completed! I suggest drafting your response in a word document before copy/pasting into the submission form."
  },
  {
    "objectID": "assignments/SR1.html",
    "href": "assignments/SR1.html",
    "title": "Pre-Course Self Reflection (SR #1)",
    "section": "",
    "text": "In this assignment you’ll introduce yourself, define your learning goals for this course, and outline a plan for how to meet them. You’ll revisit these goals and plans throughout the quarter, so it’s good to be reflective from the start!\n\n\nEveryone brings their own unique background and experiences to the course. So I can best understand your specific context, please address the following:\n\nDo you have a particular career goal in mind? If so, how does this course apply to your future career, if at all?\nHow confident do you feel in your coding skills in R? In other languages? Why?\nHow confident do you feel in your geospatial analysis skills? Why? Have you worked with a GIS before?\nIs there anything you wish your instructors knew about you, but don’t (e.g. responsibilities outside of school)?\n\n\n\n\n\nWhat skills or knowledge do you hope to gain from this course? Why?\nWhat learning objectives from the course are you most excited about? Why?\nMost importantly, how do you plan to accomplish your learning goals for this course? Be specific! Instead of stating “I will complete assignments on time”, reflect on what strategies you will employ. For example, creating a learning schedule, structured collaboration with peers, participating in online learning communities, etc.\n\n\n\n\n\nWhat do like to do for fun outside of school/work?\nWhat is a piece of media you enjoyed recently? (e.g. music, book, movie, meme)\nWhere do you call home?"
  },
  {
    "objectID": "assignments/SR1.html#description",
    "href": "assignments/SR1.html#description",
    "title": "Pre-Course Self Reflection (SR #1)",
    "section": "",
    "text": "In this assignment you’ll introduce yourself, define your learning goals for this course, and outline a plan for how to meet them. You’ll revisit these goals and plans throughout the quarter, so it’s good to be reflective from the start!\n\n\nEveryone brings their own unique background and experiences to the course. So I can best understand your specific context, please address the following:\n\nDo you have a particular career goal in mind? If so, how does this course apply to your future career, if at all?\nHow confident do you feel in your coding skills in R? In other languages? Why?\nHow confident do you feel in your geospatial analysis skills? Why? Have you worked with a GIS before?\nIs there anything you wish your instructors knew about you, but don’t (e.g. responsibilities outside of school)?\n\n\n\n\n\nWhat skills or knowledge do you hope to gain from this course? Why?\nWhat learning objectives from the course are you most excited about? Why?\nMost importantly, how do you plan to accomplish your learning goals for this course? Be specific! Instead of stating “I will complete assignments on time”, reflect on what strategies you will employ. For example, creating a learning schedule, structured collaboration with peers, participating in online learning communities, etc.\n\n\n\n\n\nWhat do like to do for fun outside of school/work?\nWhat is a piece of media you enjoyed recently? (e.g. music, book, movie, meme)\nWhere do you call home?"
  },
  {
    "objectID": "assignments/SR1.html#rubric-specifications",
    "href": "assignments/SR1.html#rubric-specifications",
    "title": "Pre-Course Self Reflection (SR #1)",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nTo receive a “Satisfactory” score, you must adhere to the following:\n\nRespond to each of the above questions on a word document. Upload a pdf of this document to Gradescope by 11:59 PM on the due date. Extensions are not allowed on this first assignment.\nYour response must include answers to each question listed in the Description section.\nYou should aim to answer each question in 2-4 sentences.\nYour responses must demonstrate genuine reflection. Obviously low effort responses will receive a “Not Yet”.\n\n\n\n\n\n\n\nTipTips for a “Satisfactory”\n\n\n\nBy clicking the boxes above, you can keep track of which items you’ve completed! I suggest drafting your response in a word document before copy/pasting into the submission form."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "ImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed. Homework Assignments (HWs) and Self-reflections (SRs) are always due on Saturdays to ensure that you have at least one day a week with no course obligations."
  },
  {
    "objectID": "assignments.html#calendar",
    "href": "assignments.html#calendar",
    "title": "Assignments",
    "section": "",
    "text": "ImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed. Homework Assignments (HWs) and Self-reflections (SRs) are always due on Saturdays to ensure that you have at least one day a week with no course obligations."
  },
  {
    "objectID": "assignments.html#assignments",
    "href": "assignments.html#assignments",
    "title": "Assignments",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nImportant\n\n\n\nEarning “Satisfactory”/ “Almost”/ “Not Yet” marks on Self-reflections (SRs), Homework Assignments (HWs), and the Portfolio Repository (PR) will determine your letter grade (e.g. A, B, etc.) for this course. See Grader Tracker below.\n\n\nLinks to assignments will become available as they are assigned.\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nSR\nPre-course Self Reflection (SR#1)\n09/30/2025\n10/04/2025\n\n\nHW\nHomework Assignment #1\n09/30/2025\n10/06/2025\n\n\nHW\nHomework Assignment #2\n10/07/2025\n10/18/2025\n\n\nHW\nHomework Assignment #3\n10/21/2025\n11/10/2025\n\n\nSR\nMid Quarter Self Reflection (SR#2)\n10/28/2025\n11/01/2025\n\n\nHW\nHomework Assignment #4\n11/13/2025\n11/26/2025\n\n\nPR\nPortfolio Repository\n11/13/2025\n12/06/2025"
  },
  {
    "objectID": "assignments.html#weekly-check-ins",
    "href": "assignments.html#weekly-check-ins",
    "title": "Assignments",
    "section": "Weekly Check-Ins",
    "text": "Weekly Check-Ins\n\n\n\n\n\n\nImportant\n\n\n\nWeekly Check-Ins will become available at the end of each class on Thursday and are due by end-of-day (11:59 PM). Completing these Check-Ins by the due dates / times will determine whether you earn a +/- on your course grade. See Grade Tracker below.\n\n\nLinks to surveys will become available as they are assigned.\n\n\n\nWeekly Check-Ins\nDate Assigned\nDate Due\n\n\n\n\nWeek 1 Check-In\nThurs 10/02/2025\nThurs 10/02/2025\n\n\nWeek 2 Check-In\nThurs 10/09/2025\nThurs 10/09/2025\n\n\nWeek 3 Check-In\nThurs 10/16/2025\nThurs 10/64/2025\n\n\nWeek 4 Check-In\nThurs 10/23/2025\nThurs 10/23/2025\n\n\nWeek 5 Check-In\nThurs 10/30/2025\nThurs 10/30/2025\n\n\nWeek 6 Check-In\nThurs 11/06/2025\nThurs 11/06/2025\n\n\nWeek 7 Check-In\nThurs 11/13/2025\nThurs 11/13/2025\n\n\nWeek 8 Check-In\nThurs 11/20/2025\nThurs 11/20/2025\n\n\nNo Check-In Week 9\nNA\nNA\n\n\nWeek 10 Check-In\nThurs 12/04/2025\nThurs 12/04/2025"
  },
  {
    "objectID": "assignments.html#grade-tracker",
    "href": "assignments.html#grade-tracker",
    "title": "Assignments",
    "section": "Grade Tracker",
    "text": "Grade Tracker\nUse the Grade Tracker, below, to determine your course grade:\n\n\n\n\n\n\n\n\n\nRedeem tokens in exchange for assignment extensions, missing class, or to revise / resubmit an assignment that received an “Almost” or “Not Yet” mark."
  },
  {
    "objectID": "assignments.html#rubric",
    "href": "assignments.html#rubric",
    "title": "Assignments",
    "section": "Rubric",
    "text": "Rubric\nEach Homework Assignment (HWs) will include an individual rubric. However, to earn a “Satisfactory” assignments must adhere to best practices for producing professional output. Below are examples of professional and unprofessional outputs for guidance.\nExamples of Professional Output:\n\nGood Example with sourced functions\nBad Example"
  },
  {
    "objectID": "assignments.html#getting-unstuck",
    "href": "assignments.html#getting-unstuck",
    "title": "Assignments",
    "section": "Getting unstuck",
    "text": "Getting unstuck\n\nWhere to find help\nBeing a great data scientist isn’t about writing perfect code; it’s about learning how to teach yourself and when to ask for help. The only way to get better at this process is to practice by taking the time to troubleshoot on our own, so you should always plan to start there! The graphic below shows the order in which you should approach different resources for help:\n\n\n\n\n\n\n\n\n\n\n\nRoadblock checklist\nIf you hit a roadblock, run through this checklist to make sure you’ve done your due diligence before bringing your question(s) to a peer, TA, or instructor.\n\nrevisit the course materials - your question may already be answered in the slides, textbook, or discussion section materials\nread the documentation - you can do so directly from RStudio by typing ?function_name in the console\nread the package’s vignette, if available - these are often linked on CRAN under the Documents sections (e.g. see {dplyr} on CRAN) or can be found through the command vignette(package = \"package-name\") and vignette(\"vignette-name\")\ntry Googling! - don’t forget to look back at our suggested troubleshooting and Googling tips (Teach Me How to Google)\n\n\n\nHow to ask questions\nWhen you decide to ask a question to a peer, TA, or instructor be sure to:\n\nProvide context. For example, “I’m trying to do this…” or “I’m working on the task where we do this…”\nShare the specific challenge. “I’m specifically trying to [insert function / package] to do this thing.”\nShare what happens and what you’ve learned. “I repeatedly get an error message that says [this]. I’ve tried [this] and [this]”\nShow your code ideally with a reprex that they can run / test.\nValue and expect the Socratic method, especially in classes and workshops – our goal is to provide critical thinking that is transferable, not just to provide a quick fix for a single error."
  },
  {
    "objectID": "assignments/SR2.html",
    "href": "assignments/SR2.html",
    "title": "Mid-Course Self Reflection (SR #2)",
    "section": "",
    "text": "In this assignment you’ll reflect on your progress in the course so far. Reread your response to the Pre-Course Self Reflection (SR #1) and answer the following questions:\n\n\n\nHow have you progressed towards your learning goals for this course?\nLooking back, have your learning goals changed? If so, how?\nLooking forward, do you have new goals?\nWhat skills are you proud of developing?\n\n\n\n\n\nLooking back at your plan for achieving your learning goals, have your strategies been effective?\nLooking forward, will you adopt new strategies? If so, what will they be?\nIf you’ve needed help, how have you sought it out? If you haven’t sought out help, why not?\n\n\n\n\n\nWhat topics have excited you the most so far? Have you spent time outside of class diving deeper into any of these topics? If so, what have you learned?\nIs there anything about this course that you are really enjoying? Anything that isn’t working for you?\nWhat else would you like me to know about your experience so far?"
  },
  {
    "objectID": "assignments/SR2.html#description",
    "href": "assignments/SR2.html#description",
    "title": "Mid-Course Self Reflection (SR #2)",
    "section": "",
    "text": "In this assignment you’ll reflect on your progress in the course so far. Reread your response to the Pre-Course Self Reflection (SR #1) and answer the following questions:\n\n\n\nHow have you progressed towards your learning goals for this course?\nLooking back, have your learning goals changed? If so, how?\nLooking forward, do you have new goals?\nWhat skills are you proud of developing?\n\n\n\n\n\nLooking back at your plan for achieving your learning goals, have your strategies been effective?\nLooking forward, will you adopt new strategies? If so, what will they be?\nIf you’ve needed help, how have you sought it out? If you haven’t sought out help, why not?\n\n\n\n\n\nWhat topics have excited you the most so far? Have you spent time outside of class diving deeper into any of these topics? If so, what have you learned?\nIs there anything about this course that you are really enjoying? Anything that isn’t working for you?\nWhat else would you like me to know about your experience so far?"
  },
  {
    "objectID": "assignments/SR2.html#rubric-specifications",
    "href": "assignments/SR2.html#rubric-specifications",
    "title": "Mid-Course Self Reflection (SR #2)",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nTo receive 2 tokens, you must adhere to the following:\n\nSubmit your response on Gradescope.\nYour response must include answers to each question listed in the Description section.\nYour responses must demonstrate genuine reflection. Obviously low effort responses will receive a “Not Yet”."
  },
  {
    "objectID": "assignments/HW1.html",
    "href": "assignments/HW1.html",
    "title": "Homework Assignment 1",
    "section": "",
    "text": "Important\n\n\n\nYou must earn a “Satisfactory” on all parts of the assignment to earn a “Satisfactory” on the assignment.\nThe assignment must be submitted through Gradescope.\nRead each part of the assignment carefully, and use the check boxes to ensure you’ve addressed all elements of the assignment!"
  },
  {
    "objectID": "assignments/HW1.html#learning-outcomes",
    "href": "assignments/HW1.html#learning-outcomes",
    "title": "Homework Assignment 1",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nbuild effective, responsible, accessible and aesthetically-pleasing maps\npractice manipulating vector and raster data to build multi-layer maps\npractice making maps in R, specifically using tmap"
  },
  {
    "objectID": "assignments/HW1.html#instructions",
    "href": "assignments/HW1.html#instructions",
    "title": "Homework Assignment 1",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a new GitHub repository.Be sure to include a README that follows these guidelines\nDownload data from here\nUnzip data and place in repository\nAdd the data to your gitignore!\nCreate a Quarto document with responses\nRender your quarto document as a pdf and submit on Gradescope ** Make sure code-fold is not set to True! Your pdf should contain all your code and outputs!\nAdd the link to your github repository to this spreadsheet\n\nYour repository should have the following structure:\n\nEDS223-HW1\n│   README.md\n│   ej_screen.qmd\n|   ej_screen.pdf\n│   Rmd/Proj files    \n└───data\n     └───ejscreen"
  },
  {
    "objectID": "assignments/HW1.html#exploring-environmental-injustice",
    "href": "assignments/HW1.html#exploring-environmental-injustice",
    "title": "Homework Assignment 1",
    "section": "Exploring environmental (in)justice",
    "text": "Exploring environmental (in)justice\nAs many of us are aware, environmental degradation, pollution, and hazards is not felt equally by all people. Environmental justice has many definitions, but the United States Environmental Protection Agency defines it as follows:\n\n“Environmental justice” means the just treatment and meaningful involvement of all people, regardless of income, race, color, national origin, Tribal affiliation, or disability, in agency decision-making and other Federal activities that affect human health and the environment so that people:\n\nare fully protected from disproportionate and adverse human health and environmental effects (including risks) and hazards, including those related to climate change, the cumulative impacts of environmental and other burdens, and the legacy of racism or other structural or systemic barriers; and\nhave equitable access to a healthy, sustainable, and resilient environment in which to live, play, work, learn, grow, worship, and engage in cultural and subsistence practices\n\n\nFor far too long, environmental inequities have been invisible, and thus ignored. Mapping environmental inequities can be a powerful tool for revealing injustices. We will be working with data from the United States Environmental Protection Agency’s previous EJScreen: Environmental Justice Screening and Mapping Tool.\nThe tool, when in existence, was intended to support research and policy goals. It was shared with the public to achieve the following:\n\n\nto be more transparent about how we consider environmental justice in our work,\n\nto assist our stakeholders in making informed decisions about pursuing environmental justice and,\n\nto create a common starting point between the agency and the public when looking at issues related to environmental justice.\n\n\nWhile this tool is no longer available, an unofficial version of the tool can be found here.\n\nDescription\nFor this assignment, you will explore an environmental justice topic of your choosing. You should select a region, community, or environmental issue that matters to you.\nYou must complete the following:\n\ncreate two maps that communicate an environmental justice issue\nwrite a brief paragraph explaining what your maps communicate\n\n\n\nData details\nEJScreen provides on environmental and demographic information for the US at the Census tract and block group levels. You will be working with data at the block group level that has been downloaded from the EPA site. To understand the associated data columns, you will need to explore the following in the data folder:\n\nTechnical documentation: ejscreen-tech-doc-version-2-2.pdf\nColumn descriptions: EJSCREEN_2023_BG_Columns.xlsx\n\nYou should also explore the limitations and caveats of the data.\nThe following code provides examples for reading and manipulating the EJScreen data. You MUST update this code to suite your problem of interest.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# read in geodatabase of EJScreen data at the Census Block Group level\nejscreen &lt;- sf::st_read(here::here(\"data\", \"ejscreen\",\"EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\")) \n\n# filter to a state you are interested in\ncalifornia &lt;- ejscreen %&gt;%\n  dplyr::filter(ST_ABBREV == \"CA\") \n\n# filter to a county you are interested in\nsanta_barbara &lt;- ejscreen %&gt;%\n  dplyr::filter(CNTY_NAME %in% c(\"Santa Barbara County\"))\n\n# find the average values for all variables within counties\ncalifornia_counties &lt;- aggregate(california, by = list(california$CNTY_NAME), FUN = mean)"
  },
  {
    "objectID": "assignments/HW1.html#rubric-specifications",
    "href": "assignments/HW1.html#rubric-specifications",
    "title": "Homework Assignment 1",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nYou must complete the following to receive a “Satisfactory” on the assignment:\n\nAll maps must include the following elements:\n\nan informative title\nlegends with legible titles, including units\nindication of scale and orientation (i.e. graticules/gridlines or scale bar and compass)\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\n\nYour github repository contains a README that follows the MEDS README guidelines, and you added your repo link to this sheet\nEJScreen maps and text must communicate about an issue\n\nit is not sufficient to make two unrelated maps\n\nThe rendered Quarto doc must show all required elements in a professional style\n\nsee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW3.html#learning-outcomes",
    "href": "assignments/HW3.html#learning-outcomes",
    "title": "Homework Assignment 3",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThis assignment will reinforce key concepts in geospatial analysis by practicing the following:\n\nload vector/raster data\n\nsimple raster operations\n\nsimple vector operations\n\nspatial joins"
  },
  {
    "objectID": "assignments/HW3.html#instructions",
    "href": "assignments/HW3.html#instructions",
    "title": "Homework Assignment 3",
    "section": "Instructions",
    "text": "Instructions\n\nCreate a new GitHub repository. Give your repository a descriptive title that reflects your analysis (not “EDS223-HW3”). Be sure to include a README that follows these guidelines.\nDownload data from here\nUnzip data and place in repository\nAdd the data to your gitignore! Run git add .gitignore to avoid attempting to push any large data to your repo.\nCreate a Quarto document with responses\nTake a screenshot of your README and add it to the beginning of your qmd.\nRender your quarto document as a pdf and submit on Gradescope\n\nMake sure code-fold is not set to True! Your pdf should contain all your code and outputs!\n\nOnce you have fully completed this assignment(including revising and resubmitting), make sure to delete the image of the README from your .qmd file. The image of your README is solely being used for grading purposes.\n\nYour repository should have the following structure:\n\nEDS223-HW3\n└───README.md\n└───Rmd/Proj files    \n└─── texas_blackout.qmd # Name your qmd file a title that is representative of your analysis!\n└───.gitignore\n    └───data\n        └───gis_osm_buildings_a_free_1.gpkg\n        └───gis_osm_roads_free_1.gpkg\n        └───ACS_2019_5YR_TRACT_48_TEXAS.gdb\n            └───census tract gdb files\n        └───VNP46A1\n            └───VIIRS data files"
  },
  {
    "objectID": "assignments/HW3.html#background",
    "href": "assignments/HW3.html#background",
    "title": "Homework Assignment 3",
    "section": "Background",
    "text": "Background\nClimate change is increasing the frequency and intensity of extreme weather events, with devastating impacts. “In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nIn this assignment, you will identify the impacts of these series of extreme winter storms by estimating the number of homes in the Houston metropolitan area that lost power and investigate whether not these impacts were disproportionately felt.\nYour analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau."
  },
  {
    "objectID": "assignments/HW3.html#description",
    "href": "assignments/HW3.html#description",
    "title": "Homework Assignment 3",
    "section": "Description",
    "text": "Description\nFor this assignment, you must produce the following:\n\na set of maps comparing night light intensities before and after the first two storms\na map of the homes in Houston that lost power\nan estimate of the number of homes in Houston that lost power\na map of the census tracts in Houston that lost power\na plot comparing the distributions of median household income for census tracts that did and did not experience blackouts\na brief reflection (approx. 100 words) summarizing your results and discussing any limitations to this study"
  },
  {
    "objectID": "assignments/HW3.html#data-details",
    "href": "assignments/HW3.html#data-details",
    "title": "Homework Assignment 3",
    "section": "Data details",
    "text": "Data details\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\nAs you’re learning in EDS 220, accessing, downloading, and preparing remote sensing data is a skill in it’s own right! To prevent this assignment from being a large data wrangling challenge, we have downloaded and prepped the following files for you to work with, stored in the VNP46A1 folder.\n\nData files:\n\nVNP46A1.A2021038.h08v05.001.2021039064328.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.tif: tile h08v06, collected on 2021-02-16\n\n\n\nRoads\nData file: gis_osm_roads_free_1.gpkg Typically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\n\nHouses\nData file: gis_osm_buildings_a_free_1.gpkg\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nYou can use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata.\n\nThe geodatabase contains a layer holding the geometry information (ACS_2019_5YR_TRACT_48_TEXAS), separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\n\n\n\n\n\n\nTip\n\n\n\nMake sure to check that these datasets have the same coordinate reference systems! If not, transform them to match."
  },
  {
    "objectID": "assignments/HW3.html#workflow-outline",
    "href": "assignments/HW3.html#workflow-outline",
    "title": "Homework Assignment 3",
    "section": "Workflow outline",
    "text": "Workflow outline\nTo complete complete the tasks of this assignment, you will need to break your analysis into the following key steps:\n\nfind locations that experienced a blackout by creating a mask\nexclude highways from analysis\nidentify homes that experienced blackouts by combining the locations of homes and blackouts\nidentify the census tracts likely impacted by blackout\n\nBelow is guidance and suggestions for each of these steps.\n\n\n\n\n\n\nTip\n\n\n\nFor improved computational efficiency and easier interoperability with sf, I recommend using the stars package for raster handling.\n\n\n\nCreate blackout mask\nTo identify places that experienced a blackout, you should create a “mask” that indicates for each cell whether or not it experienced a blackout.\n\nfind the change in night lights intensity (presumably) caused by the storm\n\nhint: this will require creating a raster object for each day (2021-02-07 and 2021-02-16)\n\nreclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nassign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1 change\nvectorize the blackout mask\n\nhint: use st_as_sf() to convert from a raster to a vector and fix any invalid geometries with st_make_valid()\n\ncrop (spatially subset) the blackout mask to the Houston area as defined by the following coordinates:\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nre-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\nExclude highways from the cropped blackout mask\nHighways may have experienced changes in their night light intensities that are unrelated to the storm. Therefore, you should excluded any locations within 200 meters of all highways in the Houston area.\n\nidentify areas within 200m of all highways\n\nhint: you may need to use st_union\n\nfind areas that experienced blackouts that are further than 200m from a highway\n\n\n\n\n\n\n\nTip\n\n\n\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\nBelow is a SQL query that can be used as an argument in st_read:\n\n\"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n\n\n\n\nIdentify the number of homes likely impacted by blackouts\n\nidentify homes that overlap with areas that experienced blackouts\n\n\n\n\n\n\n\nTip\n\n\n\nThe buildings geopackage includes data on many types of buildings. As with the roads data, we can avoid reading in data we don’t need.\nBelow is a SQL query that can be used as an argument in st_read:\n\n\"SELECT *\nFROM gis_osm_buildings_a_free_1`\nWHERE (type IS NULL AND name IS NULL)`\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\""
  },
  {
    "objectID": "assignments/HW3.html#rubric-specifications",
    "href": "assignments/HW3.html#rubric-specifications",
    "title": "Homework Assignment 3",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nYour output should serve as a stand-alone item that someone unfamiliar with the assignment would be able to understand your analysis, including the decisions made in selecting your approach and interpretation of the results.\nAssignments will be deemed “Satisfactory” based on the following criteria:\n\nData analysis\n\nCode must produce expected output based on correct data manipulation\nMultiple approaches can be used to reach the correct output. Analysis must demonstrate critical interrogation of the approach used by showing justification and verification of intermediate steps. “Correct” answers are not sufficient. Justification and verification of approach should be demonstrated using the following:\n\ncustom warning and error message (e.g. warning() and stop(); resources from EDS 221)\ninformative comments (resource from EDS 220)\nBONUS: unit tests (e.g. using {testthat}; resources from EDS 221)\n\n\n\n\nPlots, tables, and maps\n\nAll plots, tables, and maps must be clear, accurate, and effectively convey the intended information\nAll plots and maps must include the following elements:\n\nan informative title\nlegends with legible titles, including units\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\nfor maps: indication of scale and orientation (i.e. graticules/gridlines or scale bar and compass)\nfor figures: axes with legible labels and titles, including units\n\nAll tables should be rendered with legible titles and stypling (e.g. {kableExtra})\n\n\n\nWritten reflections\n\nReflections must be clear, accurate, and demonstrate a deep understanding of the analysis performed.\n\n\n\nProfessional output\n\nQuarto document must be rendered to pdf\nThe rendered output must include the following elements:\n\ndocument header with title, name, and date (ideally the date rendered)\nall packages are loaded together at the top of the document\nall unnecessary/distracting warnings and messages are suppressed\ninclude informative code comments when appropriate (resource from EDS 220)\nsuccinct documentation between steps, such as section headers, descriptions for an analysis and map/plot interpretation\ncomplete and detailed data citations\nall defined variables are used in analysis (no variables are defined that are not used)\n\nYour GitHub repository is clean and well organized. It should contain a README that follows the MEDS README guidelines.\n\n\n\n\n\n\n\nTipGuide to professional output\n\n\n\nSee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW3.html#footnotes",
    "href": "assignments/HW3.html#footnotes",
    "title": "Homework Assignment 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "assignments/PR.html#assignment-overview",
    "href": "assignments/PR.html#assignment-overview",
    "title": "Geospatial Analysis blog post",
    "section": "",
    "text": "The purpose of this assignment is to create a blogpost on your personal website that showcases your spatial data science skills.\nTo do so, please:\n\nCreate a blog post based on Assignment 3 or 4.\n\n\n\n\n\n\n\nTipTip\n\n\n\nPick the assignment that highlight the skills you are most interested in showcasing to potential employers."
  },
  {
    "objectID": "assignments/PR.html#guidance-for-creating-repositories",
    "href": "assignments/PR.html#guidance-for-creating-repositories",
    "title": "Geospatial Analysis blog post",
    "section": "Guidance for creating repositories",
    "text": "Guidance for creating repositories\nThe goal of creating a blog post is practice communicating your work in a different format. The audience of a blog post is more general than that of the repository, therefore you may be more selective in which code and output you display. In this case you are building a narrative around your work as opposed to demonstrating how it works, so you may want to only display intermediate and final results as opposed to results of self-checks and not output code.\nThe blog post should include the following: - The assignment is a starting place - You should use the background provided in the assignment as a starting place, but update the text and provide additional information and at least one additional reference. The style is up to you, but it should incorporate the following elements clearly: - [ ] A problem statement or question that your analysis will address - [ ]Background on why this issue is important - [ ] Explanation of the data sources, analyses, and results - [ ]Clear, consistently formatted figures - I.e. informative axis labels/legends, visually appealing and consistent colors and fonts - [ ] Discussion of conclusions, including any potential caveats or future directions - [ ]Posted on your personal website - [Resource on how to post a blog to your website by Sam Shanny Csik] (https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/)"
  },
  {
    "objectID": "assignments/PR.html#rubric",
    "href": "assignments/PR.html#rubric",
    "title": "Geospatial Analysis blog post",
    "section": "Rubric",
    "text": "Rubric\nTo receive a “Satisfactory”, the blog post must include the elements described above in the Blogpost Requirements section."
  },
  {
    "objectID": "assignments/PR.html#submitting-assignment",
    "href": "assignments/PR.html#submitting-assignment",
    "title": "Geospatial Analysis blog post",
    "section": "Submitting assignment",
    "text": "Submitting assignment\nOnce you have completed your blog post, submit the link to your blog post on this google form.\n\n\n\nTask\nDue Date\n\n\n\n\nBlog post submission\n12/06\n\n\nBlog post feedback released\n12/10\n\n\nBlog post resubmission\n12/12\n\n\n\nBoth a submission and a revised submission addressing all the feedback from the first revision will be needed to receive a Satisfactory. An assignment without a resubmission will not get a Satisfactory, resulting in the highest possible letter grade for the course being a C."
  },
  {
    "objectID": "course-materials/labs/week8.html#data",
    "href": "course-materials/labs/week8.html#data",
    "title": "Week 8: Lab",
    "section": "1. Data",
    "text": "1. Data\nMulti-spectral remote sensing data:\n\nLandsat’s Operational Land Imager (OLI)\n8 pre-processed scenes\n\nLevel 2 surface reflectance products\nErroneous values set to NA\nScale factor set to 100\nBands 2-7\nDates in file name\n\nScenes from the following dates:\n\n2018-06-12\n2018-08-15\n2018-10-18\n2018-11-03\n2019-01-22\n2019-02-23\n2019-04-12\n2019-07-01\n\n\nLocations of representative vegetation communities:\n\nPolygons representing sites\n\nstudy_site: character string with plant type"
  },
  {
    "objectID": "course-materials/labs/week8.html#set-up",
    "href": "course-materials/labs/week8.html#set-up",
    "title": "Week 8: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nLet’s load all necessary packages:\n\nlibrary(terra)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tmap)"
  },
  {
    "objectID": "course-materials/labs/week8.html#create-function-to-compute-ndvi",
    "href": "course-materials/labs/week8.html#create-function-to-compute-ndvi",
    "title": "Week 8: Lab",
    "section": "2. Create function to compute NDVI",
    "text": "2. Create function to compute NDVI\nLet’s start by defining a function to compute the Normalized Difference Vegetation Index (NDVI). NDVI computes the difference in reflectance between the near infrared (NIR) and red bands, normalized by their sum.\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}"
  },
  {
    "objectID": "course-materials/labs/week8.html#compute-ndvi-for-a-single-scene",
    "href": "course-materials/labs/week8.html#compute-ndvi-for-a-single-scene",
    "title": "Week 8: Lab",
    "section": "3. Compute NDVI for a single scene",
    "text": "3. Compute NDVI for a single scene\nWe have 8 scenes collected by Landsat’s OLI sensor on 8 different days throughout the year.\nLet’s start by loading in the first scene collected on June 12, 2018:\n\nlandsat_20180612 &lt;- rast(here(\"data\", \"landsat_20180612.tif\"))\n\nLet’s update the names of the layers to match the spectral bands they correspond to:\n\nnames(landsat_20180612) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\nNow we can apply the NDVI function we created to compute NDVI for this scene using the lapp() function.\n\nThe lapp() function applies a function to each cell using layers as arguments.\nTherefore, we need to tell lapp() which layers (or bands) to pass into the function.\n\nThe NIR band is the 4th layer and the red band is the 3rd layer in our raster. In this case, because we defined the NIR band as the first argument and the red band as the second argument in our function, we tell lapp() to use the 4th layer first and 3rd layer second.\n\nndvi_20180612 &lt;- lapp(landsat_20180612[[c(4, 3)]], fun = ndvi_fun)\n\n\n\nCode\ntm_shape(ndvi_20180612) +\n  tm_raster(col.legend  = tm_legend(\"NDVI\")) +\n  tm_layout(legend.outside.position = \"right\")"
  },
  {
    "objectID": "course-materials/labs/week8.html#compute-ndvi-for-all-scences",
    "href": "course-materials/labs/week8.html#compute-ndvi-for-all-scences",
    "title": "Week 8: Lab",
    "section": "1. Compute NDVI for all scences",
    "text": "1. Compute NDVI for all scences\nNow we want to repeat the same operations for all 8 scenes. Below is a possible solution (where we repeat each line for each scene), but it’s pretty clunky.\nLet’s load each layer:\n\nlandsat_20180612 &lt;-rast(here( \"data\", \"landsat_20180612.tif\"))\nlandsat_20180815 &lt;- rast(here(\"data\", \"landsat_20180815.tif\"))\nlandsat_20181018 &lt;- rast(here(\"data\", \"landsat_20181018.tif\"))\nlandsat_20181103 &lt;- rast(here(\"data\", \"landsat_20181103.tif\"))\nlandsat_20190122 &lt;- rast(here(\"data\", \"landsat_20190122.tif\"))\nlandsat_20190223 &lt;- rast(here(\"data\", \"landsat_20190223.tif\"))\nlandsat_20190412 &lt;- rast(here(\"data\", \"landsat_20190412.tif\"))\nlandsat_20190701 &lt;- rast(here(\"data\", \"landsat_20190701.tif\"))\n\nAnd rename each layer:\n\nnames(landsat_20180612) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20180815) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20181018) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20181103) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190122) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190223) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190412) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190701) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\nNext, compute NDVI for each layer:\n\nndvi_20180612 &lt;- lapp(landsat_20180612[[c(4, 3)]], fun = ndvi_fun)\nndvi_20180815 &lt;- lapp(landsat_20180815[[c(4, 3)]], fun = ndvi_fun)\nndvi_20181018 &lt;- lapp(landsat_20181018[[c(4, 3)]], fun = ndvi_fun)\nndvi_20181103 &lt;- lapp(landsat_20181103[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190122 &lt;- lapp(landsat_20190122[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190223 &lt;- lapp(landsat_20190223[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190412 &lt;- lapp(landsat_20190412[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190701 &lt;- lapp(landsat_20190701[[c(4, 3)]], fun = ndvi_fun)\n\nLet’s combine NDVI layers into a single raster stack.\n\nall_ndvi &lt;- c(ndvi_20180612, \n              ndvi_20180815, \n              ndvi_20181018, \n              ndvi_20181103, \n              ndvi_20190122, \n              ndvi_20190223, \n              ndvi_20190412, \n              ndvi_20190701)\n\nNow, update the names of each layer to match the date of each image:\n\nnames(all_ndvi) &lt;- c(\"2018-06-12\", \n                     \"2018-08-15\", \n                     \"2018-10-18\", \n                     \"2018-11-03\", \n                     \"2019-01-22\", \n                     \"2019-02-23\", \n                     \"2019-04-12\", \n                     \"2019-07-01\")"
  },
  {
    "objectID": "course-materials/labs/week8.html#compute-ndvi-for-all-scenes",
    "href": "course-materials/labs/week8.html#compute-ndvi-for-all-scenes",
    "title": "Week 8: Lab",
    "section": "1. Compute NDVI for all scenes",
    "text": "1. Compute NDVI for all scenes\nThe first attempt was pretty clunky and required a lot of copy/pasting. Because we’re performing the same operations over and over again, this is a good opportunity to generalize our workflow into a function!\n\n\n\n\n\n\nTipWhen should I create a function?\n\n\n\nThere are lots and lots of reasons to create a function! The biggest clue that you should consider creating one is if you have several lines of code duplicated.\n\n\nLet’s start over and see how we could do this more efficiently.\nWe’ll clear our environment and redefine our function for NDVI:\n\nrm(list = ls())  \nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\nNext, let’s first sketch out what operations we want to perform so we can figure out what our function needs.\n\n\n\n\n\n\nWarningDon’t run this code!\n\n\n\nBefore diving into the detailed nuances of your function, it’s helpful to start by outlining the major steps you want your function to perform using pseudo-code.\n\n\n\n# NOTE: this code is not meant to run! \n# we're just outlining the function we want to create\n\ncreate_ndvi_layer &lt;- function(){\n  # step 1: read scene\n  landsat &lt;- rast(file)\n  # step 2: rename layer\n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n  # step 3: compute NDVI\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n  return(ndvi)\n}\n\n# What do we notice as what we need to pass into our function?\n\n\n\n\n\n\n\nTipTurning pseudo-code into real code\n\n\n\nNow that we’ve written out the pseudo-code, we need to ask ourselves what argument(s) we would need to pass into our function to set off the process.\nIn this case, the first operation needs a file name to read in. So, we now we need to figure out a way to come up with a set of file names to iterate through.\n\n\nWe want a list of the scenes so that we can tell our function to compute NDVI for each. To do that we look in our data folder for the relevant file. To do so, we want to do the following:\n\nAsk for the names of all the files in the data folder\nSet the pattern option to return the names that end in .tif (the file extension for the Landsat scenes)\nSet the full.names option returns the full file path for each scene\n\n\nfiles &lt;- list.files(\n  here(\"data\"), pattern = \"*.tif\", \n  full.names = TRUE)\n\nNow let’s update our function to work with list of file names we created:\n\nPass function a number that will correspond to the index in the list of file names\n\n\ncreate_ndvi_layer &lt;- function(i){\n  landsat &lt;- rast(files[i])\n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n  return(ndvi)\n}\n\nLet’s test our function by asking it to read in the first file:\n\ntest &lt;- create_ndvi_layer(1)\n\nNow we can use our function to create a NDVI layer for each scene and stack them into a single rasterstack. And then update layer names to match date:\n\nall_ndvi &lt;- c(create_ndvi_layer(1), \n              create_ndvi_layer(2), \n              create_ndvi_layer(3), \n              create_ndvi_layer(4), \n              create_ndvi_layer(5), \n              create_ndvi_layer(6), \n              create_ndvi_layer(7), \n              create_ndvi_layer(8))\n\nnames(all_ndvi) &lt;- c(\"2018-06-12\", \n                    \"2018-08-15\", \n                    \"2018-10-18\", \n                    \"2018-11-03\", \n                    \"2019-01-22\", \n                    \"2019-02-23\", \n                    \"2019-04-12\", \n                    \"2019-07-01\")\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\nIs there a way to improve this last step even further? Wrap it into a for loop? Could we pull the date strings directly from the file names?"
  },
  {
    "objectID": "course-materials/labs/week8.html#get-data-on-vegetation-communities",
    "href": "course-materials/labs/week8.html#get-data-on-vegetation-communities",
    "title": "Week 8: Lab",
    "section": "1. Get data on vegetation communities",
    "text": "1. Get data on vegetation communities\nFirst, we’ll read in a shapefile of study sites:\n\n\nCode\ntm_shape(all_ndvi[[1]]) +\n  tm_raster() +\n  tm_shape(sites) +\n  tm_polygons() +\n  tm_layout(legend.show = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week8.html#extract-ndvi-at-study-sites",
    "href": "course-materials/labs/week8.html#extract-ndvi-at-study-sites",
    "title": "Week 8: Lab",
    "section": "2. Extract NDVI at study sites",
    "text": "2. Extract NDVI at study sites\nNow we want to find the average NDVI within each study site. The output of terra::extract() is a data.frame with rows that match the study site data set, so we bind the results to the original data set.\n\nsites_ndvi &lt;- terra::extract(all_ndvi, sites, fun = \"mean\")\n\nsites_annotated &lt;- cbind(sites, sites_ndvi)\n\nWe’re done! Except our data is very untidy… Let’s tidy it up!\n\nConvert to data frame\nTurn from wide to long format\nTurn layer names into date format\nCombine study sites by vegetation type\nSummarize results within vegetation types\n\n\nsites_clean &lt;- sites_annotated %&gt;%\n  # initial cleaning\n  st_drop_geometry() %&gt;% # drop geometry\n  select(-ID) %&gt;% # remove ID generated by terra::extract()\n  # reformat data frame\n  pivot_longer(!study_site) %&gt;% # reshape data frame\n  rename(\"NDVI\" = value) %&gt;% # assign \"value\" to NDVI\n  # create date attribute\n  mutate(\"year\" = str_sub(name, 2, 5), # pull out elements of date\n         \"month\" = str_sub(name, 7, 8),\n         \"day\" = str_sub(name, -2, -1)) %&gt;%\n  unite(\"date\", 4:6, sep = \"-\") %&gt;% # combine date elements\n  mutate(\"date\" = lubridate::as_date(date)) %&gt;%\n  # rename combine study sites by vegetation type\n  mutate(\"veg_type\" = case_when(study_site == \"forest1\" ~ \"forest\",\n                                study_site == \"forest2\" ~ \"forest\",\n                                study_site == \"forest3\" ~ \"forest\",\n                                study_site == \"grassland\" ~ \"grassland\",\n                                study_site == \"chaparral\" ~ \"chaparral\")) %&gt;%\n  # summarize results by vegetation type\n  group_by(veg_type, date) %&gt;%\n  summarize(\"NDVI\" = mean(NDVI, na.rm = TRUE))"
  },
  {
    "objectID": "course-materials/labs/week8.html#plot-results",
    "href": "course-materials/labs/week8.html#plot-results",
    "title": "Week 8: Lab",
    "section": "3. Plot results",
    "text": "3. Plot results\nLet’s plot the results!\n\n\nCode\nggplot(sites_clean,\n       aes(x = date, y = NDVI,\n           group = veg_type, col = veg_type)) +\n  scale_color_manual(values = c(\"#EAAC8B\", \"#315C2B\",\"#9EA93F\")) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Normalized Difference Vegetation Index (NDVI)\", col = \"Vegetation type\",\n       title = \"Seasonal cycles of vegetation productivity\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\nWhat do we notice in the seasonal cycle in NDVI across vegetation communities?\n\nchaparral: NDVI stays relatively constant throughout the year\nforest: NDVI is lowest in the winter and highest in the summer\ngrassland: NDVI is highest in the winter and lowest in the summer\n\nDoes this match our expectations?"
  },
  {
    "objectID": "course-materials/labs/week1.html",
    "href": "course-materials/labs/week1.html",
    "title": "Week 1: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of map-making in R using the tmap package."
  },
  {
    "objectID": "course-materials/labs/week1.html#why-tmap",
    "href": "course-materials/labs/week1.html#why-tmap",
    "title": "Week 1: Lab",
    "section": "1. Why tmap?",
    "text": "1. Why tmap?\n\nThere are MANY ways to make maps in R, but tmap or “thematic maps” offers the most flexibility.\ntmap can handle vector and raster objects from the sf, sp, raster, and stars packages.\nThe syntax of tmap is based on ggplot2 and the Grammar of Graphics\ntmap supports static AND interactive maps (yay!)\n\n\n\n\n\n\n\nTipMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/labs/week1.html#set-up",
    "href": "course-materials/labs/week1.html#set-up",
    "title": "Week 1: Lab",
    "section": "2. Set up",
    "text": "2. Set up\n\nDownload the week 1 materials needed for our lab. Upload this folder to your EDS 223 repository, in your Week 1 folder.\nNavigate to the tmap_intro.qmdfile. Follow along with the in class lab and fill out this qmd as you go!\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"stars\")\ninstall.packages(\"tmap\")\n\n\nlibrary(sf) # for vector data (more soon!)\nlibrary(stars) # for raster data (more soon!)\nlibrary(tmap) # for static and interactive maps\nlibrary(here)"
  },
  {
    "objectID": "course-materials/labs/week1.html#specifying-spatial-data",
    "href": "course-materials/labs/week1.html#specifying-spatial-data",
    "title": "Week 1: Lab",
    "section": "3. Specifying spatial data",
    "text": "3. Specifying spatial data\nSimilar to plotting in ggplot2, in order to plot spatial data, at least two aspects need to be specified:\n\nthe spatial data object to plot (similar to ggplot(data = ))\nand the plotting method (similar to geom_points())\n\nLet’s load three spatial data objects to plot:\n\na raster (more on this next week!) of elevations of the world\na vector dataset (again, more soon!) of country boundaries\na vector dataset of locations of major cities\n\n\n# raster of global elevations\nworld_elevation &lt;- read_stars(here(\"data\",\"week1\",\"worldelevation.tif\"))\n\n# country boundaries\nworld_vector &lt;- read_sf(here(\"data\",\"week1\",\"worldvector.gpkg\"))\n\n# major cities\nworld_cities &lt;- read_sf(here(\"data\",\"week1\",\"worldcities.gpkg\"))\n\n\nShapes and layers\nIn tmap, the spatial object to plot needs to be defined within the function tm_shape(). This is analogous to defining the data frame to plot in ggplot2 using ggplot(data = ).\nLet’s start by plotting the countries of the world.\n\n# plotting a single spatial object\n\ntm_shape(world_vector) + # defines the spatial object to plot\n  tm_polygons() # defines how to plot the object\n\n\n\n\n\n\n\n\n\n\nShapes hierarchy\nSimilar to ggplot2, we can plot multiple datasets by adding layers. When multiple spatial objects are being plotted, each has to be defined in a separate tm_shape() call.\nNow let’s plot the following two spatial objects:\n\ncountries of the world\nmajor cities of the world\n\nIn the next section we’ll unpack the difference between tm_polygons() and tm_dots(), but for now let’s just pay attention to the syntax of how we plot multiple spatial objects. Each spatial object needs to be specified using tm_shape() followed by a function for how to plot it.\n\n# plotting two spatial objects\n\ntm_shape(world_vector) + # defines the FIRST spatial object to plot\n  tm_polygons() + # defines how to plot the FIRST object\ntm_shape(world_cities) + # defines the SECOND objet to plot\n  tm_dots() # defines how to plot the SECOND object\n\n\n\n\n\n\n\n\nSo far, we’ve only tried plotting vector data (more on what this means next week!), but one of the major advantages of tmap is that it allows us to plot vector and raster on the same map.\nLet’s try on example of this by adding information on global elevations to our previous map.\n\n# plotting vector and raster spatial objects\n\ntm_shape(world_elevation) + # plot global elevations\n  tm_raster() + # tm_raster for raster data\ntm_shape(world_vector) +\n  tm_borders() +\ntm_shape(world_cities) +\n  tm_dots() +\n  tm_text(\"name\")\n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\nSimilar to ggplot2 the order of the “layers” matters! The order in which datasets are plotted defines how they are layered (think of this is as adding layers of paint). Spatial objects have extra features which additionally change this behavior: spatial extent and projection. When creating maps with tmap, whichever dataset is used in the first tm_shape() call sets the spatial extent and projection (more details next week!) for the entire map.\nFor example, if we swapped the order of tm_shape() calls in the previous example, we’d end up with a different map.\n\ntm_shape(world_cities) + # plot world_cities first\n  tm_dots() +\n  tm_text(\"name\") +\ntm_shape(world_elevation) +\n  tm_raster() +\ntm_shape(world_vector) +\n  tm_borders() \n\n[tip] Consider a suitable map projection, e.g. by adding `+ tm_crs(\"auto\")`.\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nSometimes this can present sticky issues! Imagine the case where we want to use the spatial extent and projection from the world_cities data, but want it plotted on top of the other datasets. We can do this by changing the main shape using the is.main argument.\n\ntm_shape(world_elevation) + \n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() +\ntm_shape(world_cities, is.main = TRUE) + # plot world_cities last, but set as main shape\n  tm_dots() +\n  tm_text(\"name\")\n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\nMap extent\nOne thing to consider when making maps is what area we want to show on the map – the spatial extent of our map. This isn’t an issue when we want to map all of our data (spatial extent of our data matches our desired map extent). But often our data will represent a larger region than what we want to map.\nWe have two options:\n\nprocess our data to create a new spatial object for exactly what we want to map (fine, but annoying)\nchange the extent of a map\n\ntmap has a few options for changing the map extent. The first is by defining a bounding box that specifies the minimum and maximum coordinates in the x and y directions that we want to represent. The values need to be in the units of the original data or we can create a bounding box using st_bbox().\nFor example, let’s restrict our previous map to just Europe using a set of min/max values.\n\ntm_shape(world_elevation, bbox = c(-15, 35, 45, 65)) + # add bounding box to restrict extent\n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() \n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\nWarning: Current projection unknown. Long lat coordinates (wgs84) assumed.\n\n\n\n\n\n\n\n\n\nWe can also restrict the extent of the map using the extent of a dataset. For example, we can restrict the map using the extent of the world_cities data.\n\ntm_shape(world_elevation, bbox = world_cities) + # bounding box = extent of world_cities\n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() \n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipBonus Tip\n\n\n\n\n\nYou can also restrict the map extent using an OpenStreetMap tool called Nominatim to automatically generate minimum and maximum coordinates in the x and y directions based on the provided query.\n\ntm_shape(world_elevation, bbox = \"Europe\") + # query the region of Europe\n  tm_raster(col.scale = tm_scale(values = terrain.colors(8)))"
  },
  {
    "objectID": "course-materials/labs/week1.html#layers",
    "href": "course-materials/labs/week1.html#layers",
    "title": "Week 1: Lab",
    "section": "4. Layers",
    "text": "4. Layers\nAgain following the syntax of ggplot2 which uses layers to plot data (e.g. geom_point()), tmap also uses layers! We’ve already used layers in our previous examples (e.g. tm_borders()), but now we’ll dig into them in more detail. All possible layer types can be found in the table below:\n\n\n\nMap layers.\n\n\nFunction\nElement\nGeometry\n\n\n\n\nBasic functions\n\n\ntm_polygons()\npolygons (borders and fill)\npolygons\n\n\ntm_symbols()\nsymbols\npoints, polygons, and lines\n\n\ntm_lines()\nlines\nlines\n\n\ntm_raster()\nraster\nraster\n\n\ntm_text()\ntext\npoints, polygons, and lines\n\n\ntm_basemap()\ntile\n\n\n\ntm_tiles()\ntile\n\n\n\nDerived functions\n\n\ntm_borders()\npolygons (borders)\npolygons\n\n\ntm_fill()\npolygons (fill)\npolygons\n\n\ntm_bubbles()\nbubbles\npoints, polygons, and lines\n\n\ntm_dots()\ndots\npoints, polygons, and lines\n\n\ntm_markers()\nmarker symbols\npoints, polygons, and lines\n\n\ntm_square()\nsquares\npoints, polygons, and lines\n\n\ntm_iso()\nlines with text labels\nlines\n\n\ntm_rgb()/tm_rgba()\nraster (RGB image)\nraster\n\n\n\n\n\n\nPolygons\nThe main function to visualize polygons is tm_polygons(). By default, it plots the internal area of the polygon in light grey and the polygon borders in slightly darker grey.\n\ntm_shape(world_vector) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nWe modify the colors using the fill and col arguments and other arguments borrowed from ggplot2.\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"lightblue\",\n              col = \"black\",\n              lwd = 0.5,\n              lty = \"dashed\")\n\n\n\n\n\n\n\n\nBut, you may have noticed in the previous table that tm_polygons isn’t the only function we can use to plot polygon data. In fact, tm_polygons is a combination of two separate functions - tm_fill() and tm_borders().\nThe tm_borders() function plots just the borders and the tm_fill() function fills polygons with a fixed color or a color palette representing a selected variable.\n\n# plot just borders\n\ntm_shape(world_vector) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n\n\n\n\n# fill polygons with fixed color\n\ntm_shape(world_vector) +\n  tm_fill(fill = \"lightblue\")\n\n\n\n\n\n\n\n\n\n# fill polygons with a color palette representing a variable\n\ntm_shape(world_vector) +\n  tm_fill(\"CO2_emissions\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningSyntax differences\n\n\n\nNote that to change the border color in tm_polygons() we used the fill argument, but in tm_borders() we used the col argument. This is necessary in tm_polygons() to differentiate between the settings for the polygons fill and borders.\n\n\n\n\nSymbols\nSymbols are a very flexible layer type. They typically represent point data, but can also be used for lines and polygons (in this case located at the centroid of each feature). Symbols are also highly flexible in how they can be visualized. They can show the values of a given variable by the color, size, and shape of the symbol.\ntm_symbols() is the main function in tmap to display and modify symbol elements. By default, this function draws a gray circle symbol with a black border for each element of an input feature.\n\ntm_shape(world_cities) +\n  tm_symbols()\n\n\n\n\n\n\n\n\ntm_symbols() has a large number of arguments to flexibly adjust how elements are displayed. While this allows adjusting its results to almost any need, it also makes this function complicated. Therefore, four additional layers are implemented in tmap: tm_squares(), tm_bubbles(), tm_dots(), tm_markers(). All of them use tm_symbols(), but with different default values.\n\ntm_squares(): uses square symbols (shape = 22)\ntm_bubbles(): uses large circle symbols\ntm_dots(): uses small circle symbols (good for displaying many locations)\ntm_markers(): uses marker icons\n\n\ntm_shape(world_cities) +\n  tm_squares()\n\n\n\n\n\n\n\ntm_shape(world_cities) +\n  tm_bubbles()\n\n\n\n\n\n\n\ntm_shape(world_cities) +\n  tm_dots()"
  },
  {
    "objectID": "course-materials/labs/week1.html#visual-variables",
    "href": "course-materials/labs/week1.html#visual-variables",
    "title": "Week 1: Lab",
    "section": "5. Visual variables",
    "text": "5. Visual variables\nFollowing ggplot2 yet again, tmap uses the basic visual variables of color, size, and shape to represent data. Which variables can be applied depends on the type of the map layer.\n\nSymbols: color, size, and shape\nLines: color and size\nPolygons: color\n\nThe type of data (quantitative or qualitative) also determines which visual variables can be used.\n\nColor: quantitative or qualitative\nSize: quantitative\nShape: qualitative\n\n\nColor\ntmap uses the many ways that colors can be specified in R:\n\nbuilt-in color names (e.g. “red”)\nhexadecimal (e.g. #00FF00)\npalettes\n\nThere are dozens of packages that contain hundreds of color palettes. The most popular are RColorBrewer and viridis. By default, tmap attempts to identify the type of the data being plotted and selects on of the built-in palettes.\ntmap offers three main ways to specify color palettes using the palette argument:\n\na vector of colors\na palette function\none of the built-in names\n\nA vector of colors can be specified by name or hexidecimal. Importantly, the number of colors provided does not need to match the number of colors in the map legend. tmap automatically interpolates new colors in the case when a smaller number of colors is provided.\n\n\n\n\n\n\nTipUpdating legend titles\n\n\n\nJust like updating axis labels, we always need to update legend titles. In tmap we can do that by updating the fill.legend argument in the attribute layer.\n\n\nHow would I update the legend to be on the right side of my plot instead of beneath? If you aren’t sure, check the documentation!\n\n# vector of colors\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values = c(\"yellow\", \"darkgreen\")),\n              fill.legend = tm_legend(title = \"Life Expectancy (years)\", \n                                      position = tm_pos_out(\"right\")))\n\n\n\n\n\n\n\n\nAnother approach is to provide the output of a palette function. When using a palette function, you can specify the number of colors to use. Below we use the viridis palette from the viridisLite package.\n\n#install.packages(\"viridisLite\")\nlibrary(viridisLite)\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values =  viridis(8)),\n              fill.legend = tm_legend(title  = \"Life Expectancy (years)\"))\n\n\n\n\n\n\n\n\nFinally, the last approach is to use the name of one of the built-in color palettes.\n\n# built-in color palette\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values = \"brewer.yl_gn\"),\n              fill.legend = tm_legend(title = \"Life Expectancy (years)\"))\n\n\n\n\n\n\n\n\n\n\nSize\nSizes can be used for points, lines (line widths), or text to represent quantitative (numerical) variables. By default, tmap represents points, lines, or text objects as the same size. The size of objects can be changed by using the size argument.\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"lightblue\") +\ntm_shape(world_cities) +\n  tm_symbols(size = \"pop2020\",\n             size.legend = tm_legend(orientation = \"portrait\"))"
  },
  {
    "objectID": "course-materials/labs/week1.html#layout",
    "href": "course-materials/labs/week1.html#layout",
    "title": "Week 1: Lab",
    "section": "6. Layout",
    "text": "6. Layout\nJust like in standard data visualizations, maps have elements that need to be provided in order to interpret them correctly. Maps need to contain either a scale bar and north arrow OR grid lines or graticules. tmap provides these elements (and others) as the following additional attribute layers.\n\n\n\nAttribute layers.\n\n\nFunction\nDescription\n\n\n\n\ntm_grid()\ndraws coordinate grid lines of the coordinate system of the main shape object\n\n\ntm_graticules()\ndraws latitude and longitude graticules\n\n\ntm_scale_bar()\nadds a scale bar\n\n\ntm_compass()\nadds a compass rose\n\n\ntm_credits()\nadds a text annotation\n\n\ntm_logo()\nadds a logo\n\n\ntm_xlab()\nadds an x axis labels\n\n\ntm_ylab()\nadds an y axis labels\n\n\ntm_minimap()\nadds minimap in the view mode only\n\n\n\n\n\n\nGrid lines\nThe tmap package offers two ways to draw coordinate lines - tm_grid() and tm_graticules(). tm_grid() represents the input data’s coordinates.\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_grid()\n\n\n\n\n\n\n\n\ntm_graticules() shows latitude and longitude lines, with degrees as units\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_graticules()\n\n\n\n\n\n\n\n\nBoth tm_grid() and tm_graticules() can be placed above or below other map layers.\n\ntm_shape(world_vector) +\n  tm_graticules() + # graticules below tm_fill()\n  tm_fill()\n\n\n\n\n\n\n\n\n\n\nScale bar and north arrow\nA scale bar is a graphic indicator of the relation between a distance on a map and the corresponding distance in the real world. A north arrow, or a map compass or compass rose, indicates the orientation of the map. North arrows can be added to every map, but are not necessary on maps of large areas (e.g. global maps) where the orientation is obvious.\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_scalebar() +\n  tm_compass(position = c(\"left\", \"top\"))\n\n\n\n\n\n\n\n\n\n\nLayout options\nSimilar to the theme() function in ggplot2, you can control many of the map elements of the map layout.Play around with some of the different tm_layout options by checking the documentation.\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"wb_income_region\",\n          #fill.scale = tm_scale(values = viridisLite::plasma(5)),\n          fill.legend  = tm_legend(title = \"Regional Income\")) +\n  tm_title(text = \"Global income\",\n            frame = TRUE)+ \n  tm_layout(text.fontfamily = \"Montserrat\",\n            style= \"beaver\",\n            bg.color = \"grey95\")"
  },
  {
    "objectID": "course-materials/labs/week1.html#interactive-options",
    "href": "course-materials/labs/week1.html#interactive-options",
    "title": "Week 1: Lab",
    "section": "7. Interactive options",
    "text": "7. Interactive options\nOne of the most powerful aspects of tmap is the ease of creating interactive maps. tmap has two modes \"plot\" which creates static maps and \"view\" which creates interactive maps that can be easily embedded in quarto docs. It’s as easy as using the tmap_mode()!\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"gdp_per_cap\",\n          fill.legend = tm_legend(title = \"GDP per capita\")) \n\n\n\n\n\n\nTo return to regular plotting mode, simply reset tmap_mode.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\"."
  },
  {
    "objectID": "course-materials/labs/week1.html#saving-maps",
    "href": "course-materials/labs/week1.html#saving-maps",
    "title": "Week 1: Lab",
    "section": "8. Saving maps",
    "text": "8. Saving maps\nMaps can be stored as objects for for adding additional layers and saving programmatically. Maps can be saved directly in tmap using the tmap_save() function.\n\nmap1 &lt;- tm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values =  viridis(8)),\n              fill.legend = tm_legend(title  = \"Life Expectancy (years)\"))\n\ntmap_save(map1, here(\"tmap-example.png\"))"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html",
    "href": "course-materials/labs/notes/week1_notes.html",
    "title": "Week 1: Lab",
    "section": "",
    "text": "tmap\nIn this lab, we’ll explore the basics of map-making in R using the tmap package."
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#why-tmap",
    "href": "course-materials/labs/notes/week1_notes.html#why-tmap",
    "title": "Week 1: Lab",
    "section": "1. Why tmap?",
    "text": "1. Why tmap?\n\nThere are MANY ways to make maps in R, but tmap or “thematic maps” offers the most flexibility.\ntmap can handle vector and raster objects from the sf, sp, raster, and stars packages.\nThe syntax of tmap is based on ggplot2 and the Grammar of Graphics\ntmap supports static AND interactive maps (yay!)\n\n\n\n\n\n\n\nTipMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#set-up",
    "href": "course-materials/labs/notes/week1_notes.html#set-up",
    "title": "Week 1: Lab",
    "section": "2. Set up",
    "text": "2. Set up\n\nDownload the week 1 materials needed for our lab. Upload this folder to your EDS 223 repository, in your Week 1 folder.\nNavigate to the tmap_intro.qmdfile. Follow along with the in class lab and fill out this qmd as you go!\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"stars\")\ninstall.packages(\"tmap\")\n\n\nlibrary(sf) # for vector data (more soon!)\nlibrary(stars) # for raster data (more soon!)\nlibrary(tmap) # for static and interactive maps\nlibrary(here)"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#specifying-spatial-data",
    "href": "course-materials/labs/notes/week1_notes.html#specifying-spatial-data",
    "title": "Week 1: Lab",
    "section": "3. Specifying spatial data",
    "text": "3. Specifying spatial data\nSimilar to plotting in ggplot2, in order to plot spatial data, at least two aspects need to be specified:\n\nthe spatial data object to plot (similar to ggplot(data = ))\nand the plotting method (similar to geom_points())\n\nLet’s load three spatial data objects to plot:\n\na raster (more on this next week!) of elevations of the world\na vector dataset (again, more soon!) of country boundaries\na vector dataset of locations of major cities\n\n\n# raster of global elevations\nworld_elevation &lt;- read_stars(here(\"data\",\"week1\",\"worldelevation.tif\"))\n\n# country boundaries\nworld_vector &lt;- read_sf(here(\"data\",\"week1\",\"worldvector.gpkg\"))\n\n# major cities\nworld_cities &lt;- read_sf(here(\"data\",\"week1\",\"worldcities.gpkg\"))\n\n\nCheck tmap version\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] here_1.0.1  tmap_4.1    stars_0.6-8 abind_1.4-8 sf_1.0-21  \n\nloaded via a namespace (and not attached):\n [1] s2_1.1.7                class_7.3-22            lwgeom_0.2-14          \n [4] KernSmooth_2.23-21      lattice_0.21-8          digest_0.6.37          \n [7] magrittr_2.0.3          evaluate_1.0.3          grid_4.3.1             \n[10] RColorBrewer_1.1-3      fastmap_1.2.0           rprojroot_2.0.4        \n[13] jsonlite_2.0.0          e1071_1.7-16            leafsync_0.1.0         \n[16] DBI_1.2.3               crosstalk_1.2.1         viridisLite_0.4.2      \n[19] cols4all_0.8            XML_3.99-0.18           codetools_0.2-19       \n[22] cli_3.6.4               rlang_1.1.6             units_0.8-7            \n[25] tmaptools_3.2           base64enc_0.1-3         yaml_2.3.10            \n[28] leaflegend_1.2.1        tools_4.3.1             raster_3.6-32          \n[31] parallel_4.3.1          maptiles_0.9.0          colorspace_2.1-1       \n[34] spacesXYZ_1.5-1         vctrs_0.6.5             logger_0.4.0           \n[37] R6_2.6.1                png_0.1-8               lifecycle_1.0.4        \n[40] proxy_0.4-27            classInt_0.4-11         leaflet_2.2.2          \n[43] leaflet.providers_2.0.0 htmlwidgets_1.6.4       pkgconfig_2.0.3        \n[46] pillar_1.10.1           terra_1.8-42            data.table_1.17.0      \n[49] glue_1.8.0              Rcpp_1.0.14             tibble_3.2.1           \n[52] xfun_0.52               rstudioapi_0.15.0       knitr_1.50             \n[55] dichromat_2.0-0.1       htmltools_0.5.8.1       rmarkdown_2.29         \n[58] leafem_0.2.4            wk_0.9.4                compiler_4.3.1         \n[61] sp_2.2-0               \n\n\n\n\nShapes and layers\nIn tmap, the spatial object to plot needs to be defined within the function tm_shape(). This is analogous to defining the data frame to plot in ggplot2 using ggplot(data = ) + geom_point().\nLet’s start by plotting the countries of the world.\n\n# plotting a single spatial object\n\ntm_shape(world_vector) + # defines the spatial object to plot\n  tm_polygons() # defines how to plot the object\n\n\n\n\n\n\n\n\n\n\nShapes hierarchy\nSimilar to ggplot2, we can plot multiple datasets by adding layers. When multiple spatial objects are being plotted, each has to be defined in a separate tm_shape() call.\nNow let’s plot the following two spatial objects:\n\ncountries of the world\nmajor cities of the world\n\nIn the next section we’ll unpack the difference between tm_polygons() and tm_dots(), but for now let’s just pay attention to the syntax of how we plot multiple spatial objects. Each spatial object needs to be specified using tm_shape() followed by a function for how to plot it.\n\n# plotting two spatial objects\n\ntm_shape(world_vector) + # defines the FIRST spatial object to plot\n  tm_polygons() + # defines how to plot the FIRST object\ntm_shape(world_cities) + # defines the SECOND objet to plot\n  tm_dots() # defines how to plot the SECOND object\n\n\n\n\n\n\n\n\nNote Can also use tm_symbols and specify the shape number - ex. + tm_symbols(shape = 22)\nSo far, we’ve only tried plotting vector data (more on what this means next week!), but one of the major advantages of tmap is that it allows us to plot vector and raster on the same map.\nLet’s try on example of this by adding information on global elevations to our previous map.\n\n# plotting vector and raster spatial objects\n\ntm_shape(world_elevation) + # plot global elevations\n  tm_raster() + # tm_raster for raster data\ntm_shape(world_vector) +\n  tm_borders() +\ntm_shape(world_cities) +\n  tm_dots() +\n  tm_text(\"name\")\n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\nNote: Could add size = 0.7, col = \"red\" inside tm_text as well to make it look a bit better.\nSimilar to ggplot2 the order of the “layers” matters! The order in which datasets are plotted defines how they are layered (think of this is as adding layers of paint). Spatial objects have extra features which additionally change this behavior: spatial extent and projection. When creating maps with tmap, whichever dataset is used in the first tm_shape() call sets the spatial extent and projection (more details next week!) for the entire map.\nFor example, if we swapped the order of tm_shape() calls in the previous example, we’d end up with a different map.\n\ntm_shape(world_cities) + # plot world_cities first\n  tm_dots() +\n  tm_text(\"name\") +\ntm_shape(world_elevation) +\n  tm_raster() +\ntm_shape(world_vector) +\n  tm_borders() \n\n[tip] Consider a suitable map projection, e.g. by adding `+ tm_crs(\"auto\")`.\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nSometimes this can present sticky issues! Imagine the case where we want to use the spatial extent and projection from the world_cities data, but want it plotted on top of the other datasets. We can do this by changing the main shape using the is.main argument.\n\ntm_shape(world_elevation) + \n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() +\ntm_shape(world_cities, is.main = TRUE) + # plot world_cities last, but set as main shape\n  tm_dots() +\n  tm_text(\"name\")\n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\nMap extent\nOne thing to consider when making maps is what area we want to show on the map – the spatial extent of our map. This isn’t an issue when we want to map all of our data (spatial extent of our data matches our desired map extent). But often our data will represent a larger region than what we want to map.\nWe have two options:\n\nprocess our data to create a new spatial object for exactly what we want to map (fine, but annoying)\nchange the extent of a map\n\ntmap has a few options for changing the map extent. The first is by defining a bounding box that specifies the minimum and maximum coordinates in the x and y directions that we want to represent. The values need to be in the units of the original data or we can create a bounding box using st_bbox().\nFor example, let’s restrict our previous map to just Europe using a set of min/max values.\n\ntm_shape(world_elevation, bbox = c(-15, 35, 45, 65)) + # add bounding box to restrict extent\n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() \n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\nWarning: Current projection unknown. Long lat coordinates (wgs84) assumed.\n\n\n\n\n\n\n\n\n\nNote: bbox coordinates should be in whatever your crs is. In our case, it is WGS84, so it is in lat long coordinates\nbbox = (xmin (west), ymin (south), xmax (east), ymax (north))\nWe can also restrict the extent of the map using the extent of a dataset. For example, we can restrict the map using the extent of the world_cities data.\n\ntm_shape(world_elevation, bbox = world_cities) + # bounding box = extent of world_cities\n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() \n\n[scale] tm_raster:() the data variable assigned to 'col' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipBonus Tip\n\n\n\n\n\nYou can also restrict the map extent using an OpenStreetMap tool called Nominatim to automatically generate minimum and maximum coordinates in the x and y directions based on the provided query.\n\ntm_shape(world_elevation, bbox = \"Europe\") + # query the region of Europe\n  tm_raster(col.scale = tm_scale(values = terrain.colors(8)))"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#layers",
    "href": "course-materials/labs/notes/week1_notes.html#layers",
    "title": "Week 1: Lab",
    "section": "4. Layers",
    "text": "4. Layers\nAgain following the syntax of ggplot2 which uses layers to plot data (e.g. geom_point()), tmap also uses layers! We’ve already used layers in our previous examples (e.g. tm_borders()), but now we’ll dig into them in more detail. All possible layer types can be found in the table below:\n\n\n\nMap layers.\n\n\nFunction\nElement\nGeometry\n\n\n\n\nBasic functions\n\n\ntm_polygons()\npolygons (borders and fill)\npolygons\n\n\ntm_symbols()\nsymbols\npoints, polygons, and lines\n\n\ntm_lines()\nlines\nlines\n\n\ntm_raster()\nraster\nraster\n\n\ntm_text()\ntext\npoints, polygons, and lines\n\n\ntm_basemap()\ntile\n\n\n\ntm_tiles()\ntile\n\n\n\nDerived functions\n\n\ntm_borders()\npolygons (borders)\npolygons\n\n\ntm_fill()\npolygons (fill)\npolygons\n\n\ntm_bubbles()\nbubbles\npoints, polygons, and lines\n\n\ntm_dots()\ndots\npoints, polygons, and lines\n\n\ntm_markers()\nmarker symbols\npoints, polygons, and lines\n\n\ntm_square()\nsquares\npoints, polygons, and lines\n\n\ntm_iso()\nlines with text labels\nlines\n\n\ntm_rgb()/tm_rgba()\nraster (RGB image)\nraster\n\n\n\n\n\n\nPolygons\nThe main function to visualize polygons is tm_polygons(). By default, it plots the internal area of the polygon in light grey and the polygon borders in slightly darker grey.\n\ntm_shape(world_vector) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nWe modify the colors using the fill and col arguments and other arguments borrowed from ggplot2.\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"lightblue\",\n              col = \"black\",\n              lwd = 0.5,\n              lty = \"dashed\")\n\n\n\n\n\n\n\n\nBut, you may have noticed in the previous table that tm_polygons isn’t the only function we can use to plot polygon data. In fact, tm_polygons is a combination of two separate functions - tm_fill() and tm_borders().\nThe tm_borders() function plots just the borders and the tm_fill() function fills polygons with a fixed color or a color palette representing a selected variable.\n\n# plot just borders\n\ntm_shape(world_vector) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n\n\n\n\n# fill polygons with fixed color\n\ntm_shape(world_vector) +\n  tm_fill(fill = \"lightblue\")\n\n\n\n\n\n\n\n\n\n# fill polygons with a color palette representing a variable\n\ntm_shape(world_vector) +\n  tm_fill(\"CO2_emissions\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningSyntax differences\n\n\n\nNote that to change the border color in tm_polygons() we used the fill argument, but in tm_borders() we used the col argument. This is necessary in tm_polygons() to differentiate between the settings for the polygons fill and borders.\n\n\n\n\nSymbols\nSymbols are a very flexible layer type. They typically represent point data, but can also be used for lines and polygons (in this case located at the centroid of each feature). Symbols are also highly flexible in how they can be visualized. They can show the values of a given variable by the color, size, and shape of the symbol.\ntm_symbols() is the main function in tmap to display and modify symbol elements. By default, this function draws a gray circle symbol with a black border for each element of an input feature.\n\ntm_shape(world_cities) +\n  tm_symbols()\n\n\n\n\n\n\n\n\ntm_symbols() has a large number of arguments to flexibly adjust how elements are displayed. While this allows adjusting its results to almost any need, it also makes this function complicated. Therefore, four additional layers are implemented in tmap: tm_squares(), tm_bubbles(), tm_dots(), tm_markers(). All of them use tm_symbols(), but with different default values.\n\ntm_squares(): uses square symbols (shape = 22)\ntm_bubbles(): uses large circle symbols\ntm_dots(): uses small circle symbols (good for displaying many locations)\ntm_markers(): uses marker icons\n\n\ntm_shape(world_cities) +\n  tm_squares()\n\n\n\n\n\n\n\ntm_shape(world_cities) +\n  tm_bubbles()\n\n\n\n\n\n\n\ntm_shape(world_cities) +\n  tm_dots()"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#visual-variables",
    "href": "course-materials/labs/notes/week1_notes.html#visual-variables",
    "title": "Week 1: Lab",
    "section": "5. Visual variables",
    "text": "5. Visual variables\nFollowing ggplot2 yet again, tmap uses the basic visual variables of color, size, and shape to represent data. Which variables can be applied depends on the type of the map layer.\n\nSymbols: color, size, and shape\nLines: color and size\nPolygons: color\n\nThe type of data (quantitative or qualitative) also determines which visual variables can be used.\n\nColor: quantitative or qualitative\nSize: quantitative\nShape: qualitative\n\n\nColor\ntmap uses the many ways that colors can be specified in R:\n\nbuilt-in color names (e.g. “red”)\nhexadecimal (e.g. #00FF00)\npalettes\n\nThere are dozens of packages that contain hundreds of color palettes. The most popular are RColorBrewer and viridis. By default, tmap attempts to identify the type of the data being plotted and selects on of the built-in palettes.\ntmap offers three main ways to specify color palettes using the palette argument:\n\na vector of colors\na palette function\none of the built-in names\n\nA vector of colors can be specified by name or hexidecimal. Importantly, the number of colors provided does not need to match the number of colors in the map legend. tmap automatically interpolates new colors in the case when a smaller number of colors is provided.\n\n\n\n\n\n\nTipUpdating legend titles\n\n\n\nJust like updating axis labels, we always need to update legend titles. In tmap we can do that by updating the fill.legend argument in the attribute layer.\n\n\n\n# vector of colors\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values = c(\"yellow\", \"darkgreen\")),\n              fill.legend = tm_legend(title = \"Life Expectancy (years)\"))\n\n\n\n\n\n\n\n\nHow would I update the legend to be on the right side of my plot instead of beneath? If you aren’t sure, check the documentation!\n\n# vector of colors\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values = c(\"yellow\", \"darkgreen\")),\n              fill.legend = tm_legend(title = \"Life Expectancy (years)\", \n                                      position = tm_pos_out(\"right\")))\n\n\n\n\n\n\n\n\nAnother approach is to provide the output of a palette function. When using a palette function, you can specify the number of colors to use. Below we use the viridis palette from the viridisLite package.\n\n#install.packages(\"viridisLite\")\nlibrary(viridisLite)\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values =  viridis(8)),\n              fill.legend = tm_legend(title  = \"Life Expectancy (years)\"))\n\n\n\n\n\n\n\n\nNoteNotice how we picked 8 colors but only have 7 categories. Like mentioned above, the number of categories doesnt need to match the number of colors!\nFinally, the last approach is to use the name of one of the built-in color palettes.\n\n# built-in color palette\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values = \"brewer.yl_gn\"),\n              fill.legend = tm_legend(title = \"Life Expectancy (years)\"))\n\n\n\n\n\n\n\n\nSome other brewer options :YlOrBr(sequential), Pastel1 (diverging), Accent (qualitative)\n\n\nSize\nSizes can be used for points, lines (line widths), or text to represent quantitative (numerical) variables. By default, tmap represents points, lines, or text objects as the same size. The size of objects can be changed by using the size argument.\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"lightblue\") +\ntm_shape(world_cities) +\n  tm_symbols(size = \"pop2020\",\n             size.legend = tm_legend(orientation = \"portrait\"))"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#layout",
    "href": "course-materials/labs/notes/week1_notes.html#layout",
    "title": "Week 1: Lab",
    "section": "6. Layout",
    "text": "6. Layout\nJust like in standard data visualizations, maps have elements that need to be provided in order to interpret them correctly. Maps need to contain either a scale bar and north arrow OR grid lines or graticules. tmap provides these elements (and others) as the following additional attribute layers.\nNOTE: UPDATE SCLALEBAR : NO MORE _\n\n\n\nAttribute layers.\n\n\nFunction\nDescription\n\n\n\n\ntm_grid()\ndraws coordinate grid lines of the coordinate system of the main shape object\n\n\ntm_graticules()\ndraws latitude and longitude graticules\n\n\ntm_scalebar()\nadds a scale bar\n\n\ntm_compass()\nadds a compass rose\n\n\ntm_credits()\nadds a text annotation\n\n\ntm_logo()\nadds a logo\n\n\ntm_xlab()\nadds an x axis labels\n\n\ntm_ylab()\nadds an y axis labels\n\n\ntm_minimap()\nadds minimap in the view mode only\n\n\n\n\n\n\nGrid lines\nThe tmap package offers two ways to draw coordinate lines - tm_grid() and tm_graticules(). tm_grid() represents the input data’s coordinates.\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_grid()\n\n\n\n\n\n\n\n\nNote: 0 min of arc, 5 min of arc, etc.\n1 degree = 60 min, 0 minutes = exactly on a whole degree, 5 min = 5 minutes of arc -&gt; 5/60 degree\ntm_graticules() shows latitude and longitude lines, with degrees as units\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_graticules()\n\n\n\n\n\n\n\n\nBoth tm_grid() and tm_graticules() can be placed above or below other map layers.\n\ntm_shape(world_vector) +\n  tm_graticules() + # graticules below tm_fill()\n  tm_fill()\n\n\n\n\n\n\n\n\n\n\nScale bar and north arrow\nA scale bar is a graphic indicator of the relation between a distance on a map and the corresponding distance in the real world. A north arrow, or a map compass or compass rose, indicates the orientation of the map. North arrows can be added to every map, but are not necessary on maps of large areas (e.g. global maps) where the orientation is obvious.\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_scalebar() +\n  tm_compass(position = c(\"left\", \"top\"))\n\n\n\n\n\n\n\n\n** Note: What if we wanted north to be south? what argument would we use? ?tm_compass** tm_compass(north = 180)\n\n\nLayout options\nSimilar to the theme() function in ggplot2, you can control many of the map elements of the map layout.Play around with some of the different tm_layout options by checking the documentation.\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"wb_income_region\",\n          #fill.scale = tm_scale(values = viridisLite::plasma(5)),\n          fill.legend  = tm_legend(title = \"Regional Income\")) +\n  tm_title(text = \"Global income\",\n            frame = TRUE)+ \n  tm_layout(text.fontfamily = \"Montserrat\",\n            style= \"beaver\",\n            bg.color = \"grey95\")"
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#interactive-options",
    "href": "course-materials/labs/notes/week1_notes.html#interactive-options",
    "title": "Week 1: Lab",
    "section": "7. Interactive options",
    "text": "7. Interactive options\nOne of the most powerful aspects of tmap is the ease of creating interactive maps. tmap has two modes \"plot\" which creates static maps and \"view\" which creates interactive maps that can be easily embedded in quarto docs. It’s as easy as using the tmap_mode()!\n\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\ntm_shape(world_vector) +\n  tm_polygons(fill = \"gdp_per_cap\",\n          fill.legend = tm_legend(title = \"GDP per capita\")) \n\n\n\n\n\n\nTo return to regular plotting mode, simply reset tmap_mode.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\"."
  },
  {
    "objectID": "course-materials/labs/notes/week1_notes.html#saving-maps",
    "href": "course-materials/labs/notes/week1_notes.html#saving-maps",
    "title": "Week 1: Lab",
    "section": "8. Saving maps",
    "text": "8. Saving maps\nMaps can be stored as objects for for adding additional layers and saving programmatically. Maps can be saved directly in tmap using the tmap_save() function.\n\nmap1 &lt;- tm_shape(world_vector) +\n  tm_polygons(fill = \"life_expectancy\", \n              fill.scale = tm_scale(values =  viridis(8)),\n              fill.legend = tm_legend(title  = \"Life Expectancy (years)\"))\n\ntmap_save(map1, here(\"tmap-example.png\"))"
  },
  {
    "objectID": "course-materials/labs/week6.html",
    "href": "course-materials/labs/week6.html",
    "title": "Week 6: Lab",
    "section": "",
    "text": "In this lab we’ll continue to explore operations that rely on interactions between vector and raster data. Today, we’ll see how to convert raster data into vector data. We’ll also explore creating false color imagery."
  },
  {
    "objectID": "course-materials/labs/week6.html#set-up",
    "href": "course-materials/labs/week6.html#set-up",
    "title": "Week 6: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nFirst, we’ll load all relevant packages.\n\nlibrary(sf) # vector handling\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data"
  },
  {
    "objectID": "course-materials/labs/week6.html#rasterization",
    "href": "course-materials/labs/week6.html#rasterization",
    "title": "Week 6: Lab",
    "section": "2. Rasterization",
    "text": "2. Rasterization\n“Rasterization” is the process of representing vector objects as raster objects. You might consider “rasterizing” vector data for the following reasons:\n\nto use in an analysis that benefits from raster operations (e.g. map algebra)\nstandardize with other data used in analysis\nsimplify data to reduce computational load\naggregated data to standard grid\n\nTo “rasterize” data using the {terra} package, we use the rasterize() function. The first two arguments define the following:\n\nx: vector object to be “rasterized”\ny: a ‘template’ raster object defining the extent, resolution, and CRS of the output\n\n\n\n\n\n\n\nDefining the template raster\n\n\n\nThe geographic resolution of the input raster has a major impact on the results.\n\nIf it is too low (cell size is too large), the result may miss the full geographic variability of the vector data\nIf it is too high (cell size is too small), computational times may be excessive\n\nThere are no simple rules to follow when deciding an appropriate geographic resolution, which is heavily dependent on the intended use of the results. Often the target resolution is imposed on the user, for example when the output of rasterization needs to be aligned to some other existing raster.\n\n\n\nLine and polygon rasterization\nThe simplest case of rasterization is simply converting the geometries of vector objects to raster objects. In this case, all we are hoping to do is indicate within each raster cell whether or not the vector object is present there.\nIn most cases, the purpose behind rasterizing vector object is to make it more directly comparable to data that is represented as raster objects. In that case, you want to use the raster object that you would like to compare to as the ‘template’ raster.\nLet’s check out how this works by investigating an example from Zion National Park\n\n# load Zion park boundary (vector object to rasterize)\nboundary &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")) \n\n# load elevation raster to use as template raster object\nelevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n# check and transform coordinate reference systems\nif(crs(elevation) == crs(boundary)) {\n  print(\"Coordinate reference systems match\")\n} else{\n  warning(\"Updating coordinate reference systems to match\")\n  # transform data to match\n  boundary &lt;- st_transform(boundary, st_crs(elevation))\n}\n\nWarning: Updating coordinate reference systems to match\n\n\nRasterization gives different results for polygon versus line vector objects.\n\nRasterized polygons: all grid cells intersecting polygon (including cells inside polygon)\nRasterized lines: grid cells intersecting line (excluding cells potentially enclose by line)\n\nLet’s check out whether or not the park boundary is represented as polygon or line. We can always inspect the geometry type of a vector object using sf::st_geometry_type().\n\nif(sf::st_geometry_type(boundary) == \"POLYGON\"){\n  print(\"polygon data\")\n} else {\n  print(\"not polygon data\")\n}\n\n[1] \"polygon data\"\n\n\nThe park boundary is a polygon, so let’s make a version that just represents the park border using a line geometry.\n\n# update park boundary object name to clarify that it's a polygon\npark_polygon &lt;- boundary\n\n# create line version park boundary\npark_border &lt;- boundary %&gt;%\n  sf::st_cast(., \"MULTILINESTRING\")\n\n\n\nCode\nmap1 &lt;- tm_shape(park_polygon) +\n  tm_polygons() +\n  tm_title(text = \"polygon\")\n\nmap2 &lt;- tm_shape(park_border) +\n  tm_lines() +\n  tm_title(text = \"line\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nNow we can rasterize both the polygon and line representations of the park boundary.\n\npolygon_raster &lt;- terra::rasterize(park_polygon, elevation)\nborder_raster &lt;- terra::rasterize(park_border, elevation)\n\n\n\nCode\nmap1 &lt;- tm_shape(polygon_raster) +\n  tm_raster() +\n  tm_title(text = \"rasterized polygon\")+ \n  tm_layout(\n    legend.position = c(\"left\", \"bottom\")  \n  )\n\nmap2 &lt;- tm_shape(border_raster) +\n  tm_raster() +\n  tm_title(text = \"rasterized line\")+ \n  tm_layout(\n    legend.position = c(\"left\", \"bottom\")  \n  )\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nIn the case where you are rasterizing without a pre-existing raster you would like to match to, you can create a template raster from scratch.\n\n\n\n\n\n\nImpact of resolution\n\n\n\nUsing a custom template raster is a also a great way to explore how the resolution impacts the results of rasterization!\n\n\n\n# create low spatial resolution template raster\ntemplate_raster_low &lt;- terra::rast(ext(park_polygon), resolution = 0.05,\n                        crs = st_crs(park_polygon))\n\n# create high spatial resolution template raster\ntemplate_raster_high &lt;- terra::rast(ext(park_polygon), resolution = 0.01,\n                        crs = st_crs(park_polygon))\n\n# rasterize to low resolution template raster\npolygon_raster_low &lt;- terra::rasterize(park_polygon, template_raster_low)\n\n# rasterize to high resolution template raster\npolygon_raster_high &lt;- terra::rasterize(park_polygon, template_raster_high)\n\n\n\nCode\nmap1 &lt;- tm_shape(polygon_raster_low) +\n  tm_raster() +\n  tm_title(text = \"low resolution\")+ \n  tm_layout(\n    legend.position = c(\"left\", \"bottom\")  \n  )\n\nmap2 &lt;- tm_shape(polygon_raster_high) +\n  tm_raster() +\n  tm_title(text = \"high resolution\") + \n    tm_layout(\n    legend.position = c(\"left\", \"bottom\")  \n  )\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nRasterizing point data\nWhen working with point data, we can use the same process as with line and polygon data. However, in some cases we might want to perform more complex operations. Instead of simply indicating whether or not the vector object falls within each grid cell, we might want to count the number of points in each grid cell or even summarize the attributes of points within each cell.\nLet’s try an example using data on cycle hire points in London from {spData}.\n\n# define point data\ncycle_hire_osm &lt;- spData::cycle_hire_osm\n\n# transform to projected CRS\ncycle_hire_osm_projected = sf::st_transform(cycle_hire_osm, \"EPSG:27700\")\n\n# define raster template\ntemplate_raster &lt;- terra::rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                       crs = crs(cycle_hire_osm_projected))\n\nIn the following code chunk we’ll perform three versions of rasterization to produce the following:\n\nraster representing the presence/absence of cycle hire points\nraster representing the number of cycle hire points\nraster representing the total capacity of cycle hire points\n\nTo represent the presence/absence of cycle hire points, we use the same procedure as before.\n\nch_raster1 &lt;- terra::rasterize(cycle_hire_osm_projected, template_raster)\n\nTo represent the number of cycle hire points, we use the fun argument which allows for various summarizing functions. Setting the argument fun = \"length\" will return a count of the number of points in each cell\n\nch_raster2 &lt;- rasterize(cycle_hire_osm_projected, template_raster, \n                       fun = \"length\")\n\nTo represent the total capacity of cycle hire points, we need to take the sum of each points’ capacity. We can do this by defining which field we would like to aggregate (in this case capacity) and what aggregating function we would like to use (in this case fun = sum).\n\nch_raster3 &lt;- rasterize(cycle_hire_osm_projected, template_raster, \n                       field = \"capacity\", fun = sum, na.rm = TRUE)\n\n\n\nCode\nmap1 &lt;- tm_shape(cycle_hire_osm_projected) +\n  tm_symbols(fill  = \"capacity\",\n             fill.legend= tm_legend(\n            position = tm_pos_out(\"right\", \"center\"))) +\n  tm_title(text = \"original points\") \n\n\nmap2 &lt;- tm_shape(ch_raster1) +\n  tm_raster(col.legend= tm_legend(\"presence\",\n                                   orientation = \"landscape\",\n            position = tm_pos_out(\"right\", \"center\"))) +\n  tm_title(text = \"presence/absence\")\n\n\nmap3 &lt;- tm_shape(ch_raster2) +\n  tm_raster(col.legend= tm_legend(\"Hire points (n)\", \n,\n            position = tm_pos_out(\"right\", \"center\"))) +\n  tm_title(text = \"count of points\")\nmap4 &lt;- tm_shape(ch_raster3) +\n  tm_raster(col.legend= tm_legend(\"Capacity (n bikes)\",\n            position = tm_pos_out(\"right\", \"center\"))) +\n  tm_title(text = \"sum of capacity\")\n\ntmap_arrange(map1,  map2, map3, map4, nrow = 2,  outer.margins = 0.05 )"
  },
  {
    "objectID": "course-materials/labs/week6.html#creating-false-color-images",
    "href": "course-materials/labs/week6.html#creating-false-color-images",
    "title": "Week 6: Lab",
    "section": "3. Creating false color images",
    "text": "3. Creating false color images\n\n\n\n\n\n\nSource Materials\n\n\n\nThe following materials are modified from Humboldt State Geospatial Online.\n\n\nEach band of a multispectral image can be displayed one band at a time as a grey scale image, or in a combination of three bands at a time as a color composite image. Computer screens can display an image in three different channels (red, green, blue) at a time, by using a different primary color for each band. When we combine these three images we get a color composite image.\nNatural or true color composite:\n\ndisplays red, green, blue RS bands in the red, green, blue channels, respectively\n\nFalse color composite:\n\ndisplays RS bands beyond the visible portion of the spectrum, or\ndisplays red, green, blue RS bands not necessarily in the red, green, and blue channels\n\nIn this example, we’ll work with a remote sensing image of Olinda, Brazil collected by the Landsat-7 Enhanced Thematic Mapper.\n\n\n\nLandsat-7 bands\n\n\nBand\nWavelengths\n\n\n\n\nBand 1\n0.45 - 0.52 micrometers (blue)\n\n\nBand 2\n0.52 - 0.60 micrometers (green)\n\n\nBand 3\n0.63 - 0.69 micrometers (red)\n\n\nBand 4\n0.77 - 0.90 micrometers (near-infrared)\n\n\nBand 5\n1.55 - 1.75 micrometers (short-wave infrared)\n\n\nBand 7\n2.08 - 2.35 micrometers (mid-infrared)\n\n\n\n\n\n\n# load Landsat image\nL7 &lt;- terra::rast(system.file(\"tif/L7_ETMs.tif\", package = \"stars\"))\n\nTo plot various band combinations, we can use tmap::tm_rgb() which allows us to define which band should be displayed in each channel.\nWe can explore various band combinations, but there are a few common false color composites:\n\nNear infrared (red channel), red (green channel), green (blue channel):\n\nPlants reflect near infrared and green light, while absorbing red\nPlants appear deep red and therefore helpful for gauging plant health\n\nShortwave infrared (red channel), near infrared (green channel), green (blue channel):\n\nWater absorbs all three wavelengths, so appears black\nWater and wet soil stand out and therefore helpful for monitoring floods\n\n\n\n\nCode\nmap1 &lt;- tm_shape(L7) +\n  tm_rgb(col = tm_vars(c(3,2,1), multivariate = TRUE)) +\n  tm_title(text = \"true color\")\n\nmap2 &lt;- tm_shape(L7) +\n  tm_rgb(col = tm_vars(c(4,3,2),  multivariate = TRUE)) +\n  tm_title(text = \"NIR, red, green\")\n\nmap3 &lt;- tm_shape(L7) +\n  tm_rgb(col = tm_vars(c(5,4,2),  multivariate = TRUE)) +\n  tm_title(text = \"SWIR, NIR, green\")\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nWe can also create create rasters of NDVI (Normalized difference vegetation index) and NDWI (Normalized difference water index).\n\n\nCode\n# define NDVI as the normalized difference between NIR and red bands\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\n# apply NDVI function to Landsat bands 3 & 4\nndvi_rast &lt;- lapp(L7[[c(4, 3)]], fun = ndvi_fun)\n\nnames(ndvi_rast) &lt;- \"NDVI\"\n\n# Visualize NDVI\nndvi_map &lt;- tm_shape(ndvi_rast) +\n  tm_raster(\n    col.scale = tm_scale_continuous(\n      values = \"brewer.rd_yl_gn\"\n    ),\n    col.legend = tm_legend(title = \"NDVI\", \n                           orientation = \"portrait\")\n  ) +\n  tm_title(text = \"NDVI\")\n\ntmap_arrange(map1, map2, ndvi_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# define NDWI as the normalized difference between green and nir bands\nndwi_fun &lt;- function(green, nir){\n  (green - nir) / (green + nir)\n}\n\n# apply NDWI function to Landsat bands 3 & 4\nndwi_rast &lt;- lapp(L7[[c(2, 4)]], fun = ndwi_fun)\nnames(ndwi_rast) &lt;- \"NDWI\"\n\nmap_ndwi &lt;- tm_shape(ndwi_rast) +\n  tm_raster(\n    col.scale = tm_scale_continuous(values = \"brewer.blues\"),\n    col.legend = tm_legend(title = \"NDWI\", \n                           orientation = \"portrait\")\n  ) +\n  tm_title(text = \"NDWI\")\n\n\ntmap_arrange(map1, map3, map_ndwi)"
  },
  {
    "objectID": "course-materials/labs/week5.html",
    "href": "course-materials/labs/week5.html",
    "title": "Week 5: Lab",
    "section": "",
    "text": "In this lab we’ll explore operations that rely on interactions between vector and raster datasets, including how to convert raster data into vector data."
  },
  {
    "objectID": "course-materials/labs/week5.html#set-up",
    "href": "course-materials/labs/week5.html#set-up",
    "title": "Week 5: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nFirst, we’ll load all relevant packages.\n\nlibrary(sf) # vector handling\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data\nlibrary(viridisLite)\n\nToday we’re heading back to Zion National Park in Utah to explore the interactions between vector and raster data.\n\n\n\n\n\n\n\n\n\n\nPhoto from Unsplash\n\nWe’ll load the following data from the {spDataLarge} package:\n\nsrtm.tif: remotely sensed elevation estimates (raster data)\nzion.gpkg: boundary of Zion National Park (vector data)\n\n\n# load raster dataset\nelevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n# load vector dataset\nboundary &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\n\n\n\n\n\n\n\nWarningRemember to check the CRS!\n\n\n\nWhenever we work with multiple spatial datasets, we need check that the coordinate reference systems match. If they don’t, we need to transform one to match the other.\n\n# check if coordinate reference systems match\nif(crs(elevation) == crs(boundary)) {\n  print(\"Coordinate reference systems match\")\n} else{\n  warning(\"Updating coordinate reference systems to match\")\n  # transform data to match\n  boundary &lt;- st_transform(boundary, st_crs(elevation))\n}\n\nWarning: Updating coordinate reference systems to match\n\n\n\n\n\n\nCode\ntm_shape(elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (meters)\")) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2)"
  },
  {
    "objectID": "course-materials/labs/week5.html#raster-cropping",
    "href": "course-materials/labs/week5.html#raster-cropping",
    "title": "Week 5: Lab",
    "section": "2. Raster cropping",
    "text": "2. Raster cropping\nMany geographic data projects involve integrating data from many different sources, such as remote sensing images (rasters) and administrative boundaries (vectors). Often the extent of input raster datasets is larger than the area of interest. In this case, raster cropping and masking are useful for unifying the spatial extent of input data. Both operations reduce object memory use and associated computational resources for subsequent analysis steps and may be a necessary preprocessing step when creating attractive maps involving raster data.\nFirst, let’s crop the extent of the elevation raster to match the extent of Zion’s boundaries. Through this process, we eliminate grid cells that fall outside the extent of the park and reduce the size of the raster. To do so, we use the terra::crop() function.\n\n# crop raster to extent of vector object\nelevation_cropped &lt;- crop(elevation, boundary)\n\nBeyond matching the extent, we can also set the values of raster cells outside of the boundaries or the park to NA using terra::mask().\n\n# mask raster based on vector object\n# (cells outside of vector are converted to NA)\nelevation_masked &lt;- mask(elevation, boundary)\n\nOften, we will want to combine both cropping and masking to reduce the size of the raster as much as possible.\n\n# crop and mask raster\nelevation_final &lt;- mask(elevation_cropped, boundary)\n\nIn some cases, we may want to mask the raster cells inside of the boundaries (i.e. assign cells inside the park to NA). We can do so with terra::mask() by setting the argument inverse = TRUE.\n\n# mask raster based on vector object\n# (cells inside of vector are converted to NA)\nelevation_inv_masked &lt;- mask(elevation_cropped, boundary, inverse = TRUE)\n\n\n\nCode\nmap1 &lt;- tm_shape(elevation) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text = \"original\")\n\nmap2 &lt;- tm_shape(elevation_cropped) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text =  \"cropped\")\n\nmap3 &lt;- tm_shape(elevation_masked) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text  = \"masked\")\n\nmap4 &lt;- tm_shape(elevation_final) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text =  \"cropped & masked\")\n\nmap5 &lt;- tm_shape(elevation_inv_masked) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text =  \"inverse mask\")\n\ntmap_arrange(map1, map2, map3, map4, map5, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week5.html#raster-vectorization",
    "href": "course-materials/labs/week5.html#raster-vectorization",
    "title": "Week 5: Lab",
    "section": "3. Raster vectorization",
    "text": "3. Raster vectorization\nThere are several ways to convert raster data into vector. The most common, and straightforward, is converting raster grid cells into polygons. For more examples, check out Geocomputation with R.\nWe could simply convert all grid cells into polygons, but it may be more helpful to create polygons based on some condition\n\n\n\n\n\n\nTipTip for HW3\n\n\n\nThe following example is relevant to homework assignment 3!\n\n\nIn this example, we’ll select grid cells higher than 2000 meters by masking the elevation raster. We’ll then convert these grid cells into polygons using the terra::as.polygons() function and turn this into a sf object.\n\nelevation_mask &lt;- elevation_final\nelevation_mask[elevation_mask &lt; 2000] &lt;- NA\n\n\n\nelevation_mask_poly &lt;- as.polygons(elevation_mask) %&gt;% \n  st_as_sf()\n\n\n\nCode\nmap1 &lt;- tm_shape(elevation_mask) +\n  tm_raster() +\n  tm_title( text = \"masked raster\")+ \n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n\nmap2 &lt;- tm_shape(elevation_mask_poly) +\n  tm_polygons() +\n  tm_title(text = \"vectorized raster\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\nRayshader\nRayshader is an R package that creates beautiful 2D and 3D maps from elevation data. It uses various shading techniques to make terrain look realistic, including:\n\nLambertian shading - simulates direct sunlight on slopes\nAmbient occlusion - adds shadows in valleys and crevices\nHypsometric tinting - colors based on elevation (greens for low, browns/whites for high)\nTexture shading - emphasizes ridges and drainage patterns\nSphere shading - adds natural ambient lighting effects\n\nThese techniques can be layered together to create stunning, professional-quality terrain maps in just a few lines of code.\n\nRoad trip! We are headed northeast to Bryce Canyon National Park!We will use the rayshader package, as well as osmdatato visualize the hoodoos of Bryce Canyon in 3D.\n\n#install.packages(\"rayshader\")\n#install.packages(\"magick\")\n#install.packages(\"osmdata\")\nlibrary(rayshader)\nlibrary(magick)\nlibrary(osmdata)\nlibrary(raster)\n\nWe’ll use elevation data from Bryce Canyon in Utah (courtesy of Tom Patterson via shadedrelief.com). Download the Bryce Canyon tif from this link. Store this Bryce_Canyon.tif file in your data folder, and be sure to add it to your .gitignore!!\n\nbryce = raster(\"data/Bryce_Canyon.tif\")\n\nbryce_mat = raster_to_matrix(bryce)\n\nbryce_small = resize_matrix(bryce_mat,0.25)\n\n\n\nBuilding Basemaps with Rayshader\n\nbryce_small %&gt;% \n  height_shade() %&gt;%  \n  plot_map() \n\n\nbryce_small %&gt;% \n  height_shade() %&gt;% \n  \n  add_overlay(sphere_shade(bryce_small, texture = \"desert\", \n                           zscale=4, colorintensity = 5), alphalayer=0.5) %&gt;%\n  \n\n  add_shadow(lamb_shade(bryce_small,zscale=6), 0) %&gt;%\n  \n\n  add_shadow(ambient_shade(bryce_small), 0) %&gt;%\n  \n\n  add_shadow(texture_shade(bryce_small,detail=8/10,contrast=9,brightness = 11), 0.1) %&gt;%\n  plot_map()\n\n\nutm_bbox &lt;- c(xmin = 396367.4, xmax = 397975.2, \n              ymin = 4163747.9, ymax = 4165291.0)\n\n\nextent_zoomed = extent(utm_bbox[1], utm_bbox[2], utm_bbox[3], utm_bbox[4])\n\nbryce_zoom = crop(bryce, extent_zoomed)\n\nbryce_zoom_mat = raster_to_matrix(bryce_zoom)\n\nbase_map = bryce_zoom_mat %&gt;% \n  height_shade() %&gt;%\n  add_overlay(sphere_shade(bryce_zoom_mat, texture = \"desert\", colorintensity = 5), alphalayer=0.5) %&gt;%\n  add_shadow(lamb_shade(bryce_zoom_mat), 0) %&gt;%\n  add_shadow(ambient_shade(bryce_zoom_mat),0) %&gt;% \n  add_shadow(texture_shade(bryce_zoom_mat,detail=8/10,contrast=9,brightness = 11), 0.1)\n\nplot_map(base_map)\n\n\n\nUsing OSM to add features to our base map\n\nosm_bbox = c(-112.174228, 37.614998, -112.156230,37.629084)\n\nbryce_highway = opq(osm_bbox) %&gt;% \n  add_osm_feature(\"highway\") %&gt;%\n  osmdata_sf() \nbryce_highway\n\n\nbryce_lines = st_transform(bryce_highway$osm_lines, crs=crs(bryce))\n\ntm_shape(bryce_lines) +\n  tm_lines(col = \"black\") +\n  tm_title(text = \"Open Street Map `highway` attribute in Bryce Canyon National Park\")\n\n\nbryce_trails = bryce_lines %&gt;% \n  filter(highway %in% c(\"path\",\"bridleway\"))\n\nbryce_footpaths = bryce_lines %&gt;% \n  filter(highway %in% c(\"footway\"))\n\nbryce_roads = bryce_lines %&gt;% \n  filter(highway %in% c(\"unclassified\", \"secondary\", \"tertiary\", \"residential\", \"service\"))\n\n\ntrails_layer = generate_line_overlay(bryce_footpaths,extent = extent_zoomed,\n                                    linewidth = 10, color=\"black\", \n                                    heightmap = bryce_zoom_mat) %&gt;% \n  add_overlay(generate_line_overlay(bryce_footpaths,extent = extent_zoomed,\n                                    linewidth = 6, color=\"white\",\n                                    heightmap = bryce_zoom_mat)) %&gt;%\n  add_overlay(generate_line_overlay(bryce_trails,extent = extent_zoomed,\n                                    linewidth = 3, color=\"black\", lty=3, offset = c(2,-2),\n                                    heightmap = bryce_zoom_mat)) %&gt;%\n  add_overlay(generate_line_overlay(bryce_trails,extent = extent_zoomed,\n                                    linewidth = 3, color=\"white\", lty=3,\n                                    heightmap = bryce_zoom_mat)) %&gt;%\n  add_overlay(generate_line_overlay(bryce_roads,extent = extent_zoomed,\n                                    linewidth = 8, color=\"grey30\",\n                                    heightmap = bryce_zoom_mat)) \n\n\nbryce_water_lines = opq(osm_bbox) %&gt;% \n  add_osm_feature(\"waterway\") %&gt;% \n  osmdata_sf() \n\n\ntm_shape(bryce_water_lines$osm_lines) +\n  tm_lines(col = \"blue\") +\n  tm_title(text = \"Open Street Map `waterway` attribute in Bryce Canyon National Park\")\n\n\nbryce_streams = st_transform(bryce_water_lines$osm_lines,crs=crs(bryce)) \n\nstream_layer = generate_line_overlay(bryce_streams,extent = extent_zoomed,\n                                    linewidth = 4, color=\"skyblue2\", \n                                    heightmap = bryce_zoom_mat)\n\n\nbryce_tourism = opq(osm_bbox) %&gt;% \n  add_osm_feature(\"tourism\") %&gt;% \n  osmdata_sf() \n\nbryce_tourism_points = st_transform(bryce_tourism$osm_points,crs=crs(bryce))\n\nbryce_attractions = bryce_tourism_points %&gt;% \n  filter(tourism == \"attraction\")\n\n\nattraction_layer = generate_label_overlay(bryce_attractions, extent = extent_zoomed,\n                                     text_size = 2, point_size = 2, color = \"white\", \n                                     halo_color = \"black\",\n                                     halo_expand = 10, \n                                     halo_blur = 20, \n                                     halo_alpha = 0.8,\n                                     heightmap = bryce_zoom_mat,\n                                     data_label_column = \"name\")\n\n\n\nCreate National Park Map\n\nbase_map %&gt;% \n  add_overlay(stream_layer, alphalayer = 0.8) %&gt;% \n  add_overlay(trails_layer) %&gt;%\n  add_overlay(attraction_layer) %&gt;% \n  plot_map(title_text = \"Bryce Canyon National Park, Utah\",\n           title_bar_color = \"lightgray\", title_bar_alpha = 1)\n\n\n\nCreating a 3D plot of Bryce Canyon’s streams and trails\n\nbase_map %&gt;% \n  add_overlay(stream_layer, alphalayer = 0.8) %&gt;% \n  add_overlay(trails_layer) %&gt;%\n  \nplot_3d(bryce_zoom_mat, windowsize=c(1200,800))\nrender_camera(theta=240,  phi=30, zoom=0.3,  fov=60)\nrender_snapshot()"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html",
    "href": "course-materials/discussions/week2-discussion.html",
    "title": "Week 2: Discussion Section",
    "section": "",
    "text": "In this section, we’ll explore key functions inside the sf package using data from Colombia. Colombia is considered one of the world’s most “megadiverse” countries, hosting close to 10% of the planet’s biodiversity. Per square kilometer, there are more bird, amphibian, butterfly, and frog species in Colombia than anywhere else on Earth. Here, ill-planned road development poses a significant threat to habitat fragmentation, wildlife movement, and water flows. The Colombian government has developed a broad set of guidelines for sustainable road development, called the Lineamientos de Infraestructura Verde Vial (LIVV) or Green Road Infrastructure Guidelines (GRI). The GRI is mandatory for all national roads across Colombia."
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week2-discussion.html#learning-objectives",
    "title": "Week 2: Discussion Section",
    "section": "",
    "text": "Use sf::st_read() to read multiple vector data types\nRetreive the CRS of a vector object with sf::st_crs()\nTransform CRS and match across all vector data types with sf::st_transform()\nPerform dplyr attribute manipulations (e.g. filter(), mutate(), select())\n\n\n\n\n\n\n\nTipMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#get-started",
    "href": "course-materials/discussions/week2-discussion.html#get-started",
    "title": "Week 2: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nAdd (at least) a subfolder to your R project: data\nCreate a Quarto document\n\nLet’s load all necessary packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nYou will be working with the following datasets:\n\nColombia’s Terrestrial Ecosystems (The Nature Conservancy/NatureServe)\nColombia’s Roads (Esri)\nBird Observations (DATAVES)\n\nNext, let’s download our data. Unzip and move this to your version-controlled R Project’s data folder.\n\nRead in the data for Colombia’s ecoregions, roads, and bird observations\n\n\nUse rename() to rename the columns decimal_longitude and decimal_latitude to long and lat in the bird observation dataset\nUse st_as_sf() to convert the bird observation dataset into an sf object\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncol &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"Colombia\", \"Colombia.shp\"))\n\nroads &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"RDLINE_colombia\", \"RDLINE_colombia.shp\"))\n\naves &lt;- read_csv(here::here(\"data\", \"week2-discussion\", \"dataves.csv\")) |&gt; \n  as_tibble() |&gt; \n  rename(long = decimal_longitude,\n         lat = decimal_latitude) |&gt; \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#your-task",
    "href": "course-materials/discussions/week2-discussion.html#your-task",
    "title": "Week 2: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nYou will be wokring with the following datasets:\n\nColombia’s Terrestrial Ecosystems (The Nature Conservancy/NatureServe)\nColombia’s Roads (Esri)\nBird Observations (DATAVES)\n\nNow, to meet this week’s learning objectives, your task:\n\nRead in the data for Colombia’s ecoregions, roads, and bird observations\n\n\nUse st_read() to read vector data (e.g., .shp, .gdb)\nUse rename() or mutate() to rename the columns decimal_longitude and decimal_latitude to long and lat in aves and sf::st_as_sf() to convert it into an sf object\n\nHint: To convert a table into a vector object, you can use st_as_sf() but remember to check the class() of an object first!\n\n\n\nCheck class() of all vector objects (including the converted aves) and use sf::st_geometry_type() to check the geometry type\nUse filter() to select a macro region of interest from N1_MacroBi in Colombia’s ecoregions dataset and save as a new vector data\n\n\nCheck class() of the new vector data\nPlot the new vector data using tmap\n\n\nUse st_crs() to retrieve CRS of all vector objects and assign a new CRS\n\n\nBonus Challenge: Check units of your object with st_crs()$units\nCheck CRS of all vector objects with st_crs()\nst_crs() &lt;- NA is a brute force way to remove a CRS, instead:\n\nFor the bird observations dataset, extract the longitude and latitude from the geometry column and use sf::st_drop_geometry()\nConvert long and lat into a geometry again with st_as_sf() to obtain a proper sf data frame\n\n\n\nLet’s bring all vector data types together\n\n\nCheck that the CRS of the ecoregions and roads datasets match\nTransform CRS of the bird observations data using sf::st_transform() to match with the other datasets\nUse tmap to plot the ecoregions, roads, and bird observations together"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html",
    "href": "course-materials/discussions/week5-discussion.html",
    "title": "Week 5: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from Geocomputation with R by Robin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week5-discussion.html#learning-objectives",
    "title": "Week 5: Discussion Section",
    "section": "1. Learning Objectives",
    "text": "1. Learning Objectives\n\nUse terra functions aggregate() and resample() to create a new raster\nUse terra functions as.polygons() to convert a raster to a vector of polygons"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#get-started",
    "href": "course-materials/discussions/week5-discussion.html#get-started",
    "title": "Week 5: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nCreate a Quarto document\n\nLet’s load all necessary packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\n\nYou will be working with the following datasets: - High points in New Zealand, obtained from spData - Artificial grain dataset (with three classes: clay, silt, and sand), obtained from spData\n\nnz_height &lt;- nz_height\ngrain &lt;- terra::rast(system.file(\"raster/grain.tif\", package = \"spData\"))"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#your-task",
    "href": "course-materials/discussions/week5-discussion.html#your-task",
    "title": "Week 5: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nSubset points higher than 3100 meters in nz_height (from spData) and create a template raster with rast(), where the resolution is 3 km x 3 km for the extent of the subset dataset\n\n\nCount numbers of the highest points in each grid cell\nFind the maximum elevation in each grid cell\n\n\nWith the previous raster, complete the following:\n\n\nAggregate the raster that counts the highest points in New Zealand/Aotearoa\nReduce its geographic resolution by half, such that cells are 6 x 6 km\nPlot the result\nResample back to the original resolution of 3 km x 3 km\n\n\nPolygonize grain and filter to only keep squares that represent clay\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\n\n\n# Load raster data representing grain sizes with the three classes clay, silt and sand\ngrain &lt;- terra::rast(system.file(\"raster/grain.tif\", package = \"spData\"))"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html",
    "href": "course-materials/discussions/week3-discussion.html",
    "title": "Week 3: Discussion Section",
    "section": "",
    "text": "In this section, we’ll explore key vector operations inside the sf package using data from California. California is considered a key global biodiversity hotspot. It is home to an exceptional variety of species and ecosystems as well as high endemism. Its mix of elevation, from the highest to the lowest points in the continental U.S., and its location, near both the ocean and mountains, make it geodiverse and so biologically special. As a result, California has one of the most extensive and advanced systems of protected areas in the U.S. It currently conserves more than 25% of its land. At large, area-based conservation is complex and implementation of conservation sites has been a way for governments to displace Indigenous peoples and local communities."
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week3-discussion.html#learning-objectives",
    "title": "Week 3: Discussion Section",
    "section": "",
    "text": "Explore topological relationships with sf functions: st_intersects(), st_intersection(), st_within(), etc.\nExplore distance relationships with sf functions: st_distance(), st_within_distance(), and st_buffer()\nLearn about spatial and distance-based joins\nPractice writing error/warning messages and unit tests to diagnose outputs"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#get-started",
    "href": "course-materials/discussions/week3-discussion.html#get-started",
    "title": "Week 3: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nAdd (at least) a subfolder to your R project: data\nCreate a Quarto document\n\nLet’s load all necessary packages:\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\nYou will be working with the following datasets:\n\nSanta Barbara County’s City Boundaries (Santa Barbara County)\nCalifornia Protected Areas Database (CPAD)\niNaturalist Research-grade Observations, 2020-2024 (via rinat)\n\nNext, let’s download our data. Unzip and move this to your version-controlled R Project’s data folder.\n\nsb_protected_areas &lt;- st_read(here(\"data\", \"cpad_super_units_sb.shp\")) |&gt;  \n  st_transform(\"ESRI:102009\")\n\nsb_city_boundaries &lt;- st_read(here(\"data\", \"sb_city_boundaries_2003.shp\")) |&gt; \n  st_transform(\"ESRI:102009\")\n\nsb_county_boundary &lt;- st_read(here(\"data\", \"sb_county_boundary_2020.shp\")) |&gt; \n  st_transform(\"ESRI:102009\")\n\naves &lt;- st_read(here(\"data\", \"aves_observations_2020_2024.shp\")) |&gt; \n  st_transform(\"ESRI:102009\")"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#your-task",
    "href": "course-materials/discussions/week3-discussion.html#your-task",
    "title": "Week 3: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nYou will be wokring with the following datasets:\n\nSanta Barbara County’s City Boundaries (Santa Barbara County)\nCalifornia Protected Areas Database (CPAD)\niNaturalist Research-grade Observations, 2020-2024 (via rinat)\n\nNow, to meet this week’s learning objectives, your task:\n\nFind how many bird observations are within protected areas in Santa Barbara County\n\n\nShow the different outputs from a spatial subset and a spatial join\nBonus Challenge: Try it out with a 5 km buffer around the protected areas too!\n\n\nFind the protected areas within 15 km of a city in Santa Barbara County\n\n\nHint: Use dplyr::filter() to select a city from sb_city_boundaries\nExplore the different outputs with st_intersects(), st_intersection(), and st_within()\nPractice a distance-based join with st_is_within_distance()\n\n\nFind the distance between your city of choice and a protected area of your choice\n\n\nNote: st_distance() finds the distance between the geometries’ edges"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html",
    "href": "course-materials/discussions/week6-discussion.html",
    "title": "Week 6: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from curriculum developed by Earth Lab and the Raster Analysis with terra book.\nIn Lab #6, you explored various band combinations and plotted false color images with tm_rgb() from {tmap}. False color images allow you to visually highlight specific features in an image that may otherwise not be readily discernible. To add to your toolbelt, you will now be plotting false color images with plotRGB() from {terra}.\nThe plotRGB() function allows you to apply a stretch to normalize the colors in an image. You can either apply a linear stretch or histogram equalization. A linear stretch distributes the values across the entire histogram range defined by the max/min lower bounds of the original raster. A histogram equalization stretches dense parts of the histogram and condenses sparse parts.\nBut why would you want to normalize the colors in an image?\nYou are provided Landsat data for the site of the Cold Springs fire that occurred near Nederland, CO. The fire occurred from July 10-14, 2016 and the Landsat images are from June 5, 2016 (pre-fire) and August 8, 2016 (post-fire). The multispectral bands, wavelength range, and associated spatial resolution of the first 7 bands in the Landsat 8 sensor are listed below."
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#background",
    "href": "course-materials/discussions/week6-discussion.html#background",
    "title": "Week 6: Discussion Section",
    "section": "1. Background",
    "text": "1. Background\nIn Lab #6, you explored various band combinations and plotted false color images with tm_rgb() from {tmap}. False color images allow you to visually highlight specific features in an image that may otherwise not be readily discernible. To add to your toolbelt, you will now be plotting false color images with plotRGB() from {terra}.\n\n\n\n\n\n\nTipMany Ways To Plot a False Color Image\n\n\n\n\n\nYou can plot false color composites with ggplot2 too! Check out this tutorial.\n\n\n\nThe plotRGB() function allows you to apply a stretch to normalize the colors in an image. You can either apply a linear stretch or histogram equalization. A linear stretch distributes the values across the entire histogram range defined by the max/min lower bounds of the original raster. A histogram equalization stretches dense parts of the histogram and condenses sparse parts.\nBut why would you want to normalize the colors in an image?\n\n\n\n\n\n\n\n\n\n\n\n“When the range of pixel brightness values is closer to 0, a darker image is rendered by default. You can stretch the values to extend to the full 0-255 range of potential values to increase the visual contrast of the image. When the range of pixel brightness values is closer to 255, a lighter image is rendered by default. You can stretch the values to extend to the full 0-255 range of potential values to increase the visual contrast of the image.”\n\nYou are provided Landsat data for the site of the Cold Springs fire that occurred near Nederland, CO. The fire occurred from July 10-14, 2016 and the Landsat images are from June 5, 2016 (pre-fire) and August 8, 2016 (post-fire). The multispectral bands, wavelength range, and associated spatial resolution of the first 7 bands in the Landsat 8 sensor are listed below.\n\n\n\n\n\n\n\n\n\nBand\nWavelength range (nanometers)\nSpatial Resolution (m)\nSpectral Width (nm)\n\n\n\n\nBand 1 - Coastal aerosol\n430 - 450\n30\n2.0\n\n\nBand 2 - Blue\n450 - 510\n30\n6.0\n\n\nBand 3 - Green\n530 - 590\n30\n6.0\n\n\nBand 4 - Red\n640 - 670\n30\n0.03\n\n\nBand 5 - Near Infrared (NIR)\n850 - 880\n30\n3.0\n\n\nBand 6 - Short-Wave Infrared 1 (SWIR1)\n1570 - 1650\n30\n8.0\n\n\nBand 7 - Short-Wave Infrared 2 (SWIR2)\n2110 - 2290\n30\n18"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#get-started",
    "href": "course-materials/discussions/week6-discussion.html#get-started",
    "title": "Week 6: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nAdd (at least) a subfolder to your R project: data\nCreate a Quarto document\n\nLet’s load all necessary packages:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(tmaptools)\n\nNext, let’s download our data. Unzip and move this to your version-controlled R Project’s data folder.\n\n# Set directory for folder\npre_fire_dir &lt;- here::here(\"data\", \"LC80340322016189-SC20170128091153\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npre_fire_bands &lt;- list.files(pre_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npre_fire_rast &lt;- rast(pre_fire_bands)\n\n# Read mask raster\npre_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016189LGN00_cfmask_crop.tif\"))\n\n\n# Set directory for folder\npost_fire_dir &lt;- here::here(\"data\", \"LC80340322016205-SC20170127160728\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npost_fire_bands &lt;- list.files(post_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npost_fire_rast &lt;- rast(post_fire_bands)\n\n# Read mask raster\npost_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016205LGN00_cfmask_crop.tif\"))\n\nAnd define the nbr_fun function to calculate the normalized burn ratio (NBR):\n\nnbr_fun &lt;- function(nir, swir2){\n    (nir - swir2)/(nir + swir2)\n}"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#your-task",
    "href": "course-materials/discussions/week6-discussion.html#your-task",
    "title": "Week 6: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nRename the bands of the pre_fire and post_fire rasters using names()\n\nNext, for each of the pre_fire and post_fire rasters…\n\nMask out clouds and shadows with the pre_mask and post_mask rasters\n\n\nHint: Set mask &gt; 0 to NA\n\n\n\n\n\n\n\nTipDealing with Clouds and Shadows\n\n\n\n\n\n“Extreme cloud cover and shadows can make the data in those areas, un-usable given reflectance values are either washed out (too bright - as the clouds scatter all light back to the sensor) or are too dark (shadows which represent blocked or absorbed light)” (Earth Lab)\n\n\n\n\nPlot a true color composite using plotRGB()\n\n\nMap the red band to the red channel, green to green, and blue to blue\nApply a linear stretch “lin” or histogram equalization “hist”\n\n\n\n\n\n\n\nTipHow To Decide How To “Stretch” Raster Imagery?\n\n\n\n\n\nTo make an informed choice about whether to apply a linear stretch or histogram equalization, check the distribution of reflectance values of the bands in each raster:\n\n# View histogram for each band\nhist(pre_fire_rast,\n     maxpixels = ncell(pre_fire_rast),\n     col = \"orange\")\n\n\n\n\n\nPlot two false color composite using plotRGB()\n\n\nMap the SWIR2 band to the red channel, NIR to green, and green to blue\nApply a linear stretch “lin” or histogram equalization “hist”\n\n\n\n\n\n\n\nTipWhat is the SWIR, NIR, Red false color scheme?\n\n\n\n“Combining SWIR, NIR, and Red bands highlights the presence of vegetation, clear-cut areas and bare soils, active fires, and smoke; in a false color image” (EOS Data Analytics)\n\n\n\nCalculate the normalized burn ratio (NBR)\n\n\nHint: Use lapp() like you previously did for NDVI and NDWI in Week 4\n\nLet’s bring it home!\n\nFind the difference NBR, where \\(dNBR = prefireNBR - postfireNBR\\)\nPlot the dnBR raster\n\n\nBonus Challenge: Use classify() to assign the severity levels below:\n\n\n\n\nSeverity Level\ndNBR Range\n\n\n\n\nEnhanced Regrowth\n&lt; -.1\n\n\nUnburned\n-.1 to +.1\n\n\nLow Severity\n+.1 to +.27\n\n\nModerate Severity\n+.27 to +.66\n\n\nHigh Severity\n&gt; .66"
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html",
    "href": "course-materials/discussions/week0-discussion.html",
    "title": "Week 0: Discussion Section",
    "section": "",
    "text": "In this section, we’ll refresh our data wrangling skills and refamiliarize some key functions, using a subset of the Global Dam Watch (GDW) database. This data source brings together open-access, georeferenced regional and global dam data, in order to improve our understanding of the environmental effects of dams and flood risks."
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html#get-started",
    "href": "course-materials/discussions/week0-discussion.html#get-started",
    "title": "Week 0: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\nCreate a version-controlled R Project\nAdd (at least) a subfolder to your R project: data\n\nLet’s install and load all necessary packages:\n\npackages &lt;- c(\"here\", \"janitor\", \"tidyverse\", \"sf\", \"terra\", \"tmap\", \"spData\", \"spDataLarge\", \"geodata\", \"kableExtra\", \"viridisLite\")\ninstalled_packages &lt;- packages %in% rownames(installed.packages())\n\nif (any(installed_packages == FALSE)) {\n  install.packages(packages[!installed_packages])\n}\n\n\nlibrary(here)\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(kableExtra)\n\nNext, let’s download our GDW data subset to practice our data wrangling skills from EDS 221. Unzip and move this to your version-controlled R Project’s data folder."
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html#load-data",
    "href": "course-materials/discussions/week0-discussion.html#load-data",
    "title": "Week 0: Discussion Section",
    "section": "2. Load Data",
    "text": "2. Load Data\n\nRead in the gdw.csv file as gdw_df (and point to the filepath with help of here())\nUse the |&gt; or %&gt;% operator to pipe your read_csv() output to clean_names()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngdw_df &lt;- read_csv(here(\"data\", \"gdw.csv\")) |&gt;\n  clean_names() # Convert variable names to lower snake case"
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html#data-exploration",
    "href": "course-materials/discussions/week0-discussion.html#data-exploration",
    "title": "Week 0: Discussion Section",
    "section": "3. Data Exploration",
    "text": "3. Data Exploration\n\nShow the first and last 10 rows of gdw_df and use kable() to make nice HTML tables\nPrint the number of rows and number of columns in gdw_df\nPrint the column names in gdw_df\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhead(gdw_df, n = 10) |&gt; \n  kable()\n\ntail(gdw_df, n = 10) |&gt; \n  kable()\n\n\ndim(gdw_df)\nnrow(gdw_df)\nncol(gdw_df)\n\n\nnames(gdw_df)"
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html#index-summarize-subset-data",
    "href": "course-materials/discussions/week0-discussion.html#index-summarize-subset-data",
    "title": "Week 0: Discussion Section",
    "section": "4. Index, Summarize, Subset Data",
    "text": "4. Index, Summarize, Subset Data\n\nUse indexing brackets to extract the gdw_df column containing country names as a vector and data frame\nUse group_by() and summarise() to find the number of dams by dam type in gdw_df\nMake a subset called sub_dam that only contains entries for dam_type == \"Dam\"\nRe-order gdw_df by ascending order of installation year\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncountry_df &lt;- gdw_df[, \"country\"]\ncountry_vec &lt;- gdw_df[[\"country\"]]\n\n\ngdw_df |&gt; \n  group_by(dam_type) |&gt;\n  summarise(count = n()) |&gt;\n  ungroup()\n\n\nsub_dam &lt;- gdw_df |&gt;\n  filter(dam_type == \"Dam\")\n\n\ngdw_df &lt;- gdw_df |&gt;\n  arrange(year_dam)"
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html#data-visualization",
    "href": "course-materials/discussions/week0-discussion.html#data-visualization",
    "title": "Week 0: Discussion Section",
    "section": "5. Data Visualization",
    "text": "5. Data Visualization\n\nMake a bar graph of average height of dam/barrier in meters (dam_hgt_m) by country\nMake a scatterplot of height of dam/barrier versus representative maximum storage capacity of reservoir in million cubic meters (cap_mcm)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngdw_df |&gt;\n  group_by(country) |&gt;\n  summarize(mean_dam_hgt_m = mean(dam_hgt_m, na.rm = TRUE)) |&gt;\n  ungroup() |&gt; \n  ggplot(aes(x = country, y = mean_dam_hgt_m)) +\n    geom_bar(stat = \"identity\") +\n    labs(x = \"Country\",\n         y = \"Average height of dam/barrier in meters\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\n\n\n\n\n\n\n\n\nggplot(data = gdw_df,\n      aes(x = cap_mcm, y = dam_hgt_m)) +\n  geom_point() +\n  labs(x = \"Storage capacity of reservoir in million cubic meters\",\n       y = \"Height of dam/barrier in meters\") +\n  theme_minimal()"
  },
  {
    "objectID": "course-materials/discussions/week0-discussion.html#looking-ahead",
    "href": "course-materials/discussions/week0-discussion.html#looking-ahead",
    "title": "Week 0: Discussion Section",
    "section": "6. Looking Ahead",
    "text": "6. Looking Ahead\nWe have practiced wrangling a data.frame object, but gdw_df also has a shape column, which contains point coordinates (longitude and latitude). Let’s print the first 3 rows of this column and take a look at its class.\n\nhead(gdw_df$shape, n = 3)\n\n[1] \"c(-97.8635419999999, 53.696359)\" \"c(-75.794246, 44.480557)\"       \n[3] \"c(104.321875, 52.2343930000001)\"\n\nclass(gdw_df$shape)\n\n[1] \"character\"\n\n\nWhile gdw_df contains spatial information, it needs to be spatially-enabled to be a spatial object. We use the sf package to do this. Over the next few weeks, you will be using the function st_read() (or read_sf()) to read all your shapefiles and geodatabases.\n\ngdw_st &lt;- st_read(here(\"data\", \"gdw.gdb\")) |&gt;\n  clean_names() # Convert variable names to lower snake case\n\n\ngdw_sf &lt;- read_sf(here(\"data\", \"gdw.gdb\")) |&gt;\n  clean_names() # Convert variable names to lower snake case"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(dismo)\nlibrary(tmap)\nlibrary(patchwork)\nmagpie &lt;- read_csv(here::here(\"data\", \"magpie_obvs.csv\"))\ntule_elk &lt;- read_csv(here::here(\"data\", \"tule_elk_obvs.csv\"))\n\nbioclim_dir &lt;- here::here(\"data\", \"climate\", \"wc2.1_2.5m\")\nbioclim &lt;- list.files(bioclim_dir, pattern = glob2rx(\"*.tif$\"), full.names = TRUE)\nbioclim_sort &lt;- bioclim[\n  # Sort filepaths based on numeric suffix\n  order(\n  # Extract numeric suffix of filenames and convert to numeric\n  as.numeric(gsub(\".*_(\\\\d+)\\\\.tif$\", \"\\\\1\", bioclim)))]\nbioclim_rast &lt;- rast(bioclim_sort)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#name-raster-layers",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#name-raster-layers",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "Name Raster Layers",
    "text": "Name Raster Layers\n\nvariables &lt;- c(\"annualMeanTemp\", \"meanDiurnalRange\", \"isothermality\", \"tempSeasonality\", \"maxTempWarmMonth\", \"maxTempColdMonth\", \"tempAnnualRange\", \"meanTempWetQ\", \"meanTempDryQ\", \"meanTempWarmQ\", \"meanTempColdQ\", \"annualPrecip\", \"precipWetMonth\", \"precipDryMonth\", \"precipSeasonality\", \"precipWetQ\", \"precipDryQ\", \"precipWarmQ\", \"precipColdQ\")\nnames(bioclim_rast) &lt;- variables"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#find-geographic-extent-of-species-occurences",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#find-geographic-extent-of-species-occurences",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "Find Geographic Extent of Species Occurences",
    "text": "Find Geographic Extent of Species Occurences\n\nmagpie_sf &lt;- magpie %&gt;%\n  rename(long = longitude,\n         lat = latitude) %&gt;%\n  drop_na(long, lat) %&gt;%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n\n\n# Obtain geographic extent/bounding box of the species occurrences\nmagpie_bbox &lt;- st_bbox(magpie_sf)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#crop-raster-and-extract-points-for-species-occurences",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#crop-raster-and-extract-points-for-species-occurences",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "Crop Raster and Extract Points for Species Occurences",
    "text": "Crop Raster and Extract Points for Species Occurences\n\n# Crop raster to match the geographic extent of the species occurrences\nbioclim_crop &lt;- crop(bioclim_rast, magpie_bbox)\n\n\n# Extract points from raster for all species occurrences\nbioClim_pts &lt;- as_tibble(extract(bioclim_crop, magpie_sf))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#crop-raster-and-extract-random-points",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#crop-raster-and-extract-random-points",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "Crop Raster and Extract Random Points",
    "text": "Crop Raster and Extract Random Points\n\nset.seed(42) # for reproducibility\n# Generate random sample points from raster\nrandom_pts &lt;- dismo::randomPoints(mask = raster(bioclim_crop[[\"annualMeanTemp\"]]),\n                                  n = nrow(magpie) * 2,\n                                  ext = magpie_bbox)\n\n\n# Extract points from raster for random sample points\nbioClim_random_pts &lt;- as_tibble(extract(bioclim_crop, random_pts))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#view-map",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#view-map",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "View Map",
    "text": "View Map\n\nmap_1 &lt;- tm_shape(raster(bioclim_crop[[\"annualPrecip\"]])) +\n  tm_raster(palette = \"Blues\", title = \"Annual Precipitation\") +\n  tm_shape(magpie_sf) +\n  tm_dots(col = \"#3a5a40\", size = 0.15) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"),\n            legend.bg.color = \"white\")\n\nmap_2 &lt;- tm_shape(raster(bioclim_crop[[\"annualMeanTemp\"]])) +\n  tm_raster(palette = \"-RdYlBu\", title = \"Annual Mean Temp\") +\n  tm_shape(magpie_sf) +\n  tm_dots(col = \"#3a5a40\", size = 0.15) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"),\n            legend.bg.color = \"white\")\n\ntmap_arrange(map_1, map_2)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#view-plot",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#view-plot",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "View Plot",
    "text": "View Plot\n\nplot_1 &lt;- ggplot(data = bioClim_pts, aes(x = annualPrecip, y = annualMeanTemp)) +\n  geom_point(shape = 16, color = \"#3a5a40\") +\n  labs(x = \"Annual Precipitation\",\n       y = \"Annual Mean Temperature\", \n       title = \"Species Climate Niche\") +\n  theme_bw()\n\nplot_2 &lt;- ggplot(data = bioClim_random_pts, aes(x = annualPrecip, y = annualMeanTemp)) +\n  geom_point(shape = 16) +\n  labs(x = \"Annual Precipitation\",\n       y = element_blank(), \n       title = \"Background Climate\") +\n  theme_bw()\n\nplot_1 + plot_2"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#create-generalizable-workflow",
    "href": "course-materials/discussions/answer-keys/week8-discussion-answerKey.html#create-generalizable-workflow",
    "title": "Week 8: Discussion Section - Answer Key",
    "section": "Create Generalizable Workflow",
    "text": "Create Generalizable Workflow\n\nclimate_envelope &lt;- function(clim_rast, clim_var1, clim_var2, occurences, species_name){\n  \n  species_name &lt;- species_name %&gt;%\n    str_to_lower() %&gt;%\n    str_replace_all(\" \", \"_\")\n  \n  occurences_sf &lt;- occurences %&gt;%\n  rename(long = longitude,\n         lat = latitude) %&gt;%\n  drop_na(long) %&gt;%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\n  \n  occurences_bbox &lt;- st_bbox(occurences_sf)\n  \n  clim_crop &lt;- crop(clim_rast, occurences_bbox)\n  \n  clim_pts &lt;- as_tibble(extract(clim_crop, occurences_sf))\n  \n  random_pts &lt;- randomPoints(mask = raster(clim_rast[[clim_var1]]),\n                             n = nrow(occurences) * 2,\n                             ext = occurences_bbox)\n\n  clim_random_pts &lt;- as_tibble(extract(clim_crop, random_pts))\n  \n  map_1 &lt;- tm_shape(raster(clim_crop[[clim_var1]])) +\n    tm_raster(palette = \"Blues\") +\n    tm_shape(occurences_sf) +\n    tm_dots(col = \"#3a5a40\", size = 0.15) +\n    tm_layout(legend.position = c(\"left\", \"bottom\"),\n              legend.bg.color = \"white\")\n  \n  map_2 &lt;- tm_shape(raster(clim_crop[[clim_var2]])) +\n    tm_raster(palette = \"-RdYlBu\") +\n    tm_shape(occurences_sf) +\n    tm_dots(col = \"#3a5a40\", size = 0.15) +\n    tm_layout(legend.position = c(\"left\", \"bottom\"),\n              legend.bg.color = \"white\")\n  \n  plot_1 &lt;- ggplot(data = clim_pts, aes_string(x = clim_var1, y = clim_var2)) +\n    geom_point(shape = 16, color = \"#3a5a40\") +\n    labs(title = \"Species Climate Niche\") +\n    theme_bw()\n  \n  plot_2 &lt;- ggplot(data = clim_random_pts, aes_string(x = clim_var1, y = clim_var2)) +\n    geom_point(shape = 16) +\n    labs(y = element_blank(), \n         title = \"Background Climate\") +\n    theme_bw()\n  \n  assign(paste0(species_name, \"_map_1\"), map_1, envir = .GlobalEnv)\n  assign(paste0(species_name, \"_map_2\"), map_2, envir = .GlobalEnv)\n  assign(paste0(species_name, \"_plot_1\"), plot_1, envir = .GlobalEnv)\n  assign(paste0(species_name, \"_plot_2\"), plot_2, envir = .GlobalEnv)\n  \n}\n\n\nclimate_envelope(clim_rast = bioclim_rast, clim_var1 = \"annualPrecip\", clim_var2 = \"annualMeanTemp\", occurences = tule_elk, species_name = \"Tule Elk\")\n\ntule_elk_map_1\ntule_elk_map_2\ntule_elk_plot_1\ntule_elk_plot_2"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\n# Load raster data representing grain sizes with the three classes clay, silt and sand\ngrain &lt;- terra::rast(system.file(\"raster/grain.tif\", package = \"spData\"))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#subset-points-in-new-zealandaotearoa",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#subset-points-in-new-zealandaotearoa",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Subset Points in New Zealand/Aotearoa",
    "text": "Subset Points in New Zealand/Aotearoa\n\n# Subset New Zealand elevation points to &gt; 3100 meters\nnz_height3100 &lt;- nz_height %&gt;% \n  dplyr::filter(elevation &gt; 3100)\n\n# Create template: define the extent, resolution, and CRS based on nz_height3100\nnz_template &lt;- rast(terra::ext(nz_height3100), \n                    resolution = 3000, \n                    crs = terra::crs(nz_height3100))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#count-points-in-each-grid-cell",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#count-points-in-each-grid-cell",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Count Points in Each Grid Cell",
    "text": "Count Points in Each Grid Cell\n\n# Convert vector points to raster data\n# Function \"length\" returns a count of the elevation points per cell\nnz_raster &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = \"length\")\n\nplot(nz_raster, main = \"Number of Elevation Points &gt; 3100 in Each Grid Cell\")\nplot(st_geometry(nz_height3100), add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#find-maximum-elevation-in-each-grid-cell",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#find-maximum-elevation-in-each-grid-cell",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Find Maximum Elevation in Each Grid Cell",
    "text": "Find Maximum Elevation in Each Grid Cell\n\n# function \"max\" returns maximum elevation value per cell\nnz_raster2 &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = max)\n\nplot(nz_raster2, main = \"Maximum Elevation in Each Grid Cell \")\nplot(st_geometry(nz_height3100), add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#aggregate-and-resample-raster",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#aggregate-and-resample-raster",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Aggregate and Resample Raster",
    "text": "Aggregate and Resample Raster\n\n# Reduce the resolution by combining 2 cells in each direction into larger cells\n# Sum the values of all cells for the resulting elevation value\nnz_raster_low &lt;- aggregate(nz_raster, fact = 2, fun = sum, na.rm = TRUE)\n\n# Convert the new raster's resolution back to the 3kmx3km resolution of original raster\nnz_resample &lt;- resample(nz_raster_low, nz_raster)\n\nplots &lt;- c(nz_raster, nz_resample)\nlabs &lt;- c(\"Original 6 x 6 km\", \"Resample 6 x 6 km\")\nplot(plots, main = labs)\nplot(nz_raster_low, main = \"Resample 3 x 3 km\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#vectorize-raster",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#vectorize-raster",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Vectorize Raster",
    "text": "Vectorize Raster\n\n# Convert raster data to polygon vector data\ngrain_poly &lt;- as.polygons(grain) %&gt;% \n  st_as_sf()\n\nplot(grain, main = \"Grain (Raster)\")\nplot(grain_poly, main = \"Grain (Vector)\")\n\n# Subset polygons to only clay\nclay &lt;- grain_poly %&gt;% \n  dplyr::filter(grain == \"clay\")\n\nplot(clay, main = \"Clay\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\ncol &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"Colombia\", \"Colombia.shp\"))\n\nroads &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"RDLINE_colombia\", \"RDLINE_colombia.shp\"))\n\naves &lt;- readr::read_csv(here::here(\"data\", \"week2-discussion\", \"dataves.csv\")) %&gt;%\n  dplyr::as_tibble() %&gt;%\n  dplyr::rename(long = decimal_longitude) %&gt;%\n  dplyr::mutate(lat = decimal_latitude) %&gt;%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#read-in-vector-data",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#read-in-vector-data",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\ncol &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"Colombia\", \"Colombia.shp\"))\n\nroads &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"RDLINE_colombia\", \"RDLINE_colombia.shp\"))\n\naves &lt;- readr::read_csv(here::here(\"data\", \"week2-discussion\", \"dataves.csv\")) %&gt;%\n  dplyr::as_tibble() %&gt;%\n  dplyr::rename(long = decimal_longitude) %&gt;%\n  dplyr::mutate(lat = decimal_latitude) %&gt;%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#check-class-and-geometry-type",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#check-class-and-geometry-type",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Check Class and Geometry Type",
    "text": "Check Class and Geometry Type\n\nclass(col)\nclass(roads)\nclass(aves)\n\n\nunique(st_geometry_type(col))\nunique(st_geometry_type(roads))\nunique(st_geometry_type(aves))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#select-macro-region-of-interest",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#select-macro-region-of-interest",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Select Macro Region of Interest",
    "text": "Select Macro Region of Interest\n\ncol_andean &lt;- col %&gt;%\n  # Set categorical \"levels\" in attribute N1_MacroBi (subregions of Colombia)\n  dplyr::mutate(N1_MacroBi = as.factor(N1_MacroBi)) %&gt;%\n  # Subset to Andean region of Colombia\n  dplyr::filter(N1_MacroBi == \"Andean\")\n\n\ntm_shape(col_andean) +\n  tm_polygons() +\n  tm_layout(main.title = \"Andean Region of Colombia\",\n            main.title.size = 1)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#play-with-coordinate-reference-system",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#play-with-coordinate-reference-system",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Play with Coordinate Reference System",
    "text": "Play with Coordinate Reference System\n\n# Print the CRS of each spatial object\nst_crs(col)\nst_crs(roads)\nst_crs(aves)\n\n# Print units of each CRS \nst_crs(col)$units\nst_crs(roads)$units\nst_crs(aves)$units\n\nThere are several ways to extract the longitude and latitude from the geometry column.\n\npurr Approach\n\naves_df_purrr &lt;- aves %&gt;%\n  # Extract lat & long from geometry column\n  mutate(lon = unlist(purrr::map(aves$geometry, 1)), # longitude = first component (x)\n         lat = unlist(purrr::map(aves$geometry, 2))) %&gt;% # latitude = second component (y)\n  st_drop_geometry() # Remove geometry column now that it's redundant\n\n\n\nst_coordinates Approach\n\naves_df_st_coords &lt;- aves %&gt;%\n  dplyr::mutate(lon = sf::st_coordinates(.)[,1],# Assign first matrix item to \"lon\"\n                lat = sf::st_coordinates(.)[,2]) %&gt;% # Assign second matrix item to \"lat\"\n  st_drop_geometry() # Remove geometry column now that it's redundant\n\nNext, convert to sf object again.\n\n\nst_as_sf Approach\n\naves_df_purrr &lt;- aves %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#bring-all-vector-data-types-together",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#bring-all-vector-data-types-together",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Bring All Vector Data Types Together",
    "text": "Bring All Vector Data Types Together\n\n# Boolen check if CRS match between 2 datasets\nst_crs(col) == st_crs(roads)\n\n# Transform bird data into same CRS as other Colombia data\naves &lt;- st_transform(aves, crs = st_crs(col))\n\n\n# Simple plot with all 3 data layers\ntm_shape(col) +\n  tm_polygons() +\n  tm_shape(roads) +\n  tm_lines() +\n  tm_shape(aves) +\n  tm_dots() +\n  tm_layout(main.title = \"Colombia ecoregions, roads,\\nand bird observations\",\n            main.title.size = 1)"
  },
  {
    "objectID": "course-materials/week8.html#class-materials",
    "href": "course-materials/week8.html#class-materials",
    "title": "Multispectral Remote Sensing Analysis",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nImage classifiction: unsupervised and supervised approaches\n\n\n Discussion\nPractice function making with rasters"
  },
  {
    "objectID": "course-materials/week8.html#assignment-reminders",
    "href": "course-materials/week8.html#assignment-reminders",
    "title": "Multispectral Remote Sensing Analysis",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 8)\n11/20/2025\n11/20/2025\n\n\nHW\nHomework Assignment #4\n11/13/2025\n11/26/2025\n\n\nPR\nPortfolio Repository\n11/13/2025\n12/06/2025"
  },
  {
    "objectID": "course-materials/week8.html#background-reading",
    "href": "course-materials/week8.html#background-reading",
    "title": "Multispectral Remote Sensing Analysis",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 10"
  },
  {
    "objectID": "course-materials/resources/good-bad-example/bad_example.html",
    "href": "course-materials/resources/good-bad-example/bad_example.html",
    "title": "Unprofessional Output Example",
    "section": "",
    "text": "Unprofessional Documents\nUnprofessional documents are messy and leave the reader struggling to follow the story of the analysis.\nExamples of components:\n\nmissing introduction for purpose of document, as well as section headers\nloading packages throughout document\nmissing comments, documentation between analysis steps, and formal and detailed data citation\nunnecessary outputs and intermediate checks retained in final version\nmessy code indentation for lists, parameters, functions within functions, etc.\nvery plain plot/map that is lacking important components such as legend, appropriate zoom level, name of study area, etc.\nmissing map explanation or general document conclusion\n\n\n\nElephant tracking in Krugar National Park\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.3.3\n\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nfp &lt;- list.files(path = here(\"course-materials\"), pattern = \"elephants.csv\", recursive = TRUE, full.names = TRUE)\n\nelephants &lt;- read_csv(fp) %&gt;% \n    sf::st_as_sf(coords = c(\"location-long\", \"location-lat\"), crs = st_crs(4326)) %&gt;%\n          filter(st_is_valid(.))\n\nRows: 283688 Columns: 11\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): sensor-type, individual-taxon-canonical-name, tag-local-identifier...\ndbl  (4): event-id, location-long, location-lat, external-temperature\nlgl  (1): visible\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nelephants\n\nSimple feature collection with 283688 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 31.06269 ymin: -25.37676 xmax: 32.00439 ymax: -23.97868\nGeodetic CRS:  WGS 84\n# A tibble: 283,688 × 10\n   `event-id` visible timestamp           `external-temperature` `sensor-type`\n *      &lt;dbl&gt; &lt;lgl&gt;   &lt;dttm&gt;                               &lt;dbl&gt; &lt;chr&gt;        \n 1 9421351127 TRUE    2007-08-13 00:30:00                     24 gps          \n 2 9421351128 TRUE    2007-08-13 02:00:00                     23 gps          \n 3 9421351129 TRUE    2007-08-13 03:31:00                     21 gps          \n 4 9421351130 TRUE    2007-08-13 04:00:00                     21 gps          \n 5 9421351131 TRUE    2007-08-13 06:00:00                     22 gps          \n 6 9421351132 TRUE    2007-08-13 07:30:00                     30 gps          \n 7 9421351133 TRUE    2007-08-13 08:00:00                     33 gps          \n 8 9421351134 TRUE    2007-08-13 11:30:00                     36 gps          \n 9 9421351135 TRUE    2007-08-13 12:00:00                     36 gps          \n10 9421351136 TRUE    2007-08-13 15:30:00                     33 gps          \n# ℹ 283,678 more rows\n# ℹ 5 more variables: `individual-taxon-canonical-name` &lt;chr&gt;,\n#   `tag-local-identifier` &lt;chr&gt;, `individual-local-identifier` &lt;chr&gt;,\n#   `study-name` &lt;chr&gt;, geometry &lt;POINT [°]&gt;\n\n\n\nmetadata_df &lt;- read_csv(list.files(path = here(\"course-materials\"),\n                                   pattern = \"elephants_metadata.csv\",\n                                     recursive = TRUE,\n                                     full.names = TRUE))\n\nRows: 14 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): tag-id, animal-id, animal-taxon, animal-comments, animal-life-sta...\ndbl   (4): deploy-off-latitude, deploy-off-longitude, deploy-on-latitude, de...\nlgl   (1): animal-sex\ndttm  (2): deploy-on-date, deploy-off-date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsort(unique(elephants$\"individual-local-identifier\")) == sort(unique(metadata_df$\"animal-id\"))\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\nmin(metadata_df$\"deploy-on-date\")\n\n[1] \"2007-08-12 22:30:00 UTC\"\n\n\n\nmax(metadata_df$\"deploy-off-date\")\n\n[1] \"2009-08-12 21:30:00 UTC\"\n\n\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.3.3\n\nlibrary(rnaturalearth)\nlibrary(osmdata)\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\n\n\nsouth_africa &lt;- rnaturalearth::ne_countries(scale = \"medium\",\n                                    returnclass = \"sf\") %&gt;%\n                                    filter(admin == \"South Africa\")\n\ntm_shape(south_africa) +\n tm_borders(col = \"black\", lwd = 0.5) +\n tm_fill(col = \"white\") +\n tm_shape(elephants) +\ntm_dots(col = \"individual-local-identifier\",\npalette = 'viridis',\nsize = 0.1,\nborder.col = \"black\",\ntitle = \"Individual\") +\n tm_layout(main.title = \"Elephant Observations\",\n   title.position = c(\"center\", \"top\"), title.size = 1.2,\n   legend.show = FALSE)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_dots()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'palette' (rename to 'values') to fill.scale =\ntm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[tm_dots()] Argument `title` unknown.\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n\n\n\n\n\n\n\n\n\nData: https://datarepository.movebank.org"
  },
  {
    "objectID": "course-materials/resources/plotting.html",
    "href": "course-materials/resources/plotting.html",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "In R, various geospatial packages exist for visualizing and manipulating spatial data:\n\nsf\nggplot2\nmapview\nleaflet\ntmap\nterra\n\nVisualizing vector data, raster data, or both overlaid can be accomplished in various ways. This resource provides example code and explanations for which packages are recommended to get the most out of your visualizations.\n\n\n\n\n\n\n\n\n\nPreference\nPackage\n\n\n\n\nAre you already familiar with ggplot for plotting, and you’re interested in a static visualization?\nggplot2\n\n\nAre you interested in highly tailorable map features and options to create either static or interactive visualizations?\ntmap\n\n\nAre you interested in mapping raster data quickly with less code and less flexibility?\nterra::plot\n\n\nAre you interested creating interactive maps quickly with high flexibility?\nleaflet\n\n\nAre you interested in exploring spatial objects of any sort interactively?\nmapview\n\n\n\n\n\n\nsf, tmap and ggplot2 are great options for visualizing vector data, which is tabular data such as points, lines, and polygons.\nExamples of vector file formats:\n\nshapefile (.shp and auxillary files .shx, .dbf, .prj, etc.)\nGeoPackage (.gpkg)\nGeoParquet (.parquet)\nGeoJSON (.geojson)\n\n\n\nUse the gbif API to download species observations for polar bears from the Global Biodiversity Information Facility.\n\n\nShow the code\n# read in a list of items containing polar bear data, including metadata\npb_data &lt;- occ_search(scientificName = \"Ursus maritimus\", \n                      limit = 300)\n\n# subset the imported data to just the relevant dataframe and attributes\npb_obs &lt;- pb_data$data %&gt;% \n  select(decimalLongitude, \n         decimalLatitude, \n         year,\n         country) %&gt;%\n  mutate(year = as.factor(year)) # year = categorical\n  \n# remove rows with NA in any col\npb_obs &lt;- na.omit(pb_obs)\n\n\n\n\n\nSpatial data will not always already contain critical spatial metadata, so you may have to manually assign it using spatial operations before plotting. For example, point data may contain latitude and longitude coordinates into separate columns and may not come with a set coordinate reference system (CRS). sf can help create a geometry column from separate latitude and longitude columns and set the CRS to WGS84, EPSG:4326.\n\n\nShow the code\n# convert separate longitude and latitude columns into \n# cohesive point geometries, and set the CRS\npb_spatial &lt;- pb_obs %&gt;% \n  sf::st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\n\n\n\nStart with the basics: plot the point data using the native R function plot(), which does not include an interactive feature and is not highly tailorable.\n\n\nShow the code\nplot(st_geometry(pb_spatial),\n     main = \"Polar Bear Observations\",\n     col = \"black\",\n     pch = 16,\n     axes = TRUE,\n     xlab = \"Longitude\",\n     ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\nThat static plot is pretty bland, and it has little context without a palette or a basemap.\n\n\n\nMake an interactive map with the color of the points representing the year of the observation on a basemap. This can be done with either mapview or leaflet.\n\n\n\n\nShow the code\nmapview(pb_spatial,\n        zcol = \"year\",\n        map.types = \"Esri.NatGeoWorldMap\",\n        legend = TRUE,\n        layer.name = \"Polar Bear Observations\")\n\n\n\n\n\n\nThe points are clickable when this is rendered locally, and a metadata window pops up for each observation.\n\n\n\n\n\nShow the code\npalette &lt;- colorFactor(palette = 'viridis',\n                       domain = pb_spatial$year)\n\nleaflet(data = pb_spatial) %&gt;%\n  addProviderTiles(\"Esri.NatGeoWorldMap\") %&gt;% \n  addCircleMarkers(\n    radius = 5,\n    color = \"black\",  # point edges\n    fillColor = ~palette(year),\n    fillOpacity = 0.7,\n    stroke = TRUE,\n    weight = 1,  # point edge thickness\n    popup = ~paste(\"Year:\", year) # clickable points, show observation year\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = palette, \n    values = ~year,\n    title = \"Polar Bear Observations\",\n    opacity = 1\n  )\n\n\n\n\n\n\n\n\n\n\nsf and ggplot can be used in conjunction to plot the polar bear observations statically on a basemap. The default x and y gridlines are cohesive with latitude/longitude point data.\n\n\nShow the code\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\") \n\nggplot(data = world) +\n  geom_sf() +\n  geom_sf(data = pb_spatial, \n          aes(fill = year),  # point color based on 'year'\n          color = \"black\",  # point edges black\n          shape = 21,\n          size = 2, \n          alpha = 0.7) +  # transparency\n  labs(title = \"Polar Bear Observations\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  # limit map to polar bear habitat latitudes\n  coord_sf(xlim = c(-180, 180), ylim = c(45, 90), expand = FALSE) +\n  theme_minimal() + \n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nggplot can make more complex maps, too:\n\n\nShow the code\nggplot() +\n  geom_sf(data = world, fill = \"palegreen\", color = \"darkgreen\") +  # Gray land with dark borders\n  geom_sf(data = pb_spatial, \n          aes(fill = year),\n          color = \"black\",\n          shape = 21, \n          size = 2, \n          alpha = 0.7) +\n  # limit the map to polar bear habitat latitudes & add more gridlines\n  coord_sf(xlim = c(-180, 180),\n           ylim = c(45, 90),\n           expand = FALSE) +\n  scale_x_continuous(breaks = seq(-180, 180, by = 10)) +\n  scale_y_continuous(breaks = seq(45, 90, by = 10)) +\n  labs(title = \"Polar Bear Observations\",\n       subtitle = \"CRS EPSG:4326\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  theme(panel.background = element_rect(fill = \"lightblue\"), # blue ocean\n        plot.title = element_text(hjust = 0.5), # center the title\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"right\",\n        legend.box.background = element_rect(color = \"black\", \n                                             size = 0.5)) \n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\ntmap is specifically designed for mapping spatial data with many highly tailorable options, making it more customizable than ggplot. It recognizes spatial objects from sf, terra, and other geospatial packages. tmap can make both static and interactive maps, as it builds on ggplot and leaflet. More detailed basemaps, like those availble from leaflet and ESRI, are only an option in interactive mode for tmap. For large-scale static data, you can load in a simple world map to use as a basemap.\ntmap allows for fine control over the locations of the title and legend. You can choose inside or outside the map, with values between 0-1 specified for the x and y position.\nMake a static tmap:\n\n\nShow the code\n# clarify the default mode is static plot\ntmap_mode(\"plot\")\n\n\nℹ tmap mode set to \"plot\".\n\n\nShow the code\ndata(World)\n\ntm_shape(World) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"white\") +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = 'viridis',\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(\n    bg.color = \"lightblue\",\n    title = \"Polar Bear\\nObservations\",\n    frame = TRUE,\n    title.position = c(0.01, 0.5),\n    title.size = 1.2,\n    legend.frame = TRUE,\n    legend.position = c(0.01, 0.2)\n  )\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_tm_dots()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'palette' (rename to 'values') to fill.scale =\ntm_scale(&lt;HERE&gt;).[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[tm_dots()] Argument `title` unknown.[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`[tip] Consider a suitable map projection, e.g. by adding `+ tm_crs(\"auto\")`.\n\n\n\n\n\n\n\n\n\nMake an interactive tmap:\n\n\nShow the code\n# set mode to interactive\ntmap_mode(\"view\")\n\n\nℹ tmap mode set to \"view\".\n\n\nShow the code\ntm_shape(World) +\n  tm_borders(col = \"black\", \n             lwd = 0.5) +\n  tm_fill(col = \"white\", \n          alpha = 0.5) +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = \"viridis\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(bg.color = \"lightblue\",\n            title = \"Polar Bear Observations\",\n            title.size = 1.2,\n            legend.frame = TRUE)\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.[tm_dots()] Argument `title` unknown.[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n\n\n\n\n\n\n\n\n\n\n\nPolygon data is composed of multiple points connected by lines to create closed shapes. Since there are many polar bears in Canada and Greenland, plot the polar bear observations on top of only Canada and Greenland polygons using both tmap and ggplot.\n\n\nShow the code\ncanada_greenland &lt;- rnaturalearth::ne_countries(scale = \"medium\", \n                                                returnclass = \"sf\") %&gt;% \n                                   filter(admin %in% c(\"Canada\", \"Greenland\"))\n\n\nWe only want to plot the polar bear points that fit within these polygons, so execute a spatial join.\n\n\nShow the code\npb_canada_greenland &lt;- st_join(pb_spatial, \n                               canada_greenland, \n                               join = st_within,\n                               left = FALSE)\n\n\n\n\n\n\nShow the code\nblue_palette &lt;- RColorBrewer::brewer.pal(n = 2, name = \"Blues\")\n\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n       ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# static map setting, this is the default, but\n# needs to be reset if previously set to \"interactive\"\ntmap_mode(\"plot\")\n\n\nℹ tmap mode set to \"plot\".\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10, # number of gridlines on x and y axes\n          alpha = 0.5) +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\",\n            title.size = 1,\n            legend.title.size = 0.9,\n            title.fontface = \"bold\",\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n\n\n\n\n\n\n\n\n\n\n\n\nAdd a polygon for polar bear habitat range from the International Union for Conservation of Nature (IUCN) Red List. This means we have 3 vector objects overlaid: polygons for country borders, points for polar bear observations, and 1 polygon for habitat.\n\n\nShow the code\n# search for file anywhere within \"course-materials\" dir\nhab_filename = \"data_0.shp\"\nhab_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = hab_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\npb_habitat = st_read(hab_fp)\n# convert from a \"simple feature collection\" with \n# 15 fields to just the geometry\npb_habitat_poly &lt;- st_geometry(pb_habitat)\n\n\n\n\nShow the code\nplot(pb_habitat_poly,\n     main = \"Polar Bear Habitat Range\",\n     col = \"lightyellow\",\n     axes = TRUE,\n     xlab = \"Latitude\",\n     ylab = \"Longitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  geom_sf(data = pb_habitat_poly,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          alpha = 0.2) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       subtitle = \"with habitat range\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n      ylab = \"Latitude\") +\n  # limit map window: zoom into Canada and Greenland\n  coord_sf(xlim = st_bbox(canada_greenland)[c(\"xmin\", \"xmax\")],\n           ylim = st_bbox(canada_greenland)[c(\"ymin\", \"ymax\")])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10,\n          alpha = 0.5) +\n  tm_shape(pb_habitat_poly) +\n  tm_fill(col = \"yellow\", \n          alpha = 0.2, \n          title = \"habitat\") +\n  tm_borders(col = \"darkgoldenrod1\") +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\\nwith Habitat Range\",\n            title.size = 1,\n            title.fontface = \"bold\",\n            legend.title.size = 0.9,\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n\n\n\n\n\n\n\n\n\n\n\n\n\nterra specializes in raster data processing, which are n-dimensional arrays.\nExamples of file formats:\n\nGeoTIFF (Tag Image File Format, .tif)\nnetCDF (Network Common Data Form, .nc)\nPNG or JPEG images (.png, .jpg)\n\n\n\nImport a raster of sea ice for the Northern Hemisphere and plot it as simply as possible with terra::plot\n\n\nShow the code\nice_filename = \"N_197812_concentration_v3.0.tif\"\nice_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = ice_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\narctic_ice &lt;- terra::rast(ice_fp)\n\nterra::plot(arctic_ice,\n            main = \"Arctic Sea Ice Concentration\")\n\n\n\n\n\n\n\n\n\nThis data has a default palette; each cell is color-coded in shades of blue to white, where dark blue is 0% ice (open ocean) and white is 100% ice. You can view the default palette (“color table”) with terra::coltab\nThe CRS is projected and in units of meters, with each raster cell representing 25 km x 25 km. See CRS metadata here.\nMake a histogram of the raster values using the base R hist to understand the numerical data distribution:\n\n\nShow the code\nhist(arctic_ice,\n     main = \"Arctic Sea Ice Concentration Raster Values\",\n     xlab = \"Values\",\n     ylab = \"Frequency\",\n     col = \"deepskyblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\n\nIn order to properly plot multiple spatial objects on top of one another, they must have the same CRS. Transform the CRS of the habitat polygon into the CRS of the raster for Arctic sea ice, then plot the habitat polygon onto the raster.\n\n\nShow the code\npb_habitat_arctic &lt;- st_transform(pb_habitat_poly, st_crs(arctic_ice))\n\nterra::plot(arctic_ice,\n            main = \"Sea Ice Concentration and Polar Bear Habitat\")\nterra::plot(pb_habitat_arctic, \n            add = TRUE, \n            border = \"darkgoldenrod1\", \n            col = adjustcolor(\"yellow\", alpha.f = 0.2))\n\n\n\n\n\n\n\n\n\nterra automatically defines the x and y axes ticks based on the spatial metadata of the raster.\n\n\n\nScale the data values between 0-100 and convert the array into a dataframe, because ggplot only accepts tabular data. Assign a new palette using RColorBrewer that is similar to the color table associated with the terra plot above.\n\n\nShow the code\n# reverse color palette to better match the default terra::plot palette values\nblue_palette &lt;- rev(RColorBrewer::brewer.pal(n = 9, name = \"Blues\"))\n\n# scale the data 0-100:\n# there are no NA values in this raster but na.rm = TRUE is good practice\nrange &lt;- range(values(arctic_ice), na.rm = TRUE)\narctic_ice_scaled &lt;- (arctic_ice-range[1]) / (range[2]-range[1]) * 100\n\narctic_ice_df &lt;- terra::as.data.frame(arctic_ice_scaled,\n                               cells = FALSE, # do not create index col\n                               xy = TRUE)  %&gt;% # include lat and long cols\n                 dplyr::rename(ice_concentration = N_197812_concentration_v3.0)\n\nggplot() +\n  geom_raster(data = arctic_ice_df,\n              aes(x = x, y = y,\n                  fill = ice_concentration)) +\n  scale_fill_gradientn(colors = blue_palette) +\n  geom_sf(data = pb_habitat_arctic,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          size = 0.2,\n          alpha = 0.1) +\n  coord_sf(default_crs = st_crs(pb_habitat_arctic)) +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill = \"grey\", color = NA),\n        plot.background = element_rect(fill = \"grey\", color = NA)) +\n  labs(title = \"Sea Ice Concentration\\nand Polar Bear Habitat Range\",\n       subtitle = \"Proj CRS: NSIDC Sea Ice Polar Stereographic North\",\n       fill = \"Sea Ice\\nConcentration\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n\n\n\n\n\nNote that ggplot does not automatically derive the units of meters from the projected CRS from the spatial metadata. Instead, it uses 4326 by default. As a result, we mask the axes ticks. Axes ticks can manually be defined with scale_x_continuous and scale_y_continuous\n\n\n\n\n\nShow the code\n# tmap_mode(\"plot\")\n# \n# #tmap_options(max.categories = 9)\n# \n# tm_shape(arctic_ice_scaled) +\n#   tm_raster(palette = blue_palette,\n#             title = \"Sea Ice\\nConcentration\",\n#             style = \"cont\",\n#             breaks = seq(0, 100, length.out = 11),\n#             midpoint = NA) +\n# tm_shape(pb_habitat_arctic) +\n#   tm_polygons(col = \"yellow\",\n#               border.col = \"darkgoldenrod1\",\n#               lwd = 1,\n#               alpha = 0.1) +\n# tm_graticules(n.x = 5, n.y = 5, \n#               labels.show = TRUE, \n#               labels.size = 0.6,\n#               alpha = 0.3) +\n# tm_layout(title = \"Sea Ice\\nConcentration\\nand Polar Bear\\nHabitat Range\",\n#           main.title.size = 0.8,\n#           title.fontface = \"bold\",\n#           legend.outside = TRUE,\n#           legend.title.size = 1,\n#           legend.outside.position = \"right\",\n#           inner.margins = c(0.1, 0.1, 0.1, 0.1)) +\n# tm_scale_bar(position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nCitation\n\n\n\n\nGBIF, polar bear observation points\nGBIF.org (01 September 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.79778w\n\n\nIUCN Red List, polar bear range polygon\nIUCN. 2024. The IUCN Red List of Threatened Species. Version 2024-1. https://www.iucnredlist.org. Accessed on Septmeber 5, 2024.\n\n\nNSIDC, sea ice concentration raster\nFetterer, F., Knowles, K., Meier, W. N., Savoie, M. & Windnagel, A. K. (2017). Sea Ice Index. (G02135, Version 3). [Data Set]. Boulder, Colorado USA. National Snow and Ice Data Center. https://doi.org/10.7265/N5K072F8. [describe subset used if applicable]. Date Accessed 09-13-2024.\n\n\n\nNSIDC sea ice concentration metadata"
  },
  {
    "objectID": "course-materials/resources/plotting.html#which-mapping-package-should-i-use",
    "href": "course-materials/resources/plotting.html#which-mapping-package-should-i-use",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "Preference\nPackage\n\n\n\n\nAre you already familiar with ggplot for plotting, and you’re interested in a static visualization?\nggplot2\n\n\nAre you interested in highly tailorable map features and options to create either static or interactive visualizations?\ntmap\n\n\nAre you interested in mapping raster data quickly with less code and less flexibility?\nterra::plot\n\n\nAre you interested creating interactive maps quickly with high flexibility?\nleaflet\n\n\nAre you interested in exploring spatial objects of any sort interactively?\nmapview"
  },
  {
    "objectID": "course-materials/resources/plotting.html#vector-data-points",
    "href": "course-materials/resources/plotting.html#vector-data-points",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "sf, tmap and ggplot2 are great options for visualizing vector data, which is tabular data such as points, lines, and polygons.\nExamples of vector file formats:\n\nshapefile (.shp and auxillary files .shx, .dbf, .prj, etc.)\nGeoPackage (.gpkg)\nGeoParquet (.parquet)\nGeoJSON (.geojson)\n\n\n\nUse the gbif API to download species observations for polar bears from the Global Biodiversity Information Facility.\n\n\nShow the code\n# read in a list of items containing polar bear data, including metadata\npb_data &lt;- occ_search(scientificName = \"Ursus maritimus\", \n                      limit = 300)\n\n# subset the imported data to just the relevant dataframe and attributes\npb_obs &lt;- pb_data$data %&gt;% \n  select(decimalLongitude, \n         decimalLatitude, \n         year,\n         country) %&gt;%\n  mutate(year = as.factor(year)) # year = categorical\n  \n# remove rows with NA in any col\npb_obs &lt;- na.omit(pb_obs)\n\n\n\n\n\nSpatial data will not always already contain critical spatial metadata, so you may have to manually assign it using spatial operations before plotting. For example, point data may contain latitude and longitude coordinates into separate columns and may not come with a set coordinate reference system (CRS). sf can help create a geometry column from separate latitude and longitude columns and set the CRS to WGS84, EPSG:4326.\n\n\nShow the code\n# convert separate longitude and latitude columns into \n# cohesive point geometries, and set the CRS\npb_spatial &lt;- pb_obs %&gt;% \n  sf::st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\n\n\n\nStart with the basics: plot the point data using the native R function plot(), which does not include an interactive feature and is not highly tailorable.\n\n\nShow the code\nplot(st_geometry(pb_spatial),\n     main = \"Polar Bear Observations\",\n     col = \"black\",\n     pch = 16,\n     axes = TRUE,\n     xlab = \"Longitude\",\n     ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\nThat static plot is pretty bland, and it has little context without a palette or a basemap.\n\n\n\nMake an interactive map with the color of the points representing the year of the observation on a basemap. This can be done with either mapview or leaflet.\n\n\n\n\nShow the code\nmapview(pb_spatial,\n        zcol = \"year\",\n        map.types = \"Esri.NatGeoWorldMap\",\n        legend = TRUE,\n        layer.name = \"Polar Bear Observations\")\n\n\n\n\n\n\nThe points are clickable when this is rendered locally, and a metadata window pops up for each observation.\n\n\n\n\n\nShow the code\npalette &lt;- colorFactor(palette = 'viridis',\n                       domain = pb_spatial$year)\n\nleaflet(data = pb_spatial) %&gt;%\n  addProviderTiles(\"Esri.NatGeoWorldMap\") %&gt;% \n  addCircleMarkers(\n    radius = 5,\n    color = \"black\",  # point edges\n    fillColor = ~palette(year),\n    fillOpacity = 0.7,\n    stroke = TRUE,\n    weight = 1,  # point edge thickness\n    popup = ~paste(\"Year:\", year) # clickable points, show observation year\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = palette, \n    values = ~year,\n    title = \"Polar Bear Observations\",\n    opacity = 1\n  )\n\n\n\n\n\n\n\n\n\n\nsf and ggplot can be used in conjunction to plot the polar bear observations statically on a basemap. The default x and y gridlines are cohesive with latitude/longitude point data.\n\n\nShow the code\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\") \n\nggplot(data = world) +\n  geom_sf() +\n  geom_sf(data = pb_spatial, \n          aes(fill = year),  # point color based on 'year'\n          color = \"black\",  # point edges black\n          shape = 21,\n          size = 2, \n          alpha = 0.7) +  # transparency\n  labs(title = \"Polar Bear Observations\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  # limit map to polar bear habitat latitudes\n  coord_sf(xlim = c(-180, 180), ylim = c(45, 90), expand = FALSE) +\n  theme_minimal() + \n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nggplot can make more complex maps, too:\n\n\nShow the code\nggplot() +\n  geom_sf(data = world, fill = \"palegreen\", color = \"darkgreen\") +  # Gray land with dark borders\n  geom_sf(data = pb_spatial, \n          aes(fill = year),\n          color = \"black\",\n          shape = 21, \n          size = 2, \n          alpha = 0.7) +\n  # limit the map to polar bear habitat latitudes & add more gridlines\n  coord_sf(xlim = c(-180, 180),\n           ylim = c(45, 90),\n           expand = FALSE) +\n  scale_x_continuous(breaks = seq(-180, 180, by = 10)) +\n  scale_y_continuous(breaks = seq(45, 90, by = 10)) +\n  labs(title = \"Polar Bear Observations\",\n       subtitle = \"CRS EPSG:4326\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  theme(panel.background = element_rect(fill = \"lightblue\"), # blue ocean\n        plot.title = element_text(hjust = 0.5), # center the title\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"right\",\n        legend.box.background = element_rect(color = \"black\", \n                                             size = 0.5)) \n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\ntmap is specifically designed for mapping spatial data with many highly tailorable options, making it more customizable than ggplot. It recognizes spatial objects from sf, terra, and other geospatial packages. tmap can make both static and interactive maps, as it builds on ggplot and leaflet. More detailed basemaps, like those availble from leaflet and ESRI, are only an option in interactive mode for tmap. For large-scale static data, you can load in a simple world map to use as a basemap.\ntmap allows for fine control over the locations of the title and legend. You can choose inside or outside the map, with values between 0-1 specified for the x and y position.\nMake a static tmap:\n\n\nShow the code\n# clarify the default mode is static plot\ntmap_mode(\"plot\")\n\n\nℹ tmap mode set to \"plot\".\n\n\nShow the code\ndata(World)\n\ntm_shape(World) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"white\") +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = 'viridis',\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(\n    bg.color = \"lightblue\",\n    title = \"Polar Bear\\nObservations\",\n    frame = TRUE,\n    title.position = c(0.01, 0.5),\n    title.size = 1.2,\n    legend.frame = TRUE,\n    legend.position = c(0.01, 0.2)\n  )\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_tm_dots()`: migrate the argument(s) related to the scale of the\nvisual variable `fill` namely 'palette' (rename to 'values') to fill.scale =\ntm_scale(&lt;HERE&gt;).[v3-&gt;v4] `tm_dots()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[tm_dots()] Argument `title` unknown.[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`[tip] Consider a suitable map projection, e.g. by adding `+ tm_crs(\"auto\")`.\n\n\n\n\n\n\n\n\n\nMake an interactive tmap:\n\n\nShow the code\n# set mode to interactive\ntmap_mode(\"view\")\n\n\nℹ tmap mode set to \"view\".\n\n\nShow the code\ntm_shape(World) +\n  tm_borders(col = \"black\", \n             lwd = 0.5) +\n  tm_fill(col = \"white\", \n          alpha = 0.5) +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = \"viridis\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(bg.color = \"lightblue\",\n            title = \"Polar Bear Observations\",\n            title.size = 1.2,\n            legend.frame = TRUE)\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.[tm_dots()] Argument `title` unknown.[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`"
  },
  {
    "objectID": "course-materials/resources/plotting.html#vector-data-polygons",
    "href": "course-materials/resources/plotting.html#vector-data-polygons",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "Polygon data is composed of multiple points connected by lines to create closed shapes. Since there are many polar bears in Canada and Greenland, plot the polar bear observations on top of only Canada and Greenland polygons using both tmap and ggplot.\n\n\nShow the code\ncanada_greenland &lt;- rnaturalearth::ne_countries(scale = \"medium\", \n                                                returnclass = \"sf\") %&gt;% \n                                   filter(admin %in% c(\"Canada\", \"Greenland\"))\n\n\nWe only want to plot the polar bear points that fit within these polygons, so execute a spatial join.\n\n\nShow the code\npb_canada_greenland &lt;- st_join(pb_spatial, \n                               canada_greenland, \n                               join = st_within,\n                               left = FALSE)\n\n\n\n\n\n\nShow the code\nblue_palette &lt;- RColorBrewer::brewer.pal(n = 2, name = \"Blues\")\n\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n       ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# static map setting, this is the default, but\n# needs to be reset if previously set to \"interactive\"\ntmap_mode(\"plot\")\n\n\nℹ tmap mode set to \"plot\".\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10, # number of gridlines on x and y axes\n          alpha = 0.5) +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\",\n            title.size = 1,\n            legend.title.size = 0.9,\n            title.fontface = \"bold\",\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n\n\n\n\n\n\n\n\n\n\n\n\nAdd a polygon for polar bear habitat range from the International Union for Conservation of Nature (IUCN) Red List. This means we have 3 vector objects overlaid: polygons for country borders, points for polar bear observations, and 1 polygon for habitat.\n\n\nShow the code\n# search for file anywhere within \"course-materials\" dir\nhab_filename = \"data_0.shp\"\nhab_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = hab_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\npb_habitat = st_read(hab_fp)\n# convert from a \"simple feature collection\" with \n# 15 fields to just the geometry\npb_habitat_poly &lt;- st_geometry(pb_habitat)\n\n\n\n\nShow the code\nplot(pb_habitat_poly,\n     main = \"Polar Bear Habitat Range\",\n     col = \"lightyellow\",\n     axes = TRUE,\n     xlab = \"Latitude\",\n     ylab = \"Longitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  geom_sf(data = pb_habitat_poly,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          alpha = 0.2) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       subtitle = \"with habitat range\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n      ylab = \"Latitude\") +\n  # limit map window: zoom into Canada and Greenland\n  coord_sf(xlim = st_bbox(canada_greenland)[c(\"xmin\", \"xmax\")],\n           ylim = st_bbox(canada_greenland)[c(\"ymin\", \"ymax\")])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10,\n          alpha = 0.5) +\n  tm_shape(pb_habitat_poly) +\n  tm_fill(col = \"yellow\", \n          alpha = 0.2, \n          title = \"habitat\") +\n  tm_borders(col = \"darkgoldenrod1\") +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\\nwith Habitat Range\",\n            title.size = 1,\n            title.fontface = \"bold\",\n            legend.title.size = 0.9,\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`"
  },
  {
    "objectID": "course-materials/resources/plotting.html#raster-data",
    "href": "course-materials/resources/plotting.html#raster-data",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "terra specializes in raster data processing, which are n-dimensional arrays.\nExamples of file formats:\n\nGeoTIFF (Tag Image File Format, .tif)\nnetCDF (Network Common Data Form, .nc)\nPNG or JPEG images (.png, .jpg)\n\n\n\nImport a raster of sea ice for the Northern Hemisphere and plot it as simply as possible with terra::plot\n\n\nShow the code\nice_filename = \"N_197812_concentration_v3.0.tif\"\nice_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = ice_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\narctic_ice &lt;- terra::rast(ice_fp)\n\nterra::plot(arctic_ice,\n            main = \"Arctic Sea Ice Concentration\")\n\n\n\n\n\n\n\n\n\nThis data has a default palette; each cell is color-coded in shades of blue to white, where dark blue is 0% ice (open ocean) and white is 100% ice. You can view the default palette (“color table”) with terra::coltab\nThe CRS is projected and in units of meters, with each raster cell representing 25 km x 25 km. See CRS metadata here.\nMake a histogram of the raster values using the base R hist to understand the numerical data distribution:\n\n\nShow the code\nhist(arctic_ice,\n     main = \"Arctic Sea Ice Concentration Raster Values\",\n     xlab = \"Values\",\n     ylab = \"Frequency\",\n     col = \"deepskyblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\n\nIn order to properly plot multiple spatial objects on top of one another, they must have the same CRS. Transform the CRS of the habitat polygon into the CRS of the raster for Arctic sea ice, then plot the habitat polygon onto the raster.\n\n\nShow the code\npb_habitat_arctic &lt;- st_transform(pb_habitat_poly, st_crs(arctic_ice))\n\nterra::plot(arctic_ice,\n            main = \"Sea Ice Concentration and Polar Bear Habitat\")\nterra::plot(pb_habitat_arctic, \n            add = TRUE, \n            border = \"darkgoldenrod1\", \n            col = adjustcolor(\"yellow\", alpha.f = 0.2))\n\n\n\n\n\n\n\n\n\nterra automatically defines the x and y axes ticks based on the spatial metadata of the raster.\n\n\n\nScale the data values between 0-100 and convert the array into a dataframe, because ggplot only accepts tabular data. Assign a new palette using RColorBrewer that is similar to the color table associated with the terra plot above.\n\n\nShow the code\n# reverse color palette to better match the default terra::plot palette values\nblue_palette &lt;- rev(RColorBrewer::brewer.pal(n = 9, name = \"Blues\"))\n\n# scale the data 0-100:\n# there are no NA values in this raster but na.rm = TRUE is good practice\nrange &lt;- range(values(arctic_ice), na.rm = TRUE)\narctic_ice_scaled &lt;- (arctic_ice-range[1]) / (range[2]-range[1]) * 100\n\narctic_ice_df &lt;- terra::as.data.frame(arctic_ice_scaled,\n                               cells = FALSE, # do not create index col\n                               xy = TRUE)  %&gt;% # include lat and long cols\n                 dplyr::rename(ice_concentration = N_197812_concentration_v3.0)\n\nggplot() +\n  geom_raster(data = arctic_ice_df,\n              aes(x = x, y = y,\n                  fill = ice_concentration)) +\n  scale_fill_gradientn(colors = blue_palette) +\n  geom_sf(data = pb_habitat_arctic,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          size = 0.2,\n          alpha = 0.1) +\n  coord_sf(default_crs = st_crs(pb_habitat_arctic)) +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill = \"grey\", color = NA),\n        plot.background = element_rect(fill = \"grey\", color = NA)) +\n  labs(title = \"Sea Ice Concentration\\nand Polar Bear Habitat Range\",\n       subtitle = \"Proj CRS: NSIDC Sea Ice Polar Stereographic North\",\n       fill = \"Sea Ice\\nConcentration\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n\n\n\n\n\nNote that ggplot does not automatically derive the units of meters from the projected CRS from the spatial metadata. Instead, it uses 4326 by default. As a result, we mask the axes ticks. Axes ticks can manually be defined with scale_x_continuous and scale_y_continuous\n\n\n\n\n\nShow the code\n# tmap_mode(\"plot\")\n# \n# #tmap_options(max.categories = 9)\n# \n# tm_shape(arctic_ice_scaled) +\n#   tm_raster(palette = blue_palette,\n#             title = \"Sea Ice\\nConcentration\",\n#             style = \"cont\",\n#             breaks = seq(0, 100, length.out = 11),\n#             midpoint = NA) +\n# tm_shape(pb_habitat_arctic) +\n#   tm_polygons(col = \"yellow\",\n#               border.col = \"darkgoldenrod1\",\n#               lwd = 1,\n#               alpha = 0.1) +\n# tm_graticules(n.x = 5, n.y = 5, \n#               labels.show = TRUE, \n#               labels.size = 0.6,\n#               alpha = 0.3) +\n# tm_layout(title = \"Sea Ice\\nConcentration\\nand Polar Bear\\nHabitat Range\",\n#           main.title.size = 0.8,\n#           title.fontface = \"bold\",\n#           legend.outside = TRUE,\n#           legend.title.size = 1,\n#           legend.outside.position = \"right\",\n#           inner.margins = c(0.1, 0.1, 0.1, 0.1)) +\n# tm_scale_bar(position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "course-materials/resources/plotting.html#data-citations",
    "href": "course-materials/resources/plotting.html#data-citations",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "Dataset\nCitation\n\n\n\n\nGBIF, polar bear observation points\nGBIF.org (01 September 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.79778w\n\n\nIUCN Red List, polar bear range polygon\nIUCN. 2024. The IUCN Red List of Threatened Species. Version 2024-1. https://www.iucnredlist.org. Accessed on Septmeber 5, 2024.\n\n\nNSIDC, sea ice concentration raster\nFetterer, F., Knowles, K., Meier, W. N., Savoie, M. & Windnagel, A. K. (2017). Sea Ice Index. (G02135, Version 3). [Data Set]. Boulder, Colorado USA. National Snow and Ice Data Center. https://doi.org/10.7265/N5K072F8. [describe subset used if applicable]. Date Accessed 09-13-2024.\n\n\n\nNSIDC sea ice concentration metadata"
  },
  {
    "objectID": "course-materials/week0.html#class-materials",
    "href": "course-materials/week0.html#class-materials",
    "title": "Intro to EDS 223 and map making",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nWelcome to EDS 223!\n\n\n Discussion\nData wrangling refresher"
  },
  {
    "objectID": "course-materials/week0.html#assignment-reminders",
    "href": "course-materials/week0.html#assignment-reminders",
    "title": "Intro to EDS 223 and map making",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nSR\nPre-course Self Reflection (SR#1)\n09/30/2025\n10/04/2025"
  },
  {
    "objectID": "course-materials/week0.html#additional-resources",
    "href": "course-materials/week0.html#additional-resources",
    "title": "Intro to EDS 223 and map making",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nMEDS Installation Guide"
  },
  {
    "objectID": "course-materials/week2.html#class-materials",
    "href": "course-materials/week2.html#class-materials",
    "title": "Intro to spatial data models",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nIntro to spatial/vector data models\n\n\n Lab materials to download\nSpatial Operations with Vector Data\n\n\n Lab solution\nSpatial Operations with Vector Data\n\n\n Discussion\nWorking with multiple vector types"
  },
  {
    "objectID": "course-materials/week2.html#assignment-reminders",
    "href": "course-materials/week2.html#assignment-reminders",
    "title": "Intro to spatial data models",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 2)\n10/09/2025\n10/09/2025\n\n\nHW\nHomework Assignment #2\n10/07/2025\n10/18/2025"
  },
  {
    "objectID": "course-materials/week2.html#background-reading",
    "href": "course-materials/week2.html#background-reading",
    "title": "Intro to spatial data models",
    "section": " Background Reading",
    "text": "Background Reading\n\nGIS Fundamentals, Chapter 2 Part 2\nGeocomputation with R, Chapter 2\nGeocomputation with R, Chapter 3\nGeocomputation with R, Chapter 7"
  },
  {
    "objectID": "course-materials/week2.html#additional-resources",
    "href": "course-materials/week2.html#additional-resources",
    "title": "Intro to spatial data models",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nsf overview\nsf cheatsheet\nIntroduction to sf and stars"
  },
  {
    "objectID": "course-materials/week7.html#class-materials",
    "href": "course-materials/week7.html#class-materials",
    "title": "Land Cover Classification",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nImage classifiction: unsupervised and supervised approaches\n\n\nNo Lab this week!\n\n\n\n Discussion\nPractice function making with rasters"
  },
  {
    "objectID": "course-materials/week7.html#assignment-reminders",
    "href": "course-materials/week7.html#assignment-reminders",
    "title": "Land Cover Classification",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 7)\n11/13/2025\n11/13/2025\n\n\nHW\nHomework Assignment #4\n11/13/2025\n11/26/2025\n\n\nPR\nPortfolio Repository\n11/13/2025\n12/06/2025"
  },
  {
    "objectID": "course-materials/week4.html#class-materials",
    "href": "course-materials/week4.html#class-materials",
    "title": "Raster spatial and geometry operations",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nIntro to raster data\n\n\n Lab materials to download\nBasics of raster operations with terra\n\n\n Lab solution\nBasics of raster operations with terra\n\n\n Discussion\nPractice raster manipulations"
  },
  {
    "objectID": "course-materials/week4.html#assignment-reminders",
    "href": "course-materials/week4.html#assignment-reminders",
    "title": "Raster spatial and geometry operations",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 4)\n10/23/2025\n10/23/2025\n\n\nHW\nHomework Assignment #3\n10/21/2025\n11/08/2025"
  },
  {
    "objectID": "course-materials/week4.html#background-reading",
    "href": "course-materials/week4.html#background-reading",
    "title": "Raster spatial and geometry operations",
    "section": " Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 3\nGeocomputation with R, Chapter 4\nGeocomputation with R, Chapter 5"
  },
  {
    "objectID": "course-materials/week10.html#class-materials",
    "href": "course-materials/week10.html#class-materials",
    "title": "Intro to active remote sensing",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nFundamentals of active remote sensing: LiDAR and RADAR\n\n\n Lab\nValidating LiDAR tree height estimates\n\n\n Lab solution\n\n\n\n Discussion\nPractice gridding and spatial interpolation"
  },
  {
    "objectID": "course-materials/week10.html#assignment-reminders",
    "href": "course-materials/week10.html#assignment-reminders",
    "title": "Intro to active remote sensing",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 10)\n12/04/2025\n12/04/2025\n\n\nSR\nEnd-of-course reflection (SR#3)\n12/02/2025\n12/06/2023\n\n\nPR\nPortfolio Repository\n11/13/2025\n12/06/2025"
  },
  {
    "objectID": "course-materials/week10.html#background-reading",
    "href": "course-materials/week10.html#background-reading",
    "title": "Intro to active remote sensing",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 9\nIntroduction to Interpreting Digital RADAR Images\nIntroduction to Light Detection and Ranging (Lidar) Remote Sensing Data (Earth Lab, CU Boulder)\nWhat is Synthetic Aperture Radar? (NASA)\nGet To Know SAR: Polarimetry (NASA)"
  },
  {
    "objectID": "course-materials/week10.html#additional-resources",
    "href": "course-materials/week10.html#additional-resources",
    "title": "Intro to active remote sensing",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nExplore Lidar Points in Plas.io (Earth Lab, CU Boulder)"
  },
  {
    "objectID": "assignments/HW2.html#instructions-1",
    "href": "assignments/HW2.html#instructions-1",
    "title": "Homework Assignment 2",
    "section": "Instructions",
    "text": "Instructions\n\nClone repository from GitHub Classrooms\nDownload data from here\nUnzip data and place in repository\nAdd the data to your gitignore!\nCreate a Quarto document with responses\nRender your quarto document as a pdf and submit on Gradescope\n\nMake sure code-fold is not set to True! Your pdf should contain all your code and outputs!\n\n\nYour repository should have the following structure:\n\nEDS223-HW2\n│   README.md\n│   HW2.qmd\n│   Rmd/Proj files    \n│\n└───.gitignore\n     └───data\n         └───ejscreen\n         └───gbif-birds-LA\n         └───mapping-inequality"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#create-a-single-object-map-with-tmap",
    "href": "course-materials/discussions/week1-discussion.html#create-a-single-object-map-with-tmap",
    "title": "Week 1: Discussion Section",
    "section": "2. Create a single-object map with tmap",
    "text": "2. Create a single-object map with tmap\n\nCreate a map of the outline Rapa Nui using tm_shape() + tm_borders()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Add Easter Island vector in default gray\ntm_shape(ei_borders) +\n  tm_borders() +\n# Set general map layout options\n  tm_layout(main.title = \"Easter Island\")"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#create-a-multiple-object-map-with-tmap",
    "href": "course-materials/discussions/week1-discussion.html#create-a-multiple-object-map-with-tmap",
    "title": "Week 1: Discussion Section",
    "section": "3. Create a multiple-object map with tmap",
    "text": "3. Create a multiple-object map with tmap\nExpand on your single-object map to create a map of Rapa Nui and…\n\n…denote the island’s borders and continuous elevation\n…denote the island’s volcanoes and roads\n…play with the color palette and essential map elements\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nei_map &lt;- tm_shape(ei_elev) +\n  tm_graticules() +\n  tm_raster(col.scale = tm_scale_continuous(values = \"-rd_yl_gn\"),\n            col.legend = tm_legend(\"Elevation (m asl)\")) +\n  tm_shape(ei_borders) + \n    tm_borders() +\n  tm_shape(ei_roads) +\n    tm_lines() +\n  tm_shape(ei_points) +\n    tm_symbols(shape = 24, \n               size = \"elevation\", \n               size.legend = tm_legend(\"Volcanoes (m asl)\")) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_scalebar() +\n  tm_title(\"Easter Island\")\n\nei_map"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#create-an-interactive-map-with-tmap",
    "href": "course-materials/discussions/week1-discussion.html#create-an-interactive-map-with-tmap",
    "title": "Week 1: Discussion Section",
    "section": "4. Create an interactive map with tmap",
    "text": "4. Create an interactive map with tmap\n\nSwitch from the default \"plot\" mode to \"view\"\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntmap_mode(\"view\") # Convert default static map to interactive map \n\nℹ tmap mode set to \"view\".\n\nei_map"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#create-a-multiple-object-map-with-ggplot2",
    "href": "course-materials/discussions/week1-discussion.html#create-a-multiple-object-map-with-ggplot2",
    "title": "Week 1: Discussion Section",
    "section": "5. Create a multiple-object map with ggplot2",
    "text": "5. Create a multiple-object map with ggplot2\n\n\n\n\n\n\nMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!\n\n\nAs a bonus challenge, reproduce your multiple-object map from earlier using ggplot2!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# extract lat & long from geom column \nvolcanoes_point &lt;- ei_points %&gt;%\n  mutate(lon = unlist(map(ei_points$geom,1)), # longitude = first component (x)\n         lat = unlist(map(ei_points$geom,2))) # latitude = second component (y)\n\n\nggplot() +\n  # first add Easter Isand borders\n  geom_sf(data = ei_borders, color = \"#212529\") +\n  # add elevation raster\n  geom_stars(data = ei_elev) +\n  scale_fill_distiller(name = \"Elevation (m asl)\", # legend title with units\n                       palette = \"RdYlGn\",         # red-yellow-green palette (no need to reverse)\n                       na.value = \"lightblue\") +  # set NA color (background ocean)\n  # add road vector\n  geom_sf(data = ei_roads, color = \"#343a40\") +\n  # add volcano vector\n  geom_point(data = volcanoes_point, \n             aes(x = lon, y = lat, \n                 size = elevation), # point size depends on attribute value\n             shape = 17,            # triangle\n             color = \"#22577a\") +\n  scale_size_continuous(name = \"Volcanoes (m asl)\") +       # legend title with units\n  ggspatial::annotation_north_arrow(location = \"br\",        # bottom right\n                                    which_north = \"true\") + # point to north pole\n  ggspatial::annotation_scale(location = \"bl\",              # bottom left\n                              width_hint = 0.5) +           # proportion of map area the scalebar should occupy\n  labs(title = \"Easter Island\") +\n  theme_minimal()"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#view-class-and-geometry-type",
    "href": "course-materials/discussions/week2-discussion.html#view-class-and-geometry-type",
    "title": "Week 2: Discussion Section",
    "section": "2. View class and geometry type",
    "text": "2. View class and geometry type\n\nCheck the class() of all vector objects (including the spatially-enabled bird observation dataset)\nUse st_geometry_type() to peak at the geometry type\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# View class of each object\nclass(col)\nclass(roads)\nclass(aves)\n\n# View geometry type of each object\nunique(st_geometry_type(col))\nunique(st_geometry_type(roads))\nunique(st_geometry_type(aves))"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#select-a-macro-ecoregion-of-interest",
    "href": "course-materials/discussions/week2-discussion.html#select-a-macro-ecoregion-of-interest",
    "title": "Week 2: Discussion Section",
    "section": "3. Select a macro ecoregion of interest",
    "text": "3. Select a macro ecoregion of interest\n\nUse filter() to select a macro region of interest from N1_MacroBi in Colombia’s ecoregions dataset\nPlot the subset using tmap\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncol_andean &lt;- col |&gt;\n  # Set categorical \"levels\" in attribute N1_MacroBi (subregions of Colombia)\n  mutate(N1_MacroBi = as.factor(N1_MacroBi)) |&gt;\n  # Subset to Andean region of Colombia\n  filter(N1_MacroBi == \"Andean\")\n\n\ntm_shape(col_andean) +\n  tm_polygons() +\n  tm_title(\"Andean Region of Colombia\")\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling."
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#play-with-coordinate-reference-system-crs",
    "href": "course-materials/discussions/week2-discussion.html#play-with-coordinate-reference-system-crs",
    "title": "Week 2: Discussion Section",
    "section": "4. Play with coordinate reference system (CRS)",
    "text": "4. Play with coordinate reference system (CRS)\nFirst, let’s use st_crs() to check the CRS and it’s units.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# View CRS of each object\nst_crs(col)\nst_crs(roads)\nst_crs(aves)\n\n# View units of each object CRS \nst_crs(col)$units\nst_crs(roads)$units\nst_crs(aves)$units\n\n\n\n\nSay, you want to remove the CRS of an object, in order to “disable” its spatial features. Setting the CRS to NA with st_crs() &lt;- NA is a brute force way to remove a CRS and “corrupt” a spatial object. Let’s do it the right way!\n\nExtract the longitude and latitude from the geometry column and use st_drop_geometry()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are several ways to extract the longitude and latitude from the geometry column.\n\nUse map() to unlist/extract\n\naves_df_purrr &lt;- aves |&gt;\n  # Extract lat & long from geometry column\n  mutate(lon = unlist(purrr::map(aves$geometry, 1)), # longitude = first component (x)\n         lat = unlist(purrr::map(aves$geometry, 2))) |&gt; # latitude = second component (y)\n  st_drop_geometry() # Remove geometry column now that it's redundant\n\n\n\nUse st_coordinates() to extract/assign\n\naves_df_st_coords &lt;- aves %&gt;%\n  dplyr::mutate(lon = st_coordinates(.)[,1], # Assign first matrix item to \"lon\"\n                lat = st_coordinates(.)[,2])  %&gt;% # Assign second matrix item to \"lat\"\n  st_drop_geometry() # Remove geometry column now that it's redundant\n\n\n\n\n\n\nConvert long and lat into a geometry again with st_as_sf() to obtain a proper sf data frame\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConvert to an sf object with st_as_sf()\n\naves_df_purrr &lt;- aves |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#make-a-map",
    "href": "course-materials/discussions/week2-discussion.html#make-a-map",
    "title": "Week 2: Discussion Section",
    "section": "5. Make a map",
    "text": "5. Make a map\n\nCheck that the CRS of the ecoregions and roads datasets match\nTransform CRS of the bird observations dataset using st_transform() to match with the other datasets\nUse tmap to plot the ecoregions, roads, and bird observations together\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Boolen check if CRS match between 2 datasets\nst_crs(col) == st_crs(roads)\n\n\n# Transform bird observation dataset into same CRS as other Colombia dataset\naves &lt;- st_transform(aves, crs = st_crs(col))\n\n\ntm_shape(col) +\n  tm_polygons() +\n  tm_shape(roads) +\n  tm_lines() +\n  tm_shape(aves) +\n  tm_dots() +\n  tm_title(\"Colombia ecoregions, roads,\\nand bird observations\")"
  },
  {
    "objectID": "course-materials/labs/notes/week2_notes.html",
    "href": "course-materials/labs/notes/week2_notes.html",
    "title": "Week 2 Lab",
    "section": "",
    "text": "NoteSource Materials\n\n\n\nThe following materials are modified from Chapter 3 of Geocomputation with R and the tmap book.\nIn this lab, we’ll explore the basics of manipulating vector data in R using the sf package."
  },
  {
    "objectID": "course-materials/labs/notes/week2_notes.html#set-up",
    "href": "course-materials/labs/notes/week2_notes.html#set-up",
    "title": "Week 2 Lab",
    "section": "1. Set up",
    "text": "1. Set up\nInstall a new package to take advantage of some preloaded data.\n\ninstall.packages(\"spData\")\n\nLet’s load all necessary packages:\n\nrm(list = ls())\nlibrary(sf) # for handling vector data\nlibrary(tmap) # for making maps\nlibrary(tidyverse) # because we love the tidyverse\nlibrary(spData) # preloaded spatial data"
  },
  {
    "objectID": "course-materials/labs/notes/week2_notes.html#simple-features-in-sf",
    "href": "course-materials/labs/notes/week2_notes.html#simple-features-in-sf",
    "title": "Week 2 Lab",
    "section": "2. Simple features in sf",
    "text": "2. Simple features in sf\nSimple features is a hierarchical data model that represents a wide range of geometry types. The sf package can represent all common vector geometry types:\n\npoints\nlines\npolygons\nand their respective ‘multi’ versions\n\nsfprovides the same functionality that the sp, rgdal, and rgeos packages provided, but is more intuitive because it builds on the tidy data model and works well with the tidyverse. sf represents spatial objects as “simple feature” objects by storing them as a data frame with the geographic data stored in a special column (usually named geom or geometry).\n\nSimple features from scratch\nLet’s start by looking at how we can construct a sf object. Typically we will load sf objects by reading in data. However, it can be helpful to see how sf objects are created from scratch.\nFirst, we create a geometry for London by supplying a point and coordinate reference system.\n\n# create st_point with longitude and latitude for London\n# simple feature geometry\nlondon_point &lt;- st_point(c(0.1, 51.5))\n\n# add coordinate reference system\n# simple feature collection\nlondon_geom &lt;- st_sfc(london_point, crs = 4326) #st_sfc creates the sf geometry list column\n\nLondon = .1 degree east (near prime meridian) and 51.5 degrees north\nThen, we supply some non-geographic attributes by creating a data frame with attributes about London.\n\n# create data frame of attributes about London\nlondon_attrib &lt;- data.frame(\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nAnd we attach the simple feature collection and data frame to create a sf object. Check out the class of the new object we created.\n\n# combine geometry and data frame\n# simple feature object\nlondon_sf &lt;- st_sf(london_attrib, geometry = london_geom)\n\n# check class\nclass(london_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nNote: will always return “sf” and “data.frame”\nWe can also check out what the CRS looks like:\n\nst_crs(london_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nst_crs(london_sf)$IsGeographic\n\n[1] TRUE\n\nst_crs(london_sf)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n\n\nExisting sf object\nNow let’s look at an existing sf object representing countries of the world:\n\nworld &lt;- spData::world\nclass(world)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(world)\n\n[1] 177  11\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nNote: we see tbl and tbl_df now because it is also a tibble. tibble is a modernized version of dataframes that are use in the tidyverse.\nWe can see that this object contains both spatial data (geom column) and attributes about those geometries. We can perform operations on the attribute data, just like we would with a normal data frame.\n\nsummary(world$lifeExp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  50.62   64.96   72.87   70.85   76.78   83.59      10 \n\n\nThe geometry column is “sticky”, meaning it will stick around unless we explicitly get rid of it. For example, dplyr’s select() function won’t get rid of it.\n\nworld_df &lt;- world %&gt;%\n  select(-geom) #doesn't actually remove the geom column\n\ncolnames(world_df) # geom still shows up as a column\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nTo drop the geom column and convert this sf object into a data frame, we need to drop the geometry column using the st_drop_geometry().\n\nworld_df &lt;- st_drop_geometry(world)\nclass(world_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_df)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\"\n\nncol(world)\n\n[1] 11\n\nncol(world_df)\n\n[1] 10\n\n\n\n\n\n\n\n\nTipsf syntax\n\n\n\nNote that all functions in the sf package start with the prefix st_ NOT sf_. Why? st_ stands for “spatiotemporal” as in data that varies in space and time."
  },
  {
    "objectID": "course-materials/labs/notes/week2_notes.html#coordinate-reference-systems-and-projections",
    "href": "course-materials/labs/notes/week2_notes.html#coordinate-reference-systems-and-projections",
    "title": "Week 2 Lab",
    "section": "3. Coordinate reference systems and projections",
    "text": "3. Coordinate reference systems and projections\nR handles coordinate reference systems using multiple formats:\n\nan identifying string specifying the authority and code such as EPSG:4325\n\nthese need to be passed as strings\nsf will accept the four digit code as an integer\n\nproj4strings are now outdated, but you might see them around\n\nfor example, +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\n\nReprojecting data\nIn some cases we will be working with data which is represented with different coordinate reference systems (CRS). Whenever we work with multiple spatial data objects, we need to check that the CRSs match.\nLet’s create another sf object for London, but now represented with a project coordinate system.\n\nlondon_proj = data.frame(x = 530000, y = 180000) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:27700\") # st_as_sfc converts foreign geometry object to an sfc object - converts coordinates to point geometries\n\nNote: - st_as_sf() converts a data frame to an sf object and can create point geometries from coordinate columns - st_sf() creates an sf object but requires the geometry column to already exist\nWe can check the CRS of any data using the st_crs() function.\n\nst_crs(london_proj)\n\nCoordinate Reference System:\n  User input: EPSG:27700 \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThis is a lot of information to read, so if we wanted to use this point with our other London point, we need to check to see if they are using the same CRS.\n\nst_crs(london_proj) == st_crs(london_sf)\n\n[1] FALSE\n\n\nTo transform the CRS of a dataset, we use the st_transform() function. In the crs argument, we need to specify the coordinate reference system. We can do this by either supplying a CRS code or specifying the CRS of another dataset using the st_crs() function.\n\nlondon_sf_transform &lt;- st_transform(london_sf, crs = st_crs(london_proj))\n\nNow if we check, the CRS between the two datasets should match\n\nif(st_crs(london_sf_transform) == st_crs(london_proj)){\n  print(\"it's a match!\")\n} else {\n  print(\"still not a match\")\n}\n\n[1] \"it's a match!\"\n\n\n\n\n\n\n\n\nTipBuilding beautiful workflows\n\n\n\nHopefully we’re already thinking about how we could build checking coordinate reference systems into our workflows.\nFor example, we could add code like the following that transforms the CRS of dataset2 to match dataset1 and prints out a warning message.\n\nif(st_crs(dataset1) != st_crs(dataset2)){\n  warning(\"coordinate refrence systems do not match\")\n  dataset2 &lt;- st_transform(dataset1, crs = st_crs(dataset1))\n}\n\n\n\n\n\nChanging map projections\nRemember that whenever we make a map we are trying to display three dimensional data with only two dimensions. To display 3D data in 2D, we use projections. Which projection you use can have big implications for how you display information.\nTo the projection of our data, we could:\n\nreproject the underlying data\nor in tmap we can specify the projection we want the map to use\n\nLet’s compare global maps using two different projections:\n\nEqual Earth is an equal-area pseudocylindrical projection (EPSG 8857)\nMercator is a conformal cylindrical map that preserves angles (EPSG 3395)\n\n\ndisplay_crs &lt;- tm_shape(world, crs = 8857) + # specifying a different CRS to display, not transforming the data itself\n  tm_fill(fill = \"area_km2\")\n\ntm_shape(world, crs = 3395) +\n  tm_fill(fill = \"area_km2\")\n\n\n\n\n\n\n\n\n**Note*: Specifying a temporary crs for display in a plot is essentially the same as if we transformed our crs using st_transform and then plotted that Example:\n\nworld_transformed &lt;- st_transform(world, crs = 8857)\n\ntransformed_crs &lt;- tm_shape(world_transformed) + \n  tm_fill(fill = \"area_km2\")\n\n\ntmap_arrange(display_crs, transformed_crs)"
  },
  {
    "objectID": "course-materials/labs/notes/week2_notes.html#vector-attribute-subsetting",
    "href": "course-materials/labs/notes/week2_notes.html#vector-attribute-subsetting",
    "title": "Week 2 Lab",
    "section": "4. Vector attribute subsetting",
    "text": "4. Vector attribute subsetting\nOften we’ll want to manipulate sf objects in the same ways as we might with tabular data in data frames. The great thing about the simple features data model, is we can largely treat spatial objects the same as data frames.\n\ndplyr functions!\nThis means that we can use all of our favorite dplyr functions on sf objects – yay!\nWe can select columns…\n\nworld %&gt;%\n  select(name_long, pop)\n\nOr remove columns…\n\nworld %&gt;%\n  select(-subregion, -area_km2)\n\nOr select AND rename columns\n\nworld %&gt;%\n  select(name = name_long, population = pop)\n\nOr filter observations based on variables\n\nworld1 &lt;- world %&gt;%\n  filter(area_km2 &lt; 10000)\n\nsummary(world1$area_km2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2417    4412    6207    5986    7614    9225 \n\nworld2 &lt;- world %&gt;%\n  filter(lifeExp &gt;= 80)\n\nnrow(world2)\n\n[1] 24\n\n\n\n\nChaining commands with pipes\nBecause we can use dplyr functions with sf objects, we can chain together commands using the pipe operator.\nLet’s try to find the country in Asia with the highest life expectancy\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;%\n  select(name_long, continent, lifeExp) %&gt;%\n  slice_max(lifeExp) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 1 × 3\n  name_long continent lifeExp\n* &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 Japan     Asia         83.6\n\n\nNote: What would happen if I didn’t drop geometry?\n\n\nVector attribute aggregation\nAggregation is the process of summarizing data with one or more ‘grouping’ variables. For example, using the ‘world’ which provides information on countries of the world, we might want to aggregate to the level of continents. It is important to note that aggregating data attributes is a different process from aggregating geographic data, which we will cover later.\nLet’s try to find the total population within each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE)) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 8 × 2\n  continent               population\n* &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811\n\n\nLet’s also find the total area and number of countries in each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 8 × 4\n  continent               population  area_km2 n_countries\n* &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;\n1 Africa                  1154946633 29946198.          51\n2 Antarctica                       0 12335956.           1\n3 Asia                    4311408059 31252459.          47\n4 Europe                   669036256 23065219.          39\n5 North America            565028684 24484309.          18\n6 Oceania                   37757833  8504489.           7\n7 Seven seas (open ocean)          0    11603.           1\n8 South America            412060811 17762592.          13\n\n\nBuilding on this, let’s find the population density of each continent, find the continents with highest density and arrange by the number of countries. We’ll drop the geometry column to speed things up.\n\nworld %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  mutate(density = round(population/area_km2)) %&gt;%\n  slice_max(density, n = 3) %&gt;%\n  arrange(desc(n_countries))\n\n# A tibble: 3 × 5\n  continent population  area_km2 n_countries density\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;   &lt;dbl&gt;\n1 Africa    1154946633 29946198.          51      39\n2 Asia      4311408059 31252459.          47     138\n3 Europe     669036256 23065219.          39      29\n\n\nNote: Before running, what do we think the output of this is going to be?\nanswer: continent, population, area, n_countries, and density for top 3 countries in desc order by n_countries"
  },
  {
    "objectID": "course-materials/labs/notes/week2_notes.html#joins-with-vector-attributes",
    "href": "course-materials/labs/notes/week2_notes.html#joins-with-vector-attributes",
    "title": "Week 2 Lab",
    "section": "5. Joins with vector attributes",
    "text": "5. Joins with vector attributes\nA critical part of many data science workflows is combining data sets based on common attributes. In R, we do this using multiple join functions, which follow SQL conventions.\nLet’s start by looking a data set on national coffee production from the spData package:\n\ncoffee_data &lt;- spData::coffee_data\nhead(coffee_data)\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\nIt appears that coffee_data contains information on the amount of coffee produced in 2016 and 2017 from a subset of countries.\n\nnrow(coffee_data)\n\n[1] 47\n\nnrow(world)\n\n[1] 177\n\n\nThe coffee production dataset does not include any spatial information, so If we wanted to make a map of coffee production, we would need to combine coffee_data with the world dataset. We do this by joining based on countries’ names.\n\nworld_coffee &lt;- left_join(world, coffee_data, by = \"name_long\")\n\nnames(world_coffee)\n\n [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n [4] \"region_un\"              \"subregion\"              \"type\"                  \n [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n[10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n[13] \"coffee_production_2017\"\n\n\nNote: if i am doing a left join, and world has 177 obs, and coffee_data has 47 obs, how many rows are going to be in our outputted dataframe? answer = 177 bc world is on the left side so we are inputting NAs anywhere there isnt a match\nAnd plot what this looks like…\n\ntm_shape(world_coffee) +\n  tm_fill(\n    \"coffee_production_2017\",\n    fill.legend = tm_legend(title = \"Coffee production (2017)\")\n  )\n\n[tip] Consider a suitable map projection, e.g. by adding `+ tm_crs(\"auto\")`.\nThis message is displayed once per session.\n\n\n\n\n\n\n\n\n\nNote: st_crs(world_coffee) = 4326 which is a geopgraphic cooridnate system. Tmap directly plots the lat and long directly on a 2D plane - called equirectangular projection. Basically its an unfolded cylinder and has horizontal stretching near the poles."
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#find-bird-observations-within-santa-barbaras-pas",
    "href": "course-materials/discussions/week3-discussion.html#find-bird-observations-within-santa-barbaras-pas",
    "title": "Week 3: Discussion Section",
    "section": "Find Bird Observations within Santa Barbara’s PAs",
    "text": "Find Bird Observations within Santa Barbara’s PAs\nYou will be working with the following datasets:\nNow, to meet this week’s learning objectives, your task:\n\nFind how many bird observations are within protected areas in Santa Barbara County\n\n\nShow the different outputs from a spatial subset and a spatial join\nBonus Challenge: Try it out with a 5 km buffer around the protected areas too!\n\n\nFind the protected areas within 15 km of a city in Santa Barbara County\n\n\nHint: Use dplyr::filter() to select a city from sb_city_boundaries\nExplore the different outputs with st_intersects(), st_intersection(), and st_within()\nPractice a distance-based join with st_is_within_distance()\n\n\nFind the distance between your city of choice and a protected area of your choice\n\n\nNote: st_distance() finds the distance between the geometries’ edges"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#find-bird-observations-within-pas-in-santa-barbara",
    "href": "course-materials/discussions/week3-discussion.html#find-bird-observations-within-pas-in-santa-barbara",
    "title": "Week 3: Discussion Section",
    "section": "2. Find Bird Observations within PAs in Santa Barbara",
    "text": "2. Find Bird Observations within PAs in Santa Barbara\n\nFind how many bird observations are within protected areas in Santa Barbara County and show how your output differs with a spatial subset versus a spatial join\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\naves_PA_subset &lt;- sb_protected_areas[aves, ] # Subset\n\nnrow(aves_PA_subset) # Check number of rows\n\n[1] 35\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_subset) +\n    tm_dots(col = \"#023047\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\naves_PA_join &lt;- st_join(aves, sb_protected_areas) # default: `st_intersects` \n\nnrow(aves_PA_join) # Check number of rows\n\n[1] 500\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_join) +\n    tm_dots(col = \"#023047\")\n\n\n\n\n\n\n\n\n\n\n\n\nMake a 5 km buffer around the protected areas and subset buffered geometries to only those with bird observations\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nst_crs(sb_protected_areas)$units # Check if units are in meters\n\n[1] \"m\"\n\nPA_buffer_5km &lt;- st_buffer(sb_protected_areas, dist = 5000) # Create 5000 m buffer around PAs\n\n\naves_buffer_subset &lt;- PA_buffer_5km[aves, ] # Subset\n\nnrow(aves_buffer_subset) # Check number of rows\n\n[1] 327\n\n\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_buffer_subset) +\n  tm_dots(col = \"#023047\")"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#find-pas-within-15km-of-goleta",
    "href": "course-materials/discussions/week3-discussion.html#find-pas-within-15km-of-goleta",
    "title": "Week 3: Discussion Section",
    "section": "3. Find PAs within 15km of Goleta",
    "text": "3. Find PAs within 15km of Goleta\n\nFind the protected areas within 15 km of a city in Santa Barbara County\n\nHint: Use dplyr::filter() to select a city from sb_city_boundaries\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Subset SB county to Goleta\ngoleta &lt;- sb_city_boundaries |&gt; \n  dplyr::filter(NAME == \"Goleta\")\n\nst_crs(goleta)$units  # Check if units are in meters\n\n[1] \"m\"\n\ngoleta_buffer_15km &lt;- st_buffer(goleta, dist = 15000) # Create 15 km buffer around Goleta\n\n\n\n\n\nExplore different outputs for #3 with st_intersects(), st_intersection(), and st_within()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngoleta_PAs_within &lt;- st_within(sb_protected_areas, goleta_buffer_15km)\n\n\ngoleta_PAs_intersect &lt;- st_intersects(sb_protected_areas, goleta_buffer_15km)\n\n\ngoleta_PAs_intersection &lt;- st_intersection(sb_protected_areas, goleta_buffer_15km)\n\n\n\n\n\nPractice a distance-based join with st_is_within_distance()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngoleta_PAs_join &lt;- st_join(sb_protected_areas, goleta, st_is_within_distance, dist = 15000)"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#find-distance-between-goleta-and-dangermond-preserve",
    "href": "course-materials/discussions/week3-discussion.html#find-distance-between-goleta-and-dangermond-preserve",
    "title": "Week 3: Discussion Section",
    "section": "3. Find distance between Goleta and Dangermond Preserve",
    "text": "3. Find distance between Goleta and Dangermond Preserve\n\nFind the distance between your city of choice and a protected area of your choice, using the geometries’ edges and the centroid\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Subset PA to Dangermond Preserve\ndangermond &lt;- sb_protected_areas |&gt;\n  dplyr::filter(UNIT_NAME == \"Jack and Laura Dangermond Preserve\")\n\n\ndanger_dist &lt;- st_distance(goleta, dangermond) # Compute the distance between geometries edges\n\n\n# Calculate the geometric center\ndangermond_centroid &lt;- st_centroid(dangermond)\ngoleta_centroid &lt;- st_centroid(goleta)\n\ndanger_dist_centroid &lt;- st_distance(goleta_centroid, dangermond_centroid) # Compute the distance between geometries edges\n\n\n# Check if the distance matrices are equal\ndanger_dist == danger_dist_centroid\n\n      [,1]\n[1,] FALSE"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#find-bird-observations-within-pas-in-santa-barbara-county",
    "href": "course-materials/discussions/week3-discussion.html#find-bird-observations-within-pas-in-santa-barbara-county",
    "title": "Week 3: Discussion Section",
    "section": "2. Find bird observations within PAs in Santa Barbara County",
    "text": "2. Find bird observations within PAs in Santa Barbara County\n\nFind how many bird observations are within protected areas in Santa Barbara County and show how your output differs with a spatial subset versus a spatial join\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\naves_PA_subset &lt;- sb_protected_areas[aves, ] # Subset\n\nnrow(aves_PA_subset) # Check number of rows\n\n[1] 35\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_subset) +\n    tm_dots(col = \"#023047\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\naves_PA_join &lt;- st_join(aves, sb_protected_areas) # default: `st_intersects` \n\nnrow(aves_PA_join) # Check number of rows\n\n[1] 500\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_join) +\n    tm_dots(col = \"#023047\")\n\n\n\n\n\n\n\n\n\n\n\n\nMake a 5 km buffer around the protected areas and subset buffered geometries to only those with bird observations\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nst_crs(sb_protected_areas)$units # Check if units are in meters\n\n[1] \"m\"\n\nPA_buffer_5km &lt;- st_buffer(sb_protected_areas, dist = 5000) # Create 5000 m buffer around PAs\n\n\naves_buffer_subset &lt;- PA_buffer_5km[aves, ] # Subset\n\nnrow(aves_buffer_subset) # Check number of rows\n\n[1] 327\n\n\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_buffer_subset) +\n  tm_dots(col = \"#023047\")"
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html",
    "href": "course-materials/labs/notes/week3_notes.html",
    "title": "Week 3: Lab",
    "section": "",
    "text": "NoteSource Materials\n\n\n\nThe following materials are modified from Chapter 4 and Chapter 5 of Geocomputation with R by Robin Lovelace.\nIn this lab, we’ll explore the basics of spatial and geometry operations on vector data in R using the sf package. We’ll be working with data representing the heigh points of New Zealand."
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html#set-up",
    "href": "course-materials/labs/notes/week3_notes.html#set-up",
    "title": "Week 3: Lab",
    "section": "1. Set Up",
    "text": "1. Set Up\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(spData)"
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html#spatial-subsetting-filtering",
    "href": "course-materials/labs/notes/week3_notes.html#spatial-subsetting-filtering",
    "title": "Week 3: Lab",
    "section": "2. Spatial subsetting (filtering)",
    "text": "2. Spatial subsetting (filtering)\nWhen working with tabular data, we have frequently found it useful to subset the data.frame we are working with based on some condition using dplyr::filter().\nFor example, last week we saw how we could filter to countries whose average life expectancy is greater than 80 years old using the following code:\n\nworld %&gt;%\n  filter(lifeExp &gt;= 80)\n\nSimilarly, we might want to filter data based on its spatial relationships. In this case, we use spatial subsetting which is the process of converting a spatial object into a new object containing only the spatial features that relate in space to another object. This is analogous the attribute subsetting that we covered last week (example above).\n\nTopological relationships\nWhen filtering based on attributes, we use conditions (for example, lifeExp &gt;= 80). In spatial subsetting, we use the relationships of objects to each other in space (topological relationships). These relationships are based on mathematical relationships, but can be more easily understood from visualizing them. The figure below shows how each relationship is satisfied.\n\nGeocomputation with R\n\n\n\n\n\n\n\nTipst_intersects() and st_disjoint()\n\n\n\nNote that st_intersects()is a “catch-all” that contains the following relationships:\n\nst_touches()\nst_overlaps()\nst_contains() and st_contains_properly()\nst_covers() and st_covered_by()\nst_within()\n\nst_disjoint() is the opposite of st_intersects()\n\n\n\n\nExamples\nThere are many ways to spatially subset in R, so we will explore a few.\nAs an example we’ll work with the following two datasets from the spData package:\n\nnz: polygons representing the 16 regions of New Zealand\nnz_height: top 101 highest points in New Zealand\n\nWe’ll explore by trying to find all the high points in the region of Canterbury (shown in dark grey).\n\n\nCode\ntm_shape(nz) +\n  tm_polygons() +\ntm_shape(canterbury) +\n  tm_polygons(fill = \"darkgrey\") +\ntm_shape(nz_height) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\n\n\nBracket subsetting\nLike attribute subsetting, the command x[y, ] (equivalent to nz_height[canterbury, ]) subsets features of a target x using the contents of a source object y. Instead of y being a vector of class logical or integer, however, for spatial subsetting both x and y must be geographic objects. Specifically, objects used for spatial subsetting in this way must have the class sf or sfc: both nz and nz_height are geographic vector data frames and have the class sf, and the result of the operation returns another sf object representing the features in the target nz_height object that intersect with (in this case high points that are located within) the Canterbury region.\n\n# subset nz_heights to just the features that intersect Canterbury\nc_height1 &lt;- nz_height[canterbury, ]\n\nBy default bracket subsetting will filter to features in x that intersect features in y. However, we can use other topological relationships by changing options.\n\nc_height_disjoint&lt;- nz_height[canterbury, , op = st_disjoint]\n\n# Check that intersect is opposite of disjoint\nnrow(c_height1) + nrow(c_height_disjoint) == nrow(nz_height)\n\n\n\nst_filter()\nThe sf package also includes the function st_filter() which is analogous to dplyr::filter(). Using st_filter() we can perform spatial subsetting in the same format as using dplyr commands. The .predicate = argument allows us to define which topological relationship we would like to filter by (e.g. st_intersects(), st_disjoint()).\nThe results from this method are the identical to the method above.\n\n# subset to the features in Cantebury\nc_height2 &lt;- nz_height %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects) # define the topological relationship\n\n\n\nTopological operators (st_intersects())\nThe previous two methods either by default or explicitly use the argument st_intersects. All topological relationships have their own topological operators which are functions that evaluate whether or not features meet the specified condition (e.g. st_intersects()). These operators can be used for spatial subsetting, but are more complicated to use.\nThe output of st_intersects() and other topological operators is a sparse geometry binary predicate list (yikes!) that’s a list that defines whether or not each feature in x intersects y.\nThis can be converted into logical vector of TRUE and FALSE values which can then be used for filtering.\n\n# sparse binary predicate list\nnz_height_sgbp &lt;- st_intersects(x = nz_height, y = canterbury)\nnz_height_sgbp\n\nSparse geometry binary predicate list of length 101, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 1\n 6: 1\n 7: 1\n 8: 1\n 9: 1\n 10: 1\n\n# convert to logical vector\nnz_height_logical &lt;- lengths(nz_height_sgbp) &gt; 0 \n\n# filter based on logical vector\nc_height3 = nz_height[nz_height_logical, ]\n\nnote: length checks length of each vector, if it is empty it is 0, if it is 1 then it is greater than 0 and returns TRUE.\nlogical indexing in R keeps rows that are TRUE, ignores rows that are false.\nNow let’s plot results from all three methods to confirm they gave the same results.\n\n\nCode\nmap1 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height1) +\n  tm_dots(fill = \"red\") +\n  tm_title(text = \"Bracket subsetting\") \n\nmap2 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height2) +\n  tm_dots(fill = \"red\") +\n  tm_title(text = \"st_filter()\")\n\n\nmap3 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height3) +\n  tm_dots(fill = \"red\") +\n  tm_title(text = \"st_intersects()\")\n\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nNote: We can check directly using st_equals(c_height1, c_height2, sparse = FALSE)\n\n\n\nDistance relationships\nThe topological relationships we have been discussing are all binary (features either intersect or don’t). In some cases, it might be helpful to subset based on a distance to a feature. In these cases we can use the st_is_within_distance() to filter features. By default st_is_within_distance() will return a sparse geometry binary predicate list as in st_intersects() above. Instead, we can return a logical by setting sparse = FALSE.\n\n# find heights within 1000 km of Canterbury\nnz_height_logical &lt;- st_is_within_distance(nz_height, canterbury, # x within y\n                      dist = units::set_units(1000, \"km\"), # set distance\n                      sparse = FALSE) # return logical vector instead\n\nc_height4 &lt;- nz_height[nz_height_logical, ] # filter based on logical\n\nNow, we should see points appear that do not intersect Caterbury, but are within 1000 km.\n\n\nCode\n# additional high points should appear\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(fill = \"darkgrey\") +\n  tm_shape(c_height4) +\n  tm_dots(fill = \"red\")"
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html#spatial-joins",
    "href": "course-materials/labs/notes/week3_notes.html#spatial-joins",
    "title": "Week 3: Lab",
    "section": "3. Spatial joins",
    "text": "3. Spatial joins\nJoins are a common way to link different data sources. Up until now, we have been performing joins using common attributes between data.frames. Last week, we saw that the same joins can be used on sf objects. However, we can also perform joins by using the spatial relationship of datasets.\nFirst, let’s remind ourselves of the different types of joins.\n\nSoftware Carpentry\n\n\nToplogical relationships\nWith spatial data, we can join based on the geometry columns using topological relationships using the st_join() function. By default st_join() will join based on geometries that intersect, but can accommodate other topological relationships by changing the join = argument. By default st_join() performs left joins, but can perform inner joins by setting left = FALSE.\n\n# specify join based on geometries x within y\nst_join(x, y, join = st_within)\n\n# specify inner join\nst_join(x, y, left = FALSE)\n\nLet’s consider the scenario where we would like to know which region each of the highest points is located in. We can left join the nz dataset (polygons of NZ’s regions) onto the nz_height dataset (points of highest points in the county).\n\nnz_height_left_join &lt;- st_join(nz_height, nz) %&gt;% \n  select(id = t50_fid, elevation, region = Name) # renaming t50_fid to id and name to region\n\nhead(nz_height_left_join)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1204143 ymin: 5048309 xmax: 1389460 ymax: 5168749\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n       id elevation     region                geometry\n1 2353944      2723  Southland POINT (1204143 5049971)\n2 2354404      2820      Otago POINT (1234725 5048309)\n3 2354405      2830      Otago POINT (1235915 5048745)\n4 2369113      3033 West Coast POINT (1259702 5076570)\n5 2362630      2749 Canterbury POINT (1378170 5158491)\n6 2362814      2822 Canterbury POINT (1389460 5168749)\n\n\nNote: spatial left join is keeping all rows from nz_height, and is adding attributes from nz where there is a match. for each highest point, which region is it in?\nNow we could use this data to summarize the number of highest point in each region!\n\nnz_height_left_join %&gt;%\n  group_by(region) %&gt;%\n  summarise(n_points = n()) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 7 × 2\n  region            n_points\n* &lt;chr&gt;                &lt;int&gt;\n1 Canterbury              70\n2 Manawatu-Wanganui        2\n3 Marlborough              1\n4 Otago                    2\n5 Southland                1\n6 Waikato                  3\n7 West Coast              22\n\n\n\n\nDistance-based joins\nSimilar to filtering, in some cases we may want to join datasets based on their proximity. Let’s see an example!\nWe’ll use the following two datasets from the spData package:\n\ncycle_hire: points representing cycle hire points across London with information on number of bikes available\ncycle_hire_osm: dataset downloaded from OpenStreetMaps representing cycle hire points across London with information on the capacity of the hire point\n\nIn this example, we would like join the capacity attribute from the cycle_hire_osm dataset to the cycle_hire dataset. Unfortunately it appears that the points from the two datasets do not perfectly align.\n\n#load data to view it \ncycle_hire &lt;-spData::cycle_hire\n\ncycle_hire_osm &lt;- spData::cycle_hire_osm\n\n\n# check whether or not points overlap\nif(any(st_intersects(cycle_hire, cycle_hire_osm, sparse = FALSE)) == TRUE){\n  print(\"points overlap\")\n} else{\n  warning(\"points don't overlap\")\n}\n\nWarning: points don't overlap\n\n\n\n\nCode\ntmap_mode(\"view\")\n\ntm_shape(cycle_hire) +\n  tm_symbols(fill = \"red\" , fill_alpha = 0.2)+\ntm_shape(cycle_hire_osm) +\n  tm_symbols(fill = \"blue\", fill_alpha = 0.2)\n\n\n\n\n\n\nWe can join by again using st_join(), but this time including a distance threshold using st_is_within_distance.\n\ncycle_hire_join &lt;- st_join(cycle_hire, cycle_hire_osm,\n                           st_is_within_distance,\n                           dist = units::set_units(20, \"m\")) %&gt;%\n                   select(id, capacity)\n\nhead(cycle_hire_join)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1975742 ymin: 51.49313 xmax: -0.08460569 ymax: 51.53006\nGeodetic CRS:  WGS 84\n  id capacity                     geometry\n1  1        9  POINT (-0.1099705 51.52916)\n2  2       27  POINT (-0.1975742 51.49961)\n3  3       NA POINT (-0.08460569 51.52128)\n4  4       NA  POINT (-0.1209737 51.53006)\n5  5       NA   POINT (-0.156876 51.49313)\n6  6        8  POINT (-0.1442289 51.51812)\n\n\nNote: For each point in cycle_hire, finding any points in cycle_hire_osm that are within 20 meters and adding the attributes (capacity) to the cycle hire station.\nLet’s build some checks to diagnose the output.\n\nif(nrow(cycle_hire) == nrow(cycle_hire_join)){\n  print(\"join matches original data dimensions\")\n} else {\n  warning(\"join does not match orginal data dimensions\")\n  print(paste(\"cycle_hire has\", nrow(cycle_hire), \"rows\"))\n  print(paste(\"cycle_hire_join has\", nrow(cycle_hire_join), \"rows\"))\n}\n\nWarning: join does not match orginal data dimensions\n\n\n[1] \"cycle_hire has 742 rows\"\n[1] \"cycle_hire_join has 762 rows\"\n\n\nNote that the joined result has more rows than the target data. This is because some of the cycle hire stations in cycle_hire have multiple matches in cycle_hire_osm. Depending on your project, you would need to think about how to resolve this. In this case, we can aggregate the values for the overlapping points by taking the mean.\n\n# aggregate values for single points in cycle_hire\ncycle_hire_join &lt;- cycle_hire_join %&gt;%\n  group_by(id) %&gt;%\n  summarise(capacity = mean(capacity))\n\n# check results\nif(nrow(cycle_hire) == nrow(cycle_hire_join)){\n  print(\"join matches original data dimensions\")\n} else {\n  warning(\"join does not match orginal data dimensions\")\n  print(paste(\"cycle_hire has\", nrow(cycle_hire), \"rows\"))\n  print(paste(\"cycle_hire_join has\", nrow(cycle_hire_join), \"rows\"))\n}\n\n[1] \"join matches original data dimensions\"\n\n\n\n\n4. Spatial aggregation\nAs with aggregating attribute data, spatial data aggregation condenses data: outputs have few rows than inputs. Think about our friend group_by() %&gt;% summarise()! Spatial aggregation is the same.\nLet’s consider the example where we would like to make a map of the mean elevation of high points within each region of NZ. There are several ways to do this, but the first thing we should be thinking is that we will need to retain the geometry column of the NZ regions in order to make a map.\nThe first approach is by leveraging st_join() again. But in this example, we want the nz object to be the target to maintain the geometries we need.\n\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\nnz_elevation_join &lt;- st_join(x = nz, y = nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarise(elevation = mean(elevation, na.rm = TRUE))\n\nThe second approah uses the aggregate() function. Although it doesn’t follow the dplyr piping convention we’re used to, aggregate() will come in handy later, so it’s nice to see how it works.\nThe syntax looks slightly different, in this case the argument x is the data we would like to aggregate (nz_height) and the by argument specifies the geometry that you would like to group by. The FUN argument defines the function that you would like to use to aggregate, in this case mean.\n\nnz_elevation_agg &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\n\n\n\nCode\nmap1 &lt;- tm_shape(nz_elevation_join) +\n  tm_polygons(fill = \"elevation\",\n              fill.legend= tm_legend(title = \"Mean elevation (meters)\")) +\n  tm_title(text = \"group_by()\") \n\n\nmap2 &lt;- tm_shape(nz_elevation_agg) +\n  tm_polygons(fill = \"elevation\",\n              fill.legend = tm_legend(title = \"Mean elevation (meters)\")) +\n  tm_title(text = \"aggregate()\")\n \n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteJoining incongruent layers\n\n\n\nAn important consideration when aggregating spatial objects is that geometries are congruent, meaning aggregating zones align with the units being aggregating. This is often the case with administrative boundaries where units are sub-units of one another (e.g. countries &gt; states &gt; counties).\nHowever, in some cases the aggregating zones do not share common borders with the target. This is an issue because it’s not clear how to aggregate underlying data. Areal interpolation overcomes this issue by transferring values to another using algorithms, including simple area weighted approaches.\n\nincongruent &lt;- spData::incongruent\naggregating_zones &lt;- spData::aggregating_zones\n\ntm_shape(incongruent) +\n  tm_polygons(fill = \"lightblue\",\n              col = \"blue\") +\n  tm_shape(aggregating_zones) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n\n\n\nThe simplest useful method for this is area weighted spatial interpolation, which transfers values from the incongruent object to a new column in aggregating_zones in proportion with the area of overlap: the larger the spatial intersection between input and output features, the larger the corresponding value. This is implemented in st_interpolate_aw(), as demonstrated in the code chunk below.\n\n# select just the value to be aggregated\nincongruent_2 &lt;- incongruent %&gt;%\n  select(value)\n\n# use area-weighted interpolation to aggregrate the \"value\" attribute\naggregating_zones_area_weighted &lt;- st_interpolate_aw(incongruent_2, aggregating_zones, extensive = TRUE)\n\nWarning in st_interpolate_aw.sf(incongruent_2, aggregating_zones, extensive =\nTRUE): st_interpolate_aw assumes attributes are constant or uniform over areas\nof x\n\naggregating_zones_area_weighted$value\n\n[1] 19.61613 25.66872\n\n\n\n\nNote: extensive = true for count/sum data, values are split proportionally by area overlap for ex. if 30% of area a overlaps area b, then 30% of the population moves to area b.\nextensive = false is for rate/ density data. values are averaged based on area overlap."
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html#aggregating",
    "href": "course-materials/labs/notes/week3_notes.html#aggregating",
    "title": "Week 3: Lab",
    "section": "1. Aggregating",
    "text": "1. Aggregating\n\nGeometry unions\nWe may come across situations where we would like to summarize data across several spatial units. For example, summarizing the population across states within regions of the US. Based on our experience with tabular data, we can summarize attributes by using group_by() %&gt;% summarize(). Alternatively we can use the aggregate() function. So far this should look familiar, but if plot the outputs we notice that these functions have also aggregated the underlying geometries.\n\n# load US states\nus_states &lt;- spData::us_states\n\n# summarize total population within each region\nregions1 &lt;- us_states %&gt;%\n  group_by(REGION) %&gt;%\n  summarise(population = sum(total_pop_15, na.rm = TRUE))\n\n# alternative approach\nregions2 &lt;- aggregate(x = us_states[, \"total_pop_15\"], # data and attribute to be aggregated\n                      by = list(us_states$REGION), # attribute to aggregate by\n                      FUN = sum, na.rm = TRUE) # aggregating function\n\n\n\nCode\nmap1 &lt;- tm_shape(us_states) +\n  tm_polygons(fill = \"total_pop_15\",\n              fill.legend  = tm_legend(\"Total population\")) +\n  tm_title(text = \"US States\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))  # update margins, bottom, left, top, right\n\nmap2 &lt;- tm_shape(regions1) +\n  tm_polygons(fill = \"population\",\n              fill.legend  = tm_legend(\"Total population\")) +\n  tm_title(text = \"group_by()\") +\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(regions2) +\n  tm_polygons(fill = \"total_pop_15\",\n              fill.legend  = tm_legend(\"Total population\")) +\n  tm_title(text = \"aggregate()\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nWhat’s going on here? Behind the scenes, R is using st_union() to combine geometries within each group. We can also use st_union() to combine any pair of spatial objects.\n\n# combine geometries of western states\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nus_west_union &lt;- st_union(us_west)\n\n# combine geometries of Texas and western states\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\n\n\n\nCode\nmap1 &lt;- tm_shape(us_west) +\n  tm_polygons() +\n  tm_title(text = \"western states\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(us_west_union) +\n  tm_polygons() +\n  tm_title(text = \"western states union\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(texas) +\n  tm_polygons() +\n  tm_title(text = \"TX\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap4 &lt;- tm_shape(texas_union) +\n  tm_polygons() +\n  tm_title(text = \"TX + western states union\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, map4, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html#filtering",
    "href": "course-materials/labs/notes/week3_notes.html#filtering",
    "title": "Week 3: Lab",
    "section": "2. Filtering",
    "text": "2. Filtering\n\nBuffers\nIn the previous section, we saw that we can filter spatial objects based on their proximity using st_is_within_distance(). An alternative approach to finding items that are within a set distance would be to expand the geometry and then intersect with objects of interest. We can change the size of geometries by creating a “buffer” using st_buffer()\nIn this example, let’s create 5 km and 50 km buffers around the Seine.\n\nseine_buffer_5km &lt;- st_buffer(seine, dist = 5000)\nseine_buffer_50km = st_buffer(seine, dist = 50000)\n\n\n\nCode\nmap1 &lt;- tm_shape(seine_buffer_5km) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_title(text = \"5km buffer\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(seine_buffer_50km) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_title(text = \"50km buffer\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nThe Seine is actually comprised of multiple geometries. To make things simpler, and more look better on maps, we can combine geometries using our new friend st_union!\n\nseine_union &lt;- st_union(seine_buffer_50km)\n\n\n\nCode\ntm_shape(seine_union) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_title(text= \"50km buffer\")\n\n\n\n\n\n\n\n\n\nNow let’s see an example of using a buffer to find objects within a set distance. Here, we’ll repeat our previous example of finding points within 100 km of Canterbury. And check to see if the results match our previous approach!\n\n# create buffer around high points\nnz_height_buffer &lt;- st_buffer(nz_height, dist = 1000000)\n\n# filter buffered points with those that intersect Canterbury\nc_height5 &lt;- nz_height_buffer %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects)\n\n# check to see if results match previous approach\nif(nrow(c_height4) == nrow(c_height5)){\n  print(\"results from buffer approach match st_is_within_distance() approach\")\n} else{\n  warning(\"approaches giving different results\")\n}\n\n[1] \"results from buffer approach match st_is_within_distance() approach\"\n\n\n\n\nClipping\nBeyond filtering observations based on their spatial proximity, in some cases we might want to filter (or remove) portions of geometries. Spatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features.\nClipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents.\nThere are several options for clipping geometries:\n\nst_intersection(x, y) - portion of x intersecting y\nst_difference(x, y) - portion of x not intersecting y\nst_difference(y, x) - portion of y not intersecting x\nst_union(x, y) - portion either in x or y\nst_sym_difference(x, y) - portions of x and y that do not intersect\n\nTo illustrate the concept, we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one.\n\nx &lt;- st_sfc(st_point(c(0, 1))) %&gt;%\n  st_buffer(dist = 1) %&gt;%\n  st_sf() \n\ny &lt;- st_sfc(st_point(c(1, 1))) %&gt;%\n  st_buffer(dist = 1) %&gt;%\n  st_sf()\n\nintersection &lt;- st_intersection(x, y) \ndifference_x_y &lt;- st_difference(x, y) \ndifference_y_x &lt;- st_difference(y, x)\nunion &lt;- st_union(x, y)\nsym_difference &lt;- st_sym_difference(x, y) \n\n\n\nCode\nx &lt;- x %&gt;% st_set_crs(4326)\ny &lt;- y %&gt;% st_set_crs(4326)\nbbox &lt;- st_union(x, y)\n\nmap1 &lt;- tm_shape(x, bbox = bbox) +\n  tm_borders(col = \"red\") +\n  tm_shape(y) +\n  tm_borders(col = \"blue\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- map1 +\n  tm_shape(intersection, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_intersection()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- map1 +\n  tm_shape(difference_x_y, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_difference(x,y)\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap4 &lt;- map1 +\n  tm_shape(difference_y_x, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_difference(y,x)\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap5 &lt;- map1 +\n  tm_shape(union, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_union()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap6 &lt;- map1 +\n  tm_shape(sym_difference, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_title(text = \"st_sym_diffference()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, map4, map5, map6, nrow = 2)\n\n\n\n\n\n\n\n\n\nNow let’s see how we could use these updated geometries for filtering. Extending this simple example, we’ll create 100 random points. We want to find the points that intersect both x and y. We have a few different approaches that all produce the same results.\n\n# create random points\nbb &lt;- st_bbox(bbox) # create bounding box of x and y\nbox &lt;- st_as_sfc(bb)\np &lt;- st_sample(x = box, size = 100) %&gt;% # randomly sample the bounding box\n  st_as_sf()\n\n# find intersection of x and y\nx_and_y &lt;- st_intersection(x, y)\n\n# filter points\n# first approach: bracket subsetting\np_xy1 = p[x_and_y, ]\n\n# second approach: st_filter()\np_xy2 &lt;- p %&gt;%\n  st_filter(., x_and_y)\n\n# third approach: st_intersection()\np_xy3 = st_intersection(p, x_and_y)\n\n\n\nCode\nmap2 &lt;- map1 +\n  tm_shape(p) +\n  tm_dots(alpha = 0.5) +\n  tm_layout(main.title = \"original\")\n\nmap3 &lt;- map2 +\n  tm_shape(p_xy1) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"bracket subsetting\")\n\nmap4 &lt;- map2 +\n  tm_shape(p_xy2) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"st_filter()\")\n\nmap5 &lt;- map2 +\n  tm_shape(p_xy3) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"st_intersection()\")\n\ntmap_arrange(map2, map3, map4, map5, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/notes/week3_notes.html#making-life-easier",
    "href": "course-materials/labs/notes/week3_notes.html#making-life-easier",
    "title": "Week 3: Lab",
    "section": "3. Making life easier!",
    "text": "3. Making life easier!\nWorking with, and especially plotting, complex spatial objects can become quite cumbersome. This section we’ll see a few ways to create and manipulate geometries to make them easier to work with.\n\nCentroids\nCentroids are basically the center of spatial objects. They can be a handy way to display summary statistics (or we might actually use them for analysis – e.g. to find the distance between polygons).\nThere are many ways to that we might want to define the “center” of an object, but the most common is the geographic centroid which is the center of mass of a spatial object. The geographic centroid can be found using st_centroid().\nSometimes the geographic centroid may fall outside of the boundaries of the object (picture the centroid of a doughnut!). While correct, it might be confusing on a map, so we can use st_point_on_surface() to ensure that the centroid is placed onto the object.\nLet’s inspect a few examples!\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\n\n\nCode\nmap1 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(nz_centroid) +\n  tm_symbols(fill = \"red\", fill_alpha = 0.5) +\n  tm_shape(nz_pos) +\n  tm_symbols(fill = \"blue\", fill_alpha = 0.5)+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(seine) +\n  tm_lines() +\n  tm_shape(seine_centroid) +\n  tm_symbols(fill = \"red\", fill_alpha = 0.5) +\n  tm_shape(seine_pos) +\n  tm_symbols(fill = \"blue\", fill_alpha = 0.5)+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nSimplification\nWe may also want to simplify a geometry to make it easier to plot or take up less storage. There are several different algorithms for simplifying geometries, check out Geocomputation with R for more examples. The sf packages uses the Douglas-Peucker algorithm within the st_simplify() function.\nLet’s see an example!\n\nseine_simple &lt;- st_simplify(seine, dTolerance = 2000)  # 2000 m\n\n\n\nCode\nmap1 &lt;- tm_shape(seine) +\n  tm_lines() +\n  tm_title(\"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(seine_simple) +\n  tm_lines() +\n  tm_title(\"st_simplify()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html",
    "href": "course-materials/labs/notes/week4_notes.html",
    "title": "Week 4: Lab",
    "section": "",
    "text": "terra\nIn this lab we’ll be exploring the basics of raster data, including spatial data and geometry operations. Raster data represents continuous surfaces, as opposed to the discrete features represented in the vector data model. We’ll primarily be working with data from Zion National Park in Utah."
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html#set-up",
    "href": "course-materials/labs/notes/week4_notes.html#set-up",
    "title": "Week 4: Lab",
    "section": "1. Set Up",
    "text": "1. Set Up\nFirst, let’s install the {geoData} package which we’ll use later in the lab to get access to example datasets.\n\ninstall.packages(\"geodata\")\n\nNow let’s load all the necessary packages. R has several packages for handling raster data. In this lab, we’ll use the {terra} package.\n\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(kableExtra) # table formatting\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data\nlibrary(geodata) # spatial data"
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html#raster-objects",
    "href": "course-materials/labs/notes/week4_notes.html#raster-objects",
    "title": "Week 4: Lab",
    "section": "2. Raster objects",
    "text": "2. Raster objects\nIn this section we’ll learn how to create raster data objects by reading in data and how to do basic data manipulations.\n\nCreating raster objects\nThe {terra} package represents raster objects using the SpatRaster class. The easiest way to create SpatRaster objects is to read them in using the rast() function. Raster objects can handle both continuous and categorical data.\nWe’ll start with an example of two datasets for Zion National Park from the spDataLarge package:\n\nsrtm.tif: remotely sensed elevation estimates (continuous data)\nnlcd.tif: simplified version of the National Land Cover Database 2011 product (categorical data)\n\n\n# create raster objects\nzion_elevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion_land &lt;- rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\n\n# test class of raster object\nclass(zion_elevation)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nNotes: Look at zion object. - 457 x 456 x 1 = 457 rows, 456 columns, 1 layer - ‘INT2U’ : Data stored as unsigned 2-byte integers - depth = 0 : single 2D layer - extent: geogrpahic bounding box - functions: available functions you can call\n\nmost of this is under the hood stuff we don’t directly interact with.\n\n\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(\"Elevation (m)\"))+   \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n  \n\nmap2 &lt;- tm_shape(zion_land) +\n  tm_raster(col.legend = tm_legend(\"Land cover\"))+\n      tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\nThe SpatRaster class can also handle multiple “layers”. Layers can store different variables for the same region in one object. This is similar to attributes (or columns) in data.frames. Later in the course when we discuss multispectral data, we’ll learn more about why remotely-sensed data will often contain multiple “bands” or layers.\nAs an example, we’ll load a dataset from spDataLarge containing the four bands of the Landsat 8 image for Zion National Park.\n\nlandsat &lt;- rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\n\nnlyr(landsat) # test number of layers in raster object\n\n[1] 4\n\n\n\ntm_shape(landsat) +\n  tm_raster(col.legend = tm_legend(\"Unscaled reflectance\") ,\n            col.free = FALSE)\n\n\n\n\n\n\n\n\nNote: col.free = FALSE forces all 4 bands to use the same color scale with the same value range.\nlandsat_1 (Blue) - Very light, low values overall\nlandsat_2 (Green) - Similar to blue but slightly more contrast - Vegetation reflects moderately in green - Values mostly 10,000-20,000\nlandsat_3 (Red) - More contrast, darker overall - Vegetation absorbs red light for photosynthesis - Notice the darker patterns (likely vegetation) - Values range 10,000-25,000\nlandsat_4 (Near-Infrared/NIR) - Highest values and most contrast!\n\nhighest reflectance band overall (up to 35,000)\nDark areas are likely healthy vegetation reflecting NIR light\nThis is why NIR is so valuable for vegetation mapping\n\nhigher values = more light was reflected back to the satelittle sensor\nWe can subset layers using either the layer number or name:\n\nnames(landsat)\n\n[1] \"landsat_1\" \"landsat_2\" \"landsat_3\" \"landsat_4\"\n\nlandsat3 &lt;- subset(landsat, 3)\nlandsat4 &lt;- subset(landsat, \"landsat_4\")\n\nWe can combine SpatRaster objects into one, using c():\n\nlandsat34 &lt;- c(landsat3, landsat4)\n\n\n\nMerging Rasters\nIn some cases, data for a region will be stored in multiple, contiguous files. To use them as a single raster, we need to merge them.\nIn this example, we download elevation data for Austria and Switzerland and merge the two rasters into one.\n\naustria &lt;- geodata::elevation_30s(country = \"AUT\", path = tempdir())\nswitzerland &lt;- geodata::elevation_30s(country = \"CHE\", path = tempdir())\n\n# Merge Austria and Switzerland rasters\nmerged &lt;- merge(austria, switzerland)\n\n\nmap1 &lt;- tm_shape(austria) +\n  tm_raster(col.legend = tm_legend( \"Elevation (m)\")) +\n  tm_title(text = \"Austria\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n  \nmap2 &lt;- tm_shape(switzerland) +\n  tm_raster(col.legend = tm_legend( \"Elevation (m)\")) +\n  tm_title(text = \"Switzerland\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap3 &lt;- tm_shape(merged) +\n tm_raster(col.legend = tm_legend( \"Elevation (m)\")) +\n  tm_title(text = \"Merged\") +\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n  \ntmap_arrange(map1, map2, map3, nrow = 1)  \n\n\n\n\n\n\n\n\nnote: if we wanted to update breaks: tm_raster(col.scale = tm_scale_continuous(values = c(0, 500, 1000, 1500, 2000, 3000)),             col.legend = tm_legend(\"Elevation (m)\"))\n\n\nInspecting raster objects\nWe can get info on raster values just by typing the name or using the summary function.\n\nsummary(zion_elevation)\n\nWarning: [summary] used a sample\n\n\n      srtm     \n Min.   :1024  \n 1st Qu.:1535  \n Median :1836  \n Mean   :1843  \n 3rd Qu.:2114  \n Max.   :2892  \n\n\nNote: warning message is useful here, it took a sample rather than use all the data. likely because data is massive.\nsrtm is the name of our raster layer\nWe can get global summaries, such as standard deviation.\n\nglobal(zion_elevation, sd)\n\n           sd\nsrtm 416.6776\n\n\nnote: global is summarized values of an entire spatial raster\nOr we can use freq() to get the counts with categories.\n\nfreq(zion_land)\n\n  layer      value  count\n1     1      Water   1209\n2     1  Developed  17517\n3     1     Barren 106070\n4     1     Forest 767537\n5     1  Shrubland 545771\n6     1 Herbaceous   4878\n7     1 Cultivated   8728\n8     1   Wetlands   6497\n\n\nnote: counts = number of pixels in each category\n\n\nIndexing\nWe can index rasters using row-column indexing or cell IDs.\n\n# row 1, column 1\nzion_elevation[1, 1]\n\n  srtm\n1 1728\n\n# cell ID 1\nzion_elevation[1]\n\n  srtm\n1 1728\n\n# cell ID 468\nzion_elevation[468] # exercise : what would the row column indexing be for this value?\n\n  srtm\n1 1717\n\nzion_elevation[2,3]\n\n  srtm\n1 1717\n\n\nnote: Cell IDs move left to right, top to bottom For multi-layer rasters, subsetting returns the values in both layers.\nView Cell ID #1 for our landsat. shows cell ID 1 for each band\n\nlandsat[1]\n\n  landsat_1 landsat_2 landsat_3 landsat_4\n1      9833      9579      9861     14114\n\n\nWe can also modify/overwrite cell values.\n\nzion_elevation[1, 1] &lt;- 0\nzion_elevation[1, 1]\n\n  srtm\n1    0\n\n\nReplacing values in multi-layer rasters requires a matrix with as many columns as layers and rows as replaceable cells.\n\nlandsat[1] &lt;- cbind(c(0), c(0),c(0), c(0))\nlandsat[1]\n\n  landsat_1 landsat_2 landsat_3 landsat_4\n1         0         0         0         0\n\n\nWe can also use a similar approach to replace values that we suspect are incorrect.\n\ntest_raster &lt;- zion_elevation\n\n#check NA values\nglobal(is.na(test_raster), \"sum\")\n\n     sum\nsrtm   0\n\n# Assign elevation of less than 20 to NA\ntest_raster[test_raster &lt; 20] &lt;- NA\n\n#check NA values\nglobal(is.na(test_raster), \"sum\")\n\n     sum\nsrtm   1"
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html#spatial-subsetting",
    "href": "course-materials/labs/notes/week4_notes.html#spatial-subsetting",
    "title": "Week 4: Lab",
    "section": "3. Spatial subsetting",
    "text": "3. Spatial subsetting\nWe can move from subsetting based on specific cell IDs to extract info based on spatial objects.\nTo use coordinates for subsetting, we can “translate” coordinates into a cell ID with the functions terra::cellFromXY() or terra::extract().\n\n# create point within area covered by raster\npoint &lt;- matrix(c(-113, 37.5), ncol = 2) # -113°longitude,  37.5° latitude\n\n# approach 1\n# find cell ID for point\nid &lt;- cellFromXY(zion_elevation, xy = point) #xy needs to be a matrix \n\n# index to cell\nzion_elevation[id]\n\n  srtm\n1 2398\n\n# approach 2\n# extract raster values at point\nterra::extract(zion_elevation, point)\n\n  srtm\n1 2398\n\n\nNote: extract extracts values from a spatRaster for a set of locations. The locations can be points, lines, polygons, a matrix (like we had), or a vector with cell numbers\nWe can also subset raster objects based on the extent another raster object. Here we extract the values of our elevation raster that fall within the extent of a clipping raster that we create.\n\n#can find extent of our current raster\nterra::ext(zion_elevation)\n\nSpatExtent : -113.239583212784, -112.85208321281, 37.1320834298579, 37.5129167631658 (xmin, xmax, ymin, ymax)\n\n# create a raster with a smaller extent\nclip &lt;- rast(xmin = -113.3, xmax = -113, ymin = 37.2, ymax = 37.9,\n            resolution = 0.3, #set spatial resolution, each cell is 0.3 degrees x 0.3 degrees in size\n            vals = 1) # vector with cell values, we want each cell to have value 1\n\n# select values that fall within smaller extent\nzion_elevation_clip &lt;- zion_elevation[clip]\n\n# verify that output has fewer values than original\nif(ncell(zion_elevation) == nrow(zion_elevation_clip)) {\n  warning(\"clipping did not remove cells\")\n} else {\n  print(\"clipping removed cells\")\n}\n\n[1] \"clipping removed cells\"\n\n# View our new clipped raster\n# zion_elevation_clip # returns elevations individually instead of within a raster\n\nIn the previous example, we just got the values of the raster back (and lost the raster format). In some cases, we might want the output to be the raster cells themselves.\nWe can do this use the “[” operator and setting “drop = FALSE”.\n\nzion_elevation_clip &lt;- zion_elevation[clip, drop = FALSE]\n\n\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(zion_elevation_clip) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"clipped\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\n\ntmap_arrange(map1, map2, nrow = 1)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningWoops, is the clipped version really smaller?\n\n\n\nPlotting side-by-side, the clipped version appears to take up more space – does this mean the clipping didn’t work? How can we tell?\n\nVisually: If we look at the features represented, we can see that the clipped version doesn’t represent all the features present in the original version.\nQuantitatively: We can directly check whether the extents match using the ext() function!\n\n\nif(ext(zion_elevation) == ext(zion_elevation_clip)){\n  print(\"extents match\")\n} else{\n  print(\"extents do not match\")\n}\n\n[1] \"extents do not match\"\n\n\n\n\nIn the previous example, we subsetted the extent of the raster (removed cells). Another common use of spatial subsetting is to select cells based on their values. In this case we create a “masking” raster comprised of logicals or NAs that dictates the cells we would like to preserve.\n\n# create raster mask of the same resolution and extent\nrmask &lt;- zion_elevation\n\n# set all cells with elevation less than 2000 meters to NA\nrmask[rmask &lt; 2000] &lt;- NA\n \n# subset elevation raster based on mask\n\n# approach 1: bracket subsetting\nmasked1 &lt;- zion_elevation[rmask, drop = FALSE] \n\n# approach 2: mask() function\nmasked2 &lt;- mask(zion_elevation, rmask)           \n\n\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap2 &lt;- tm_shape(masked1) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"bracket subsetting\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\nmap3 &lt;- tm_shape(masked2) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"mask()\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\ntmap_arrange(map1, map2, map3, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html#map-algebra",
    "href": "course-materials/labs/notes/week4_notes.html#map-algebra",
    "title": "Week 4: Lab",
    "section": "4. Map algebra",
    "text": "4. Map algebra\n“Map algebra” is the set of operations that modify or summarize raster cell values with reference to surrounding cells, zones, or statistical functions that apply to every cell. Map algebra is typically categorized into local, focal, and zonal operations.\n\nLocal operations\nLocal operations are computed on each cell individually. For example, we can use ordinary arithmetic or logical statements.\n\nzion_elevation + zion_elevation # doubles each cells' value\n\n# see how max value differs\n\nzion_elevation^2 # raises each cells' value to the power of 2\n\nlog(zion_elevation) # takes the log of each cells' value\n\nzion_elevation &gt; 5 # determines whether each cell has a value greater than 5\n\nWe can also classify intervals of values into groups. For example, we could classify elevation into low, middle, and high elevation cells.\nFirst, we need to construct a reclassification matrix:\n\nThe first column corresponds to the lower end of the class\nThe second column corresponds to the upper end of the class\nThe third column corresponds to the new value for the specified ranges in columns 1 and 2\n\n\n# create reclassification matrix\nrcl &lt;- matrix(c(1000, 1500, 1, # group 1 ranges from 1000 - 1500 m\n                1500, 2000, 2, # group 2 ranges from 1500 - 2000 m\n                2000, 2500, 3, # group 3 ranges from 2000 - 2500 m\n                2500, 3000, 4), # group 4 ranges from 2500 - 3000 m\n                ncol = 3, byrow = TRUE)\n\n# use reclassification matrix to reclassify elevation raster\nreclassified &lt;- classify(zion_elevation, rcl = rcl)\n\n#summary(reclassified)\n\n# change reclassified values into factors (rather than numeric)\nvalues(reclassified) &lt;- as.factor(values(reclassified))\nsummary(reclassified)\n\nWarning: [summary] used a sample\n\n\n srtm     \n 0:    1  \n 1:23228  \n 2:42837  \n 3:27255  \n 4: 6845  \n\n# question: What is the 0: 1 representing? \n#zion_elevation[1, 1] &lt;- 0\n\n\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(reclassified) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text= \"reclassified\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\nFor more efficient processing, we can use a set of map algebra functions:\n\napp() applies a function to each cell of a raster to summarize the values of multiple layers into one layer\ntapp() is an extension of app() that allows us to apply an operation on a subset of layers\nlapp() allows us to apply a function to each cell using layers as arguments\n\nWe can use the lapp()function to compute the Normalized Difference Vegetation Index (NDVI). (More on this later in the quarter!) Let’s calculate NDVI for Zion National Park using multispectral satellite data.\nFirst, we need to define a function to calculate NDVI. Then, we can use lapp() to calculate NDVI in each raster cell. To do so, we just need the NIR and red bands.\n\n# define NDVI as the normalized difference between NIR and red bands\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\n# apply NDVI function to Landsat bands 3 & 4\nndvi_rast &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n\n\ntm_shape(ndvi_rast) +\n  tm_raster(col.legend = tm_legend(title = \"NDVI\"))\n\n\n\n\n\n\n\n\n\n\nFocal Operations\nLocal operations operate on one cell, though from multiple layers. Focal operations take into account a central (focal) cell and its neighbors. The neighborhood (or kernel, moving window, filter) can take any size or shape. A focal operation applies an aggregation function to all cells in the neighborhood and updates the value of the central cell before moving on to the next central cell.\nThe image below provides an example of using a moving window filter. The large orange square highlights the 8 cells that are considered “neighbors” to the central cell (value = 8). Using this approach, the value of the central cell will be updated to the minimum value of its neighboring cells (in this case 0). This process then repeats for each cell.\nNote: Why are the cells on the border in the filtered raster now have values of NA?\nThe cells along the border do not have a complete set of “neighbors”, therefore the filtering operation returns a NA.\n\nGeocomputation with R\n\n\n\n\n\n\n\nTipCritical thinking\n\n\n\n\n\nWhy are the cells on the border in the filtered raster now have values of NA?\nThe cells along the border do not have a complete set of “neighbors”, therefore the filtering operation returns a NA.\n\n\n\nWe can use the focal() function to perform spatial filtering. We define the size, shape, and weights of the moving window using a matrix. In the following example we’ll find the minimum value in 9x9 cell neighborhoods.\n\nelevation_focal &lt;- focal(zion_elevation, \n                         w = matrix(1, nrow = 9, ncol = 9), # create moving window\n                         # 1 = weight assigned to each cell, every cell is equally considered\n                         fun = min) # function to map new values\n\n\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"original\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\nmap2 &lt;- tm_shape(elevation_focal) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"aggregated\")+ \n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\nNote:What should we expect to observe in the output for this spatial filtering example?\n\nOverall, we see more lower values because we are finding the minimum value in each neighborhood\nThe output looks “grainier” because many cells have the same values as their neighbors\n\n\n\nZonal Operations\nSimilar to focal operations, zonal operations apply an aggregation function to multiple cells. However, instead of applying operations to neighbors, zonal operations aggregate based on “zones”. Zones can are defined using a categorical raster and do not necessarily have to be neighbors\nFor example, we could find the average elevation within the elevations zones we created.\n\nzonal(zion_elevation, reclassified, fun = \"mean\") %&gt;%\n  kable(col.names = c(\"Elevation zone\", \"Mean elevation (m)\")) %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nElevation zone\nMean elevation (m)\n\n\n\n\n0\n0.000\n\n\n1\n1292.281\n\n\n2\n1771.967\n\n\n3\n2222.776\n\n\n4\n2640.340"
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html#changing-extent-origin-and-resolution",
    "href": "course-materials/labs/notes/week4_notes.html#changing-extent-origin-and-resolution",
    "title": "Week 4: Lab",
    "section": "1. Changing extent, origin, and resolution",
    "text": "1. Changing extent, origin, and resolution\n\nExtent\nIn the simplest case, two images differ only in their extent. Let’s start by increasing the extent of a elevation raster.\n\nelev_2 &lt;- extend(zion_elevation, c(1, 200)) # add one row and two columns to the top AND bottom , default = fill with NA values\n\n# can change with fill = 1, 6, etc. \n\nnote: check different dimensions between zion_elevation and elev_2\nIf we wanted to add rows to only bottom:"
  },
  {
    "objectID": "course-materials/labs/notes/week4_notes.html#resampling",
    "href": "course-materials/labs/notes/week4_notes.html#resampling",
    "title": "Week 4: Lab",
    "section": "2. Resampling",
    "text": "2. Resampling\nAggregation/disaggregation work when both rasters have the same origins.\nBut what do we do in the case where we have two or more rasters with different origins and resolutions? Resampling computes values for new pixel locations based on custom resolutions and origins.\nThe images below show that we are trying to find the values of the original raster within the cells defined by the new “target” raster.\n\nGeocomputation with R\n\nIn most cases, the target raster would be an object you are already working with, but here we define a target raster.\n\ntarget_rast &lt;- rast(xmin = -113.2, xmax = -112.9,\n                   ymin = 37.14, ymax = 37.5,\n                   nrow = 450, ncol = 460, \n                   crs = crs(zion_elevation))\n\nzion_elevation_resample &lt;- resample(zion_elevation, y = target_rast, method = \"bilinear\")\n\n\nmap4 &lt;- tm_shape(zion_elevation_resample) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (m)\"))+\n  tm_title(text = \"resampled\")+\n  tm_layout(meta.margins = c(.2, 0, 0, 0))\n\n\ntmap_arrange(map1, map4, nrow = 1)"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#explore-elevation-at-mt.-mongón-perú",
    "href": "course-materials/discussions/week4-discussion.html#explore-elevation-at-mt.-mongón-perú",
    "title": "Week 4: Discussion Section",
    "section": "2. Explore elevation at Mt. Mongón, Perú",
    "text": "2. Explore elevation at Mt. Mongón, Perú\n\nMake a boxplot and histogram of elevation at Mt. Mongón, Perú\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhist(dem,\n     main = \"Digital Elevation Model Distribution\",\n     xlab = \"Value\")\n\n\n\n\n\n\n\nboxplot(dem,\n        main = \"Digital Elevation Model Distribution\",\n        ylab = \"Value\")\n\n\n\n\n\n\n\n\n\n\n\n\nReclassify dem and compute the mean for the three classes:\n\nLow, where elevation is less than 300\nMedium\nHigh, where elevation is greater than 500\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Define a reclassification matrix\nrcl &lt;- matrix(c(-Inf, 300, 0, # values -Inf to 300 = 0\n                300, 500, 1,  # values 300 to 500 = 1\n                500, Inf, 2), # values 500 to Inf = 2\n              ncol = 3, byrow = TRUE)\n\n# Apply the matrix to reclassify the raster, making all cells 0 or 1 or 2\ndem_rcl &lt;- terra::classify(dem, rcl = rcl)\n\n# Assign labels to the numerical categories\nlevels(dem_rcl) &lt;- tibble::tibble(id = 0:2, \n                                  cats = c(\"low\", \"medium\", \"high\"))\n\n# Calculate mean elevation for each category using original DEM values\nelevation_mean &lt;- terra::zonal(dem, dem_rcl, fun = \"mean\")\nelevation_mean\n\n    cats      dem\n1    low 274.3910\n2 medium 392.0486\n3   high 765.2197"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#explore-ndvi-and-ndwi-at-zion-national-park",
    "href": "course-materials/discussions/week4-discussion.html#explore-ndvi-and-ndwi-at-zion-national-park",
    "title": "Week 4: Discussion Section",
    "section": "3. Explore NDVI and NDWI at Zion National Park",
    "text": "3. Explore NDVI and NDWI at Zion National Park\nLandsat 8 bands 2-5 correspond to bands 1-4 for this raster. Bands are as follows:\n\n\n\nBand\n\nColor\n\n\n\n\n1\nblue\n30 meter\n\n\n2\ngreen\n30 meter\n\n\n3\nred\n30 meter\n\n\n4\nnear-infrared\n30 meter\n\n\n\n\n\n\n\n\n\nApply a scale factor and offset for all grid cells\n\n\n\n\nLandsat Level-2 products are written as scaled integers to allow us to convert the data from floating point to integer for delivery. In most cases these are written to a 16-bit integer, which saves disk space and provides faster download times. Each floating point pixel has an offset applied and then multiplied by a gain to bring the value into the 16-bit integer (or unsigned integer) range. These values are referred to as scaled integers. To allow the user to get the data back to its original floating point value, a scale factor and offset are provided for each band.\n\n\n\n\nScience product\nScale factor\nOffset\n\n\n\n\nSurface reflectance\n0.0000275\n-0.2\n\n\n\n\n\nFirst, correct the scale across all grid cells and then apply the following functions at each grid cell, in the image:\n\n\\[\\text{NDWI} = \\frac{\\text{(green - NIR)}}{\\text{(green + NIR)}}\\] \\[\\text{NDVI} = \\frac{\\text{(NIR - red)}}{\\text{(NIR + red)}}\\]\n\nCalculate the Normalized Difference Vegetation Index (NDVI) at Zion National Park\nCalculate the Normalized Difference Water Index (NDWI) at Zion National Park\nFind a correlation between NDVI and NDWI at Zion National Park\n\nHint: Explore the terra package to find a function that can help achieve this!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nscale_factor &lt;- 0.0000275\noffset &lt;- 0.2\n\nscale_function &lt;- function(x) {\n  x * scale_factor + offset\n}\n\nlandsat_scaled &lt;- terra::app(landsat, fun = scale_function)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nndwi_fun &lt;- function(green, nir){\n    (green - nir)/(green + nir)\n}\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red)/(nir + red)\n}\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nndwi_rast &lt;- terra::lapp(landsat_scaled[[c(2, 4)]],\n                         fun = ndwi_fun)\n\nndvi_rast &lt;- terra::lapp(landsat_scaled[[c(4, 3)]],\n                         fun = ndvi_fun)\n\n\nplot(ndwi_rast,\n     main = \"Zion National Park NDWI\")\n\n\n\n\n\n\n\nplot(ndvi_rast,\n     main = \"Zion National Park NDVI\")\n\n\n\n\n\n\n\n\n\ncombine &lt;- c(ndvi_rast, ndwi_rast) # Stack rasters\n\nplot(combine, main = c(\"NDVI\", \"NDWI\")) # Plot\n\n\n\n\n\n\n\n\n\n# Calculate correlation between raster layers \nterra::layerCor(combine, fun = cor)\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.9062336\n[2,] -0.9062336  1.0000000"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#change-resolution-of-elevation-at-zion-national-park",
    "href": "course-materials/discussions/week4-discussion.html#change-resolution-of-elevation-at-zion-national-park",
    "title": "Week 4: Discussion Section",
    "section": "4. Change resolution of elevation at Zion National Park",
    "text": "4. Change resolution of elevation at Zion National Park\n\nUse all the methods available to change the resolution of elevation at Zion National Park to 0.01 by 0.01 degrees\n\nNote: The srtm raster has a resolution of 0.00083 by 0.00083 degrees\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(srtm)\n\n\n\n\n\n\n\n\n\nrast_template &lt;- terra::rast(terra::ext(srtm), res = 0.01) # Create empty template\n\n\nsrtm_resampl1 &lt;- terra::resample(srtm, y = rast_template, method = \"bilinear\")\nsrtm_resampl2 &lt;- terra::resample(srtm, y = rast_template, method = \"near\")\nsrtm_resampl3 &lt;- terra::resample(srtm, y = rast_template, method = \"cubic\")\nsrtm_resampl4 &lt;- terra::resample(srtm, y = rast_template, method = \"cubicspline\")\nsrtm_resampl5 &lt;- terra::resample(srtm, y = rast_template, method = \"lanczos\")\n\n\nsrtm_resampl_all &lt;- c(srtm_resampl1, srtm_resampl2, srtm_resampl3, srtm_resampl4, srtm_resampl5)\nlabs &lt;- c(\"Bilinear\", \"Near\", \"Cubic\", \"Cubic Spline\", \"Lanczos\")\n\nplot(srtm_resampl_all, main = labs)"
  },
  {
    "objectID": "course-materials/labs/notes/week5_notes.html",
    "href": "course-materials/labs/notes/week5_notes.html",
    "title": "Week 5: Lab",
    "section": "",
    "text": "terra\nIn this lab we’ll explore operations that rely on interactions between vector and raster datasets, including how to convert raster data into vector data."
  },
  {
    "objectID": "course-materials/labs/notes/week5_notes.html#set-up",
    "href": "course-materials/labs/notes/week5_notes.html#set-up",
    "title": "Week 5: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nFirst, we’ll load all relevant packages.\n\nlibrary(sf) # vector handling\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data\nlibrary(viridisLite)\n\nToday we’re heading back to Zion National Park in Utah to explore the interactions between vector and raster data.\n\nPhoto from Unsplash\n\nWe’ll load the following data from the {spDataLarge} package:\n\nsrtm.tif: remotely sensed elevation estimates (raster data)\nzion.gpkg: boundary of Zion National Park (vector data)\n\n\n# load raster dataset\nelevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n# load vector dataset\nboundary &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\n\n\n\n\n\n\n\nWarningRemember to check the CRS!\n\n\n\nWhenever we work with multiple spatial datasets, we need check that the coordinate reference systems match. If they don’t, we need to transform one to match the other.\n\n# check if coordinate reference systems match\nif(crs(elevation) == crs(boundary)) {\n  print(\"Coordinate reference systems match\")\n} else{\n  warning(\"Updating coordinate reference systems to match\")\n  # transform data to match\n  boundary &lt;- st_transform(boundary, st_crs(elevation))\n}\n\nWarning: Updating coordinate reference systems to match\n\n\n\n\n\n\nCode\ntm_shape(elevation) +\n  tm_raster(col.legend = tm_legend(title = \"Elevation (meters)\")) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2)"
  },
  {
    "objectID": "course-materials/labs/notes/week5_notes.html#raster-cropping",
    "href": "course-materials/labs/notes/week5_notes.html#raster-cropping",
    "title": "Week 5: Lab",
    "section": "2. Raster cropping",
    "text": "2. Raster cropping\nMany geographic data projects involve integrating data from many different sources, such as remote sensing images (rasters) and administrative boundaries (vectors). Often the extent of input raster datasets is larger than the area of interest. In this case, raster cropping and masking are useful for unifying the spatial extent of input data. Both operations reduce object memory use and associated computational resources for subsequent analysis steps and may be a necessary preprocessing step when creating attractive maps involving raster data.\nFirst, let’s crop the extent of the elevation raster to match the extent of Zion’s boundaries. Through this process, we eliminate grid cells that fall outside the extent of the park and reduce the size of the raster. To do so, we use the terra::crop() function.\n\n# crop raster to extent of vector object (boundary)\nelevation_cropped &lt;- crop(elevation, boundary)\n\nBeyond matching the extent, we can also set the values of raster cells outside of the boundaries or the park to NA using terra::mask().\n\n# mask raster based on vector object\n# (cells outside of vector are converted to NA)\nelevation_masked &lt;- mask(elevation, boundary)\n\nOften, we will want to combine both cropping and masking to reduce the size of the raster as much as possible.\n\n# crop and mask raster\nelevation_final &lt;- mask(elevation_cropped, boundary)\n\nIn some cases, we may want to mask the raster cells inside of the boundaries (i.e. assign cells inside the park to NA). We can do so with terra::mask() by setting the argument inverse = TRUE.\n\n# mask raster based on vector object\n# (cells inside of vector are converted to NA)\nelevation_inv_masked &lt;- mask(elevation_cropped, boundary, inverse = TRUE)\n\n\n\nCode\nmap1 &lt;- tm_shape(elevation) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text = \"original\")\n\nmap2 &lt;- tm_shape(elevation_cropped) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text =  \"cropped\")\n\nmap3 &lt;- tm_shape(elevation_masked) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text  = \"masked\")\n\nmap4 &lt;- tm_shape(elevation_final) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text =  \"cropped & masked\")\n\nmap5 &lt;- tm_shape(elevation_inv_masked) +\n  tm_raster(col.legend = tm_legend_hide()) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_title(text =  \"inverse mask\")\n\ntmap_arrange(map1, map2, map3, map4, map5, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/notes/week5_notes.html#raster-vectorization",
    "href": "course-materials/labs/notes/week5_notes.html#raster-vectorization",
    "title": "Week 5: Lab",
    "section": "3. Raster vectorization",
    "text": "3. Raster vectorization\nThere are several ways to convert raster data into vector. The most common, and straightforward, is converting raster grid cells into polygons. For more examples, check out Geocomputation with R.\nWe could simply convert all grid cells into polygons, but it may be more helpful to create polygons based on some condition\n\n\n\n\n\n\nTipTip for HW3\n\n\n\nThe following example is relevant to homework assignment 3!\n\n\nIn this example, we’ll select grid cells higher than 2000 meters by masking the elevation raster. We’ll then convert these grid cells into polygons using the terra::as.polygons() function and turn this into a sf object.\n\nelevation_mask &lt;- elevation_final\n\n# Set all elevation values below 2000 to NA\nelevation_mask[elevation_mask &lt; 2000] &lt;- NA\n\n\n#Convert high elevation (above 2000) pixels into polygon shapes and then transform into a simple feature \nelevation_mask_poly &lt;- as.polygons(elevation_mask) %&gt;% \n  st_as_sf()\n\n\n\nCode\nmap1 &lt;- tm_shape(elevation_mask) +\n  tm_raster() +\n  tm_title( text = \"masked raster\")+ \n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n\nmap2 &lt;- tm_shape(elevation_mask_poly) +\n  tm_polygons() +\n  tm_title(text = \"vectorized raster\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\nRayshader\nRayshader is an R package that creates beautiful 2D and 3D maps from elevation data. It uses various shading techniques to make terrain look realistic, including:\nLambertian shading - simulates direct sunlight on slopes Ambient occlusion - adds shadows in valleys and crevices Hypsometric tinting - colors based on elevation (greens for low, browns/whites for high) Texture shading - emphasizes ridges and drainage patterns Sphere shading - adds natural ambient lighting effects\nThese techniques can be layered together to create stunning, professional-quality terrain maps in just a few lines of code.\n\nRoad trip! We are headed northeast to Bryce Canyon National Park!We will use the rayshader package, as well as osmdatato visualize the hoodoos of Bryce Canyon in 3D.\n\n#install.packages(\"rayshader\")\n#install.packages(\"magick\")\n#install.packages(\"osmdata\")\nlibrary(rayshader)\nlibrary(magick)\n\nLinking to ImageMagick 6.9.13.29\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(osmdata)\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\n\n\nbryce = raster(\"course-materials/data/week5/Bryce_Canyon.tif\")\n\n# Transform the spatial data structure extracted by raster into a regular R matrix\nbryce_mat = raster_to_matrix(bryce)\n\n# Reduce size by 1/4 for plotting purposes\nbryce_small = resize_matrix(bryce_mat,0.25)\n\n\n\nBuilding Basemaps with Rayshader\n\nbryce_small %&gt;% \n  #map colors to elevation values\n  height_shade() %&gt;%  # Default is dark to light gradient - not intuitive! \n  plot_map() # plot map \n\n\nbryce_small %&gt;% \n  height_shade() %&gt;% \n  \n  # Add desert-colored ambient lighting overlay at 50% transparency\n  # zscale controls vertical exaggeration, colorintensity makes colors more vibrant\n  add_overlay(sphere_shade(bryce_small, texture = \"desert\", \n                           zscale=4, colorintensity = 5), alphalayer=0.5) %&gt;%\n  \n  # Add hillshade shadow ( Lambertian (cosine) shading) \n  # Second parameter (0) controls shadow intensity - 0 means full strength\n  add_shadow(lamb_shade(bryce_small,zscale=6), 0) %&gt;%\n  \n  # better defines ridges and drainage networks, which aren’t well-captured by Lambertian shading.\n  add_shadow(ambient_shade(bryce_small), 0) %&gt;%\n  \n  # Add texture shading (emphasizes ridges and drainage patterns)\n  # 0.1 means apply at 10% intensity for subtle effect\n  add_shadow(texture_shade(bryce_small,detail=8/10,contrast=9,brightness = 11), 0.1) %&gt;%\n  plot_map()\n\nNotes: texture_shade: - detail : - Controls the scale/fineness of texture features to emphasize - Lower values (0.1-0.3): Shows only major ridges and valleys (coarse features) - Higher values (0.7-1.0): Shows fine details like small gullies and ripples\n-contrast: - Controls how dramatic the light/dark differences are - Lower values (1-3): Subtle, soft texture shading - Higher values (9-20): Sharp, dramatic contrast between ridges and valleys\nbrightness: - Controls the overall lightness of the texture layer - Lower values (1-5): Darker overall - Higher values (10-20): Brighter overall, prevents over-darkening\n\n# Define UTM bounding box coordinates for our area of interest ( for our OSM)\nutm_bbox &lt;- c(xmin = 396367.4, xmax = 397975.2, \n              ymin = 4163747.9, ymax = 4165291.0)\n\n\n# Create extent object from bounding box coordinates\nextent_zoomed = extent(utm_bbox[1], utm_bbox[2], utm_bbox[3], utm_bbox[4])\n\n# Crop the full Bryce raster to just our area of interest\nbryce_zoom = crop(bryce, extent_zoomed)\n\n# Convert the cropped raster to a matrix for rayshader\nbryce_zoom_mat = raster_to_matrix(bryce_zoom)\n\n# Build the base map with multiple shading layers\nbase_map = bryce_zoom_mat %&gt;% \n  height_shade() %&gt;%\n  add_overlay(sphere_shade(bryce_zoom_mat, texture = \"desert\", colorintensity = 5), alphalayer=0.5) %&gt;%\n  add_shadow(lamb_shade(bryce_zoom_mat), 0) %&gt;%\n  add_shadow(ambient_shade(bryce_zoom_mat),0) %&gt;% \n  add_shadow(texture_shade(bryce_zoom_mat,detail=8/10,contrast=9,brightness = 11), 0.1)\n\nplot_map(base_map)\n\n\n\nUsing OSM to add features to our base map\n\nosm_bbox = c(-112.174228, 37.614998, -112.156230,37.629084)\n\n# Query OpenStreetMap for all \"highway\" features (roads, paths, trails)\nbryce_highway = opq(osm_bbox) %&gt;% \n  add_osm_feature(\"highway\") %&gt;% # specify the feature we want to find\n  osmdata_sf() # convert object to sf\nbryce_highway\n\nNote: opq = overpassquery - What it does: Creates a query to the OpenStreetMap (OSM) Overpass API to download map data for a specific geographic area. - The Overpass API is: A service that lets you search and download specific features from OpenStreetMap’s database (like roads, trails, buildings, waterways, etc.)\n\n# Transform OSM data to match the coordinate system of our Bryce raster\nbryce_lines = st_transform(bryce_highway$osm_lines, crs=crs(bryce))\n\ntm_shape(bryce_lines) +\n  tm_lines(col = \"black\") +\n  tm_title(text = \"Open Street Map `highway` attribute in Bryce Canyon National Park\")\n\n\n# be a bit more specific with our lines - separate into trails, footpaths, and roads\nbryce_trails = bryce_lines %&gt;% \n  filter(highway %in% c(\"path\",\"bridleway\"))\n\nbryce_footpaths = bryce_lines %&gt;% \n  filter(highway %in% c(\"footway\"))\n\nbryce_roads = bryce_lines %&gt;% \n  filter(highway %in% c(\"unclassified\", \"secondary\", \"tertiary\", \"residential\", \"service\"))\n\n\n# Create a combined trails layer with different styles for each type\n\n#generate_line_overlay() takes spatial line data (from OpenStreetMap) and converts it into a raster image overlay that can be added to your rayshader map\n\n\ntrails_layer = generate_line_overlay(bryce_footpaths,extent = extent_zoomed,\n                                    linewidth = 10, color=\"black\", \n                                    heightmap = bryce_zoom_mat) %&gt;% # size and resolution to make the overlay image so it matches your terrain perfectly\n  add_overlay(generate_line_overlay(bryce_footpaths,extent = extent_zoomed,\n                                    linewidth = 6, color=\"white\",\n                                    heightmap = bryce_zoom_mat)) %&gt;%\n  add_overlay(generate_line_overlay(bryce_trails,extent = extent_zoomed,\n                                    linewidth = 3, color=\"black\", lty=3, offset = c(2,-2),\n                                    heightmap = bryce_zoom_mat)) %&gt;%\n  add_overlay(generate_line_overlay(bryce_trails,extent = extent_zoomed,\n                                    linewidth = 3, color=\"white\", lty=3,\n                                    heightmap = bryce_zoom_mat)) %&gt;%\n  add_overlay(generate_line_overlay(bryce_roads,extent = extent_zoomed,\n                                    linewidth = 8, color=\"grey30\",\n                                    heightmap = bryce_zoom_mat)) \n\nNote: : generate_line_overlay produces first layer, and then we are adding on top so we use add_overlay\n\n# add water lines \nbryce_water_lines = opq(osm_bbox) %&gt;% \n  add_osm_feature(\"waterway\") %&gt;% \n  osmdata_sf() \n\n\ntm_shape(bryce_water_lines$osm_lines) +\n  tm_lines(col = \"blue\") +\n  tm_title(text = \"Open Street Map `waterway` attribute in Bryce Canyon National Park\")\n\n\n#transform streams to have same crs as bryce\nbryce_streams = st_transform(bryce_water_lines$osm_lines,crs=crs(bryce)) \n\n#create stream layer\nstream_layer = generate_line_overlay(bryce_streams,extent = extent_zoomed,\n                                    linewidth = 4, color=\"skyblue2\", \n                                    heightmap = bryce_zoom_mat)\n\n\nbryce_tourism = opq(osm_bbox) %&gt;% \n  add_osm_feature(\"tourism\") %&gt;% \n  osmdata_sf() \n\nbryce_tourism_points = st_transform(bryce_tourism$osm_points,crs=crs(bryce))\n\nbryce_attractions = bryce_tourism_points %&gt;% \n  filter(tourism == \"attraction\")\n\n\nattraction_layer = generate_label_overlay(bryce_attractions, extent = extent_zoomed,\n                                     text_size = 2, point_size = 2, color = \"white\", \n                                     halo_color = \"black\",\n                                     halo_expand = 10, #how thick the halo is\n                                     halo_blur = 20, # how blurry or soft the halo is \n                                     halo_alpha = 0.8, # transparency of halo\n                                     heightmap = bryce_zoom_mat,\n                                     data_label_column = \"name\") # # Which column from bryce_attractions to use as labels\n\n\n\nCreate National Park Map\n\nbase_map %&gt;% \n  add_overlay(stream_layer, alphalayer = 0.8) %&gt;% \n  add_overlay(trails_layer) %&gt;%\n  add_overlay(attraction_layer) %&gt;% \n  plot_map(title_text = \"Bryce Canyon National Park, Utah\",\n           title_bar_color = \"lightgray\", title_bar_alpha = 1)\n\n\n\nCreating a 3D plot of Bryce Canyon’s streams and trails\n\nbase_map %&gt;% \n  add_overlay(stream_layer, alphalayer = 0.8) %&gt;% \n  add_overlay(trails_layer) %&gt;%\n  # Specify window size\n  plot_3d(bryce_zoom_mat, windowsize=c(1200,800))\n# Set camera angle: theta= horizontal rotation, phi=vertical rotation ( birds eye = 0), zoom=distance, fov=field of view ( narrow view = 30, normal = 60, wide = 90)\nrender_camera(theta=240,  phi=30, zoom=0.3,  fov=60)\n\n# take a snapshot of the current view\nrender_snapshot()"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#subset-points-in-new-zealandaotearoa",
    "href": "course-materials/discussions/week5-discussion.html#subset-points-in-new-zealandaotearoa",
    "title": "Week 5: Discussion Section",
    "section": "Subset Points in New Zealand/Aotearoa",
    "text": "Subset Points in New Zealand/Aotearoa\n\n# Subset New Zealand elevation points to &gt; 3100 meters\nnz_height3100 &lt;- nz_height %&gt;% \n  dplyr::filter(elevation &gt; 3100)\n\n# Create template: define the extent, resolution, and CRS based on nz_height3100\nnz_template &lt;- rast(terra::ext(nz_height3100), \n                    resolution = 3000, \n                    crs = terra::crs(nz_height3100))"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#count-points-in-each-grid-cell",
    "href": "course-materials/discussions/week5-discussion.html#count-points-in-each-grid-cell",
    "title": "Week 5: Discussion Section",
    "section": "Count Points in Each Grid Cell",
    "text": "Count Points in Each Grid Cell\n\n# Convert vector points to raster data\n# Function \"length\" returns a count of the elevation points per cell\nnz_raster &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = \"length\")\n\nplot(nz_raster, main = \"Number of Elevation Points &gt; 3100 in Each Grid Cell\")\nplot(st_geometry(nz_height3100), add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#find-maximum-elevation-in-each-grid-cell",
    "href": "course-materials/discussions/week5-discussion.html#find-maximum-elevation-in-each-grid-cell",
    "title": "Week 5: Discussion Section",
    "section": "Find Maximum Elevation in Each Grid Cell",
    "text": "Find Maximum Elevation in Each Grid Cell\n\n# function \"max\" returns maximum elevation value per cell\nnz_raster2 &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = max)\n\nplot(nz_raster2, main = \"Maximum Elevation in Each Grid Cell \")\nplot(st_geometry(nz_height3100), add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#aggregate-and-resample-raster",
    "href": "course-materials/discussions/week5-discussion.html#aggregate-and-resample-raster",
    "title": "Week 5: Discussion Section",
    "section": "Aggregate and Resample Raster",
    "text": "Aggregate and Resample Raster\n\n# Reduce the resolution by combining 2 cells in each direction into larger cells\n# Sum the values of all cells for the resulting elevation value\nnz_raster_low &lt;- aggregate(nz_raster, fact = 2, fun = sum, na.rm = TRUE)\n\n# Convert the new raster's resolution back to the 3kmx3km resolution of original raster\nnz_resample &lt;- resample(nz_raster_low, nz_raster)\n\nplots &lt;- c(nz_raster, nz_resample)\nlabs &lt;- c(\"Original 6 x 6 km\", \"Resample 6 x 6 km\")\nplot(plots, main = labs)\n\n\n\n\n\n\n\nplot(nz_raster_low, main = \"Resample 3 x 3 km\")"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#vectorize-raster",
    "href": "course-materials/discussions/week5-discussion.html#vectorize-raster",
    "title": "Week 5: Discussion Section",
    "section": "Vectorize Raster",
    "text": "Vectorize Raster\n\n# Convert raster data to polygon vector data\ngrain_poly &lt;- as.polygons(grain) %&gt;% \n  st_as_sf()\n\nplot(grain, main = \"Grain (Raster)\")\n\n\n\n\n\n\n\nplot(grain_poly, main = \"Grain (Vector)\")\n\n\n\n\n\n\n\n# Subset polygons to only clay\nclay &lt;- grain_poly %&gt;% \n  dplyr::filter(grain == \"clay\")\n\nplot(clay, main = \"Clay\")"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#high-elevation-points-in-new-zealandaotearoa",
    "href": "course-materials/discussions/week5-discussion.html#high-elevation-points-in-new-zealandaotearoa",
    "title": "Week 5: Discussion Section",
    "section": "3. High elevation points in New Zealand/Aotearoa",
    "text": "3. High elevation points in New Zealand/Aotearoa\n\nSubset points higher than 3100 meters in nz_height\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnz_height3100 &lt;- nz_height |&gt;  \n  dplyr::filter(elevation &gt; 3100) # rows where column elevation &gt; 3100 meters\n\n\n\n\n\nCreate a template raster with rast(), where the resolution is 3 km x 3 km, with the same extent and CRS as the previous subset\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnz_template &lt;- rast(terra::ext(nz_height3100), # Set extent based on nz_height3100\n                    resolution = 3000, \n                    crs = terra::crs(nz_height3100)) # Set CRS based on nz_height3100\n\n\n\n\n\nCount numbers of the highest points in each grid cell\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnz_raster &lt;- rasterize(nz_height3100, nz_template, # Convert vector points to raster data\n                       fun = \"length\") # \"length\" returns a count per cell\n\n\nplot(nz_raster, main = \"Number of elevation points &gt; 3100 in each grid cell\")\nplot(st_geometry(nz_height3100), add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nFind the maximum elevation in each grid cell\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnz_raster2 &lt;- rasterize(nz_height3100, nz_template, # Convert vector points to raster data\n                        fun = max) # \"max\" returns maximum elevation value per cell\n\n\nplot(nz_raster2, main = \"Maximum elevation in each grid cell \")\nplot(st_geometry(nz_height3100), add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nWith the raster template, complete the following:\n\n\nAggregate raster that counts the highest points in New Zealand/Aotearoa, to reduce its geographic resolution by half\nResample back to the original resolution\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnz_raster_low &lt;- aggregate(nz_raster, \n                           fact = 2, # Reduce resolution by combining 2 cells in each direction into larger cells\n                           fun = sum, na.rm = TRUE) # Sum values of all cells for resulting elevation value\n\n\n# Convert resolution back to original 3kmx3km resolution\nnz_resample &lt;- resample(nz_raster_low, nz_raster)\n\n\nplots &lt;- c(nz_raster, nz_resample)\nlabs &lt;- c(\"Original 3 x 3 km\", \"Resample 3 x 3 km\")\nplot(plots, main = labs)\n\n\n\n\n\n\n\n\n\nplot(nz_raster_low, main = \"Resample 6 x 6 km\")"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#vectorize-grain-raster",
    "href": "course-materials/discussions/week5-discussion.html#vectorize-grain-raster",
    "title": "Week 5: Discussion Section",
    "section": "4. Vectorize grain raster",
    "text": "4. Vectorize grain raster\n\nPolygonize grain and filter to only keep squares that represent clay\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Convert raster data to polygon vector data\ngrain_poly &lt;- as.polygons(grain) %&gt;% \n  st_as_sf()\n\nplot(grain, main = \"Grain (Raster)\")\n\n\n\n\n\n\n\nplot(grain_poly, main = \"Grain (Vector)\")\n\n\n\n\n\n\n\n# Subset polygons to only clay\nclay &lt;- grain_poly %&gt;% \n  dplyr::filter(grain == \"clay\")\n\nplot(clay, main = \"Clay\")"
  },
  {
    "objectID": "assignments.html#class-materials",
    "href": "assignments.html#class-materials",
    "title": "Remote sensing data collection",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nResolutions: spatial, temporal, spectral, radiometric\n\n\n Lab\nVector and raster interactions cont.; False color images\n\n\n Lab materials to download\nVector and raster interactions cont.; False color images\n\n\n Discussion\nWorking with false color images"
  },
  {
    "objectID": "assignments.html#assignment-reminders",
    "href": "assignments.html#assignment-reminders",
    "title": "Remote sensing data collection",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportantImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nCheck-In\nCheck-In (week 6)\n11/06/2025\n11/06/2025\n\n\nHW\nHomework Assignment #3\n10/21/2025\n11/10/2025"
  },
  {
    "objectID": "assignments.html#background-reading",
    "href": "assignments.html#background-reading",
    "title": "Remote sensing data collection",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2\nHow to Interpret a False-Color Satellite Image"
  },
  {
    "objectID": "assignments.html#additional-resources",
    "href": "assignments.html#additional-resources",
    "title": "Remote sensing data collection",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nHow raster functions map to stars functions\nSpatiotemporal raster data handling with stars\nA comparison of terra and stars packages"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#rename-and-mask-the-bands",
    "href": "course-materials/discussions/week6-discussion.html#rename-and-mask-the-bands",
    "title": "Week 6: Discussion Section",
    "section": "2. Rename and mask the bands",
    "text": "2. Rename and mask the bands\n\n\n\n\n\n\nDealing with clouds and shadows\n\n\n\n\n\n“Extreme cloud cover and shadows can make the data in those areas, un-usable given reflectance values are either washed out (too bright - as the clouds scatter all light back to the sensor) or are too dark (shadows which represent blocked or absorbed light)” (Earth Lab)\n\n\n\n\nRename the bands of the pre_fire and post_fire rasters using names()\nMask out clouds and shadows of the pre_mask and post_mask rasters (i.e., set mask &gt; 0 to NA)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Rename bands\nbands &lt;- c(\"Aerosol\", \"Blue\", \"Green\", \"Red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(pre_fire_rast) &lt;- bands\nnames(post_fire_rast) &lt;- bands\n\n\n# Set all cells with values greater than 0 to NA\npre_mask[pre_mask &gt; 0] &lt;- NA\n\n# Subset raster based on mask\npre_fire_rast &lt;- mask(pre_fire_rast, mask = pre_mask)\n\n# View raster\nplot(pre_fire_rast)\n\n\n\n\n\n\n\n\n\n# Set all cells with values greater than 0 to NA\npost_mask[post_mask &gt; 0] &lt;- NA\n\n# Subset raster based on mask\npost_fire_rast &lt;- mask(post_fire_rast, mask = post_mask)\n\n# View raster\nplot(post_fire_rast)"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#plot-true-and-false-color-composites",
    "href": "course-materials/discussions/week6-discussion.html#plot-true-and-false-color-composites",
    "title": "Week 6: Discussion Section",
    "section": "3. Plot true and false color composites",
    "text": "3. Plot true and false color composites\n\nPlot a true color composite using plotRGB()\n\nMap the red band to the red channel, green to green, and blue to blue\nApply a linear stretch “lin” or histogram equalization “hist”\n\n\n\n\n\n\n\n\nHow to decide how to stretch a raster image?\n\n\n\n\n\nTo make an informed choice about whether to apply a linear stretch or histogram equalization, check the distribution of reflectance values of the bands in each raster:\n\n# View histogram for each band\nhist(pre_fire_rast,\n     maxpixels = ncell(pre_fire_rast),\n     col = \"orange\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplotRGB(pre_fire_rast, r = 4, g = 3, b = 2, stretch = \"lin\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\nplotRGB(post_fire_rast, r = 4, g = 3, b = 2, stretch = \"lin\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\nPlot two false color composite using plotRGB()\n\nMap the SWIR2 band to the red channel, NIR to green, and green to blue\nApply a linear stretch “lin” or histogram equalization “hist”\n\n\n\n\n\n\n\n\nWhat is the SWIR, NIR, Red false color scheme?\n\n\n\n“Combining SWIR, NIR, and Red bands highlights the presence of vegetation, clear-cut areas and bare soils, active fires, and smoke; in a false color image” (EOS Data Analytics)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplotRGB(pre_fire_rast, r = 7, g = 5, b = 3, stretch = \"lin\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\nplotRGB(post_fire_rast, r = 7, g = 5, b = 3, stretch = \"lin\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#calculate-the-normalized-burn-ratio-nbr",
    "href": "course-materials/discussions/week6-discussion.html#calculate-the-normalized-burn-ratio-nbr",
    "title": "Week 6: Discussion Section",
    "section": "4. Calculate the normalized burn ratio (NBR)",
    "text": "4. Calculate the normalized burn ratio (NBR)\n\nCalculate the normalized burn ratio (NBR)\n\nHint: Use lapp() like you previously did for NDVI and NDWI in Week 4\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npre_nbr_rast &lt;- terra::lapp(pre_fire_rast[[c(5, 7)]], fun = nbr_fun)\n\nplot(pre_nbr_rast, main = \"Cold Springs Pre-Fire NBR\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\npost_nbr_rast &lt;- terra::lapp(post_fire_rast[[c(5, 7)]], fun = nbr_fun)\n\nplot(post_nbr_rast, main = \"Cold Springs Post-Fire NBR\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#calculate-and-plot-difference-nbr",
    "href": "course-materials/discussions/week6-discussion.html#calculate-and-plot-difference-nbr",
    "title": "Week 6: Discussion Section",
    "section": "6. Calculate and plot difference NBR",
    "text": "6. Calculate and plot difference NBR\n\nFind the difference NBR, where \\(\\text{d}NBR = \\text{prefire}NBR- \\text{postfire}NBR\\)\nPlot the dnBR raster\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndiff_nbr &lt;- pre_nbr_rast - post_nbr_rast\n\ntm_shape(diff_nbr) +\n  tm_raster(style = \"equal\", n = 6, \n            palette = get_brewer_pal(\"YlOrRd\", n = 6, plot = FALSE),\n            title = \"Difference NBR (dNBR)\", colorNA = \"black\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#bonus-challenge-classify-severeity-levels-of-burn",
    "href": "course-materials/discussions/week6-discussion.html#bonus-challenge-classify-severeity-levels-of-burn",
    "title": "Week 6: Discussion Section",
    "section": "5. Bonus challenge: Classify severeity levels of burn",
    "text": "5. Bonus challenge: Classify severeity levels of burn\n\nUse classify() to assign the severity levels below:\n\n\n\n\nSeverity Level\ndNBR Range\n\n\n\n\nEnhanced Regrowth\n&lt; -.1\n\n\nUnburned\n-.1 to +.1\n\n\nLow Severity\n+.1 to +.27\n\n\nModerate Severity\n+.27 to +.66\n\n\nHigh Severity\n&gt; .66\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Set categories for severity levels\ncategories &lt;- c(\"Enhanced Regrowth\", \"Unburned\", \"Low Severity\", \"Moderate Severity\", \"High Severity\")\n\n# Create reclassification matrix\nrcl &lt;- matrix(c(-Inf, -0.1, 1, # group 1 ranges for Enhanced Regrowth\n                -0.1, 0.1, 2, # group 2 ranges for Unburned\n                0.1, 0.27, 3, # group 3 ranges for Low Severity\n                0.27, 0.66, 4, # group 4 ranges for Moderity Severity\n                0.66, Inf, 5), # group 5 ranges for High Severity\n                ncol = 3, byrow = TRUE)\n\n# Use reclassification matrix to reclassify dNBR raster\nreclassified &lt;- classify(diff_nbr, rcl = rcl)\n\nreclassified[is.nan(reclassified)] &lt;- NA\n\n\ntm_shape(reclassified) +\n  tm_raster(style = \"cat\",\n            labels = c(categories, \"Missing\"),\n            palette = get_brewer_pal(\"YlOrRd\", n = 5, plot = FALSE),\n            title = \"Severity Level\", colorNA = \"black\")+\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "assignments/PR.html#guidance-for-creating-your-blog-post",
    "href": "assignments/PR.html#guidance-for-creating-your-blog-post",
    "title": "Geospatial Analysis blog post",
    "section": "Guidance for creating your blog post",
    "text": "Guidance for creating your blog post\nThe goal of creating a blog post is practice communicating your work in a different format. The audience of a blog post is more general than that of the repository, therefore you may be more selective in which code and output you display. In this case you are building a narrative around your work as opposed to demonstrating how it works, so you may want to only display intermediate and final results as opposed to results of self-checks and not output code.\nThe original assignment is a starting place. You should use the background provided in the assignment as a starting place, but update the text and provide additional information and at least one additional reference."
  },
  {
    "objectID": "assignments/PR.html#submitting-assignment-1",
    "href": "assignments/PR.html#submitting-assignment-1",
    "title": "Geospatial Analysis blog post",
    "section": "Submitting assignment",
    "text": "Submitting assignment\nPlease submit your PR to this Google Form."
  },
  {
    "objectID": "assignments/PR.html#blog-post-requirements",
    "href": "assignments/PR.html#blog-post-requirements",
    "title": "Geospatial Analysis blog post",
    "section": "Blog Post Requirements",
    "text": "Blog Post Requirements\nThe style is up to you, but it should incorporate the following elements clearly:\n\nA problem statement or question that your analysis will address\nBackground on why this issue is important\n[ ]\nExplanation of the data sources, analyses, and results\nClear, consistently formatted figures - I.e. informative axis labels/legends, visually appealing and consistent colors and fonts\nDiscussion of conclusions, including any potential caveats or future directions\nPosted on your personal website\n\nResource on how to post a blog to your website by Sam Shanny Csik\nCheck out this example from a MEDS alumn"
  },
  {
    "objectID": "assignments/PR.html#requirements",
    "href": "assignments/PR.html#requirements",
    "title": "Geospatial Analysis blog post",
    "section": "Blog Post Requirements",
    "text": "Blog Post Requirements\nThe style is up to you, but it should incorporate the following elements clearly:\n\nA problem statement or question that your analysis will address\nBackground on why this issue is important\n[ ]\nExplanation of the data sources, analyses, and results\nClear, consistently formatted figures - I.e. informative axis labels/legends, visually appealing and consistent colors and fonts\nDiscussion of conclusions, including any potential caveats or future directions\nPosted on your personal website\n\nResource on how to post a blog to your website by Sam Shanny Csik\nCheck out this example from a MEDS alumn"
  },
  {
    "objectID": "assignments/PR.html#requirements-section",
    "href": "assignments/PR.html#requirements-section",
    "title": "Geospatial Analysis blog post",
    "section": "Blog Post Requirements",
    "text": "Blog Post Requirements\nThe style is up to you, but it should incorporate the following elements clearly:\n\nA problem statement or question that your analysis will address\nBackground on why this issue is important (involving an additional reference)\nExplanation of the data sources, analyses, and results\nClear, consistently formatted figures - I.e. informative axis labels/legends, visually appealing and consistent colors and fonts\nDiscussion of conclusions, including any potential caveats or future directions\nPosted on your personal website\n\nResource on how to post a blog to your website by Sam Shanny Csik\nCheck out this example from a MEDS alumn"
  },
  {
    "objectID": "assignments/PR.html",
    "href": "assignments/PR.html",
    "title": "Geospatial Analysis blog post",
    "section": "",
    "text": "The purpose of this assignment is to create a blogpost on your personal website that showcases your spatial data science skills.\nTo do so, please:\n\nCreate a blog post based on Assignment 3 or 4.\n\n\n\n\n\n\n\nTipTip\n\n\n\nPick the assignment that highlight the skills you are most interested in showcasing to potential employers."
  }
]