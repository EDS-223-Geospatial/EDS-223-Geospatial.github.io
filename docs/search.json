[
  {
    "objectID": "resources.html#r-programming",
    "href": "resources.html#r-programming",
    "title": "Resources",
    "section": "R Programming",
    "text": "R Programming\n\nTidyverse style guide\nTidy design principles\nr-spatial\nSpatial Statistics for Data Science: Theory and Practice with R. Paula Moraga, 2023."
  },
  {
    "objectID": "resources.html#coordinate-systems-and-projections",
    "href": "resources.html#coordinate-systems-and-projections",
    "title": "Resources",
    "section": "Coordinate Systems and Projections",
    "text": "Coordinate Systems and Projections\n\nGeographic vs projected coordinate systems (Esri)\nCoordinate Reference System and Spatial Projection (Earth Lab, CU Boulder)\nGuide to map projections (Axis Maps)\nChoosing a projection (Penn State)\nDiscover coordinate systems"
  },
  {
    "objectID": "resources.html#mapmaking",
    "href": "resources.html#mapmaking",
    "title": "Resources",
    "section": "Mapmaking",
    "text": "Mapmaking\n\nColor palette finder with paletteer\nColorBrewer 2.0\nIntro to Color Visualization (NASA)\nGIS icons\nGuide to common errors in map production (Journal of Maps)"
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Week 9: Land Cover Classification",
    "section": "",
    "text": "Reminder\n\n\n\nCheck-in Quiz 4 due Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week9.html#class-materials",
    "href": "course-materials/week9.html#class-materials",
    "title": "Week 9: Land Cover Classification",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nImage classification and supervised approach\nDownload data  Template  Answer key\n\n\nDiscussion\nTBD\nDownload data  Template  Answer key"
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "Week 7: Remote Sensing Data",
    "section": "",
    "text": "Reminder\n\n\n\nCheck-in Quiz 2 due Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week7.html#class-materials",
    "href": "course-materials/week7.html#class-materials",
    "title": "Week 7: Remote Sensing Data",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nData collection and processing with remote sensing data\nTemplate  Answer key\n\n\nDiscussion\nTBD\nTemplate  Answer key"
  },
  {
    "objectID": "course-materials/week7.html#background-reading",
    "href": "course-materials/week7.html#background-reading",
    "title": "Week 7: Remote Sensing Data",
    "section": "Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2\nHow to Interpret a False-Color Satellite Image"
  },
  {
    "objectID": "course-materials/week7.html#technical-background",
    "href": "course-materials/week7.html#technical-background",
    "title": "Week 7: Remote Sensing Data",
    "section": "Technical Background",
    "text": "Technical Background\n\nHow raster functions map to stars functions\nSpatiotemporal raster data handling with stars\nA comparison of terra and stars packages"
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "Week 5: Lorem Ipsum",
    "section": "",
    "text": "Session\n Topic\n Lab\n\n\n\n\nLecture\nLooking ahead in EDS 223\n\n\n\nDiscussion"
  },
  {
    "objectID": "course-materials/week5.html#class-materials",
    "href": "course-materials/week5.html#class-materials",
    "title": "Week 5: Lorem Ipsum",
    "section": "",
    "text": "Session\n Topic\n Lab\n\n\n\n\nLecture\nLooking ahead in EDS 223\n\n\n\nDiscussion"
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Week 3: Vector Operations",
    "section": "",
    "text": "Reminder\n\n\n\nAssignment 2 posted Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week3.html#class-materials",
    "href": "course-materials/week3.html#class-materials",
    "title": "Week 3: Vector Operations",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nIntro to subsetting, aggregating, summarizing, and simplifying\nTemplate  Answer key\n\n\nDiscussion\n\nTemplate  Answer key"
  },
  {
    "objectID": "course-materials/week3.html#background-reading",
    "href": "course-materials/week3.html#background-reading",
    "title": "Week 3: Vector Operations",
    "section": "Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 4\nGeocomputation with R, Chapter 5\nGIS Fundamentals, Chapter 9 Part 1\nDouglas–Peucker Algorithm (Cartography Playground)\nLine Simplification with Visvalingam–Whyatt Algorithm (Mike Bostok)"
  },
  {
    "objectID": "course-materials/week3.html#technical-background",
    "href": "course-materials/week3.html#technical-background",
    "title": "Week 3: Vector Operations",
    "section": "Technical Background",
    "text": "Technical Background\n\nsf overview\nsf cheatsheet\nIntroduction to sf and stars"
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Week 10: Active Remote Sensing",
    "section": "",
    "text": "Reminder\n\n\n\nAssignment 4 due Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week10.html#class-materials",
    "href": "course-materials/week10.html#class-materials",
    "title": "Week 10: Active Remote Sensing",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nFundamentals of LiDAR and RADAR\nDownload data  Template  Answer key\n\n\nDiscussion\nWorkflows and maps\nDownload data  Template  Answer key"
  },
  {
    "objectID": "course-materials/week10.html#background-reading",
    "href": "course-materials/week10.html#background-reading",
    "title": "Week 10: Active Remote Sensing",
    "section": "Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 9\nIntroduction to Interpreting Digital RADAR Images\nIntroduction to Light Detection and Ranging (Lidar) Remote Sensing Data (Earth Lab, CU Boulder)\nWhat is Synthetic Aperture Radar? (NASA)\nGet To Know SAR: Polarimetry (NASA)"
  },
  {
    "objectID": "course-materials/week10.html#additional-resources",
    "href": "course-materials/week10.html#additional-resources",
    "title": "Week 10: Active Remote Sensing",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nExplore Lidar Points in Plas.io (Earth Lab, CU Boulder)"
  },
  {
    "objectID": "course-materials/labs/week9.html",
    "href": "course-materials/labs/week9.html",
    "title": "Week 9: Lab",
    "section": "",
    "text": "Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\nClassifying remotely sensed imagery into land cover classes enables us to understand the distribution and change in land cover types over large areas.\nThere are many approaches for performing land cover classification:\nThis lab is based on a materials developed by Chris Kibler."
  },
  {
    "objectID": "course-materials/labs/week9.html#task",
    "href": "course-materials/labs/week9.html#task",
    "title": "Week 9: Lab",
    "section": "Task",
    "text": "Task\nIn this lab, we are using a form of supervised classification, a decision tree classifier.\nDecision trees classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data.\nIn this lab, we will create a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types: (1) green vegetation; (2) dry grass or soil; (3) urban; and (4) water.\nOur goals in this lab are:\n\nLoad and process Landsat scene\nCrop and mask Landsat data to study area\nExtract spectral data at training sites\nTrain and apply decision tree classifier\nPlot results"
  },
  {
    "objectID": "course-materials/labs/week9.html#data",
    "href": "course-materials/labs/week9.html#data",
    "title": "Week 9: Lab",
    "section": "Data",
    "text": "Data\nLandsat 5 Thematic Mapper\n\nLandsat 5\n1 scene from September 25, 2007\n\nBands: 1, 2, 3, 4, 5, 7\nCollection 2 surface reflectance product\n\nStudy area and training data\n\nPolygon representing southern Santa Barbara county\nPolygons representing training sites\n\ntype: character string with land cover type"
  },
  {
    "objectID": "course-materials/labs/week9.html#set-up",
    "href": "course-materials/labs/week9.html#set-up",
    "title": "Week 9: Lab",
    "section": "Set Up",
    "text": "Set Up\nWe’ll be working with vector and raster data, so will need both sf and terra. To train our classification algorithm and plot the results, we’ll use the rpart and rpart.plot packages.\nSet your working directory to the folder that holds the data for this lab.\n- Note: my filepaths may look different than yours!\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)"
  },
  {
    "objectID": "course-materials/labs/week9.html#load-landsat-data",
    "href": "course-materials/labs/week9.html#load-landsat-data",
    "title": "Week 9: Lab",
    "section": "Load Landsat Data",
    "text": "Load Landsat Data\nLet’s create a raster stack. Each file name ends with the band number (e.g. B1.tif).\n\nNotice that we are missing a file for band 6\nBand 6 corresponds to thermal data, which we will not be working with for this lab\n\nTo create a raster stack, we will create a list of the files that we would like to work with and read them all in at once using the rast function. We’ll then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n# list files for each band, including the full file path\nfilelist &lt;- list.files(here::here(\"course-materials\", \"data\", \"week9\", \"landsat-data\"), full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat_20070925 &lt;- rast(filelist)\n\n# update layer names to match band\nnames(landsat_20070925) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = \"lin\")"
  },
  {
    "objectID": "course-materials/labs/week9.html#load-study-area",
    "href": "course-materials/labs/week9.html#load-study-area",
    "title": "Week 9: Lab",
    "section": "Load Study Area",
    "text": "Load Study Area\nWe want to constrain our analysis to the southern portion of the county where we have training data, so we’ll read in a file that defines the area we would like to study.\n\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9\", \"SB_county_south.shp\"))\n\n# project to match the Landsat data\nSB_county_south &lt;- st_transform(SB_county_south, crs = crs(landsat_20070925))"
  },
  {
    "objectID": "course-materials/labs/week9.html#crop-and-mask-landsat-data-to-study-area",
    "href": "course-materials/labs/week9.html#crop-and-mask-landsat-data-to-study-area",
    "title": "Week 9: Lab",
    "section": "Crop and Mask Landsat Data to Study Area",
    "text": "Crop and Mask Landsat Data to Study Area\nNow, we can crop and mask the Landsat data to our study area.\n\nWhy? This reduces the amount of data we’ll be working with and therefore saves computational time\nBonus: We can also remove any objects we’re no longer working with to save space\n\n\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat_20070925, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat_20070925, SB_county_south, landsat_cropped)"
  },
  {
    "objectID": "course-materials/labs/week9.html#convert-landsat-values-to-reflectance",
    "href": "course-materials/labs/week9.html#convert-landsat-values-to-reflectance",
    "title": "Week 9: Lab",
    "section": "Convert Landsat Values to Reflectance",
    "text": "Convert Landsat Values to Reflectance\nNow we need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\nIn this case, we are working with Landsat Collection 2.\n\nThe valid range of pixel values for this collection goes from 7,273 to 43,636…\n\nwith a multiplicative scale factor of 0.0000275\nwith an additive scale factor of -0.2\n\n\nLet’s reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%!\n\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat &lt;- (landsat * 0.0000275 - 0.2) * 100\n\n# plot true color image to check results\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n# check values are 0 - 100\nsummary(landsat)\n\n      blue           green            red             NIR       \n Min.   : 1.11   Min.   : 0.74   Min.   : 0.00   Min.   : 0.23  \n 1st Qu.: 2.49   1st Qu.: 2.17   1st Qu.: 1.08   1st Qu.: 0.75  \n Median : 3.06   Median : 4.59   Median : 4.45   Median :14.39  \n Mean   : 3.83   Mean   : 5.02   Mean   : 4.92   Mean   :11.52  \n 3rd Qu.: 4.63   3rd Qu.: 6.76   3rd Qu.: 7.40   3rd Qu.:19.34  \n Max.   :39.42   Max.   :53.32   Max.   :56.68   Max.   :57.08  \n NA's   :39856   NA's   :39855   NA's   :39855   NA's   :39856  \n     SWIR1           SWIR2      \n Min.   : 0.10   Min.   : 0.20  \n 1st Qu.: 0.41   1st Qu.: 0.60  \n Median :13.43   Median : 8.15  \n Mean   :11.88   Mean   : 8.52  \n 3rd Qu.:18.70   3rd Qu.:13.07  \n Max.   :49.13   Max.   :48.07  \n NA's   :42892   NA's   :46809"
  },
  {
    "objectID": "course-materials/labs/week9.html#classify-image",
    "href": "course-materials/labs/week9.html#classify-image",
    "title": "Week 9: Lab",
    "section": "Classify Image",
    "text": "Classify Image\nLet’s begin by extracting reflectance values for training data!\n\nWe will load the shapefile identifying different locations within our study area as containing one of our 4 land cover types.\nWe can then extract the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\n# read in and transform training data\ntraining_data &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9\", \"trainingdata.shp\")) %&gt;%\n  st_transform(., crs = crs(landsat))\n\n# extract reflectance values at training sites\ntraining_data_values &lt;- extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %&gt;%\n  mutate(type = as.factor(type)) # convert landcover type to factor\n\nNext, let’s train the decision tree classifier!\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are).\n\nThe rpart function implements the CART algorithm\nThe rpart function needs to know the model formula and training data you would like to use\nBecause we are performing a classification, we set method = \"class\"\nWe also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\nTo understand how our decision tree will classify pixels, we can plot the results!\n\nNote: The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\n# establish model formula\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n                          data = SB_training_data,\n                          method = \"class\",\n                          na.action = na.omit)\n\n# plot decision tree\nprp(SB_decision_tree)\n\n\n\n\n\n\n\n\n…and apply the decision tree!\nThe terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n# classify image based on decision tree\nSB_classification &lt;- predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)\n\n[1] \"green_vegetation\" \"soil_dead_grass\"  \"urban\"            \"water\""
  },
  {
    "objectID": "course-materials/labs/week9.html#plot-results",
    "href": "course-materials/labs/week9.html#plot-results",
    "title": "Week 9: Lab",
    "section": "Plot Results",
    "text": "Plot Results\nNow we can plot the results and check out our land cover map!\n\n# plot results\ntm_shape(SB_classification) +\n  tm_raster(col.scale = tm_scale_categorical(values = c(\"#8DB580\", \"#F2DDA4\", \"#7E8987\", \"#6A8EAE\")),\n            col.legend = tm_legend(labels = c(\"green vegetation\", \"soil/dead grass\", \"urban\", \"water\"),\n                                   title = \"Landcover type\")) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "course-materials/labs/week4.html",
    "href": "course-materials/labs/week4.html",
    "title": "Week 4: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of working with raster data, including attribute, spatial, and geometry operations. This lab follows chapters 3, 4, and 5 of Geocomputation with R by Robin Lovelace."
  },
  {
    "objectID": "course-materials/labs/week4.html#set-up",
    "href": "course-materials/labs/week4.html#set-up",
    "title": "Week 4: Lab",
    "section": "Set Up",
    "text": "Set Up\n\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap)\nlibrary(geodata)"
  },
  {
    "objectID": "course-materials/labs/week4.html#manipulating-raster-objects",
    "href": "course-materials/labs/week4.html#manipulating-raster-objects",
    "title": "Week 4: Lab",
    "section": "Manipulating Raster Objects",
    "text": "Manipulating Raster Objects\nRaster data represents continuous surfaces, as opposed to the discrete features represented in the vector data model. Here we’ll learn how to create raster data objects from scratch and how to do basic data manipulations.\nLet’s create a SpatRaster object using a digitial elevation model for Zion National Park.\n\n# Set file path\nraster_filepath &lt;- system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \n\n# Create raster object\nmy_rast &lt;- rast(raster_filepath)\n\n# Test class of raster object\nclass(my_rast)\n\n# Gives summary information\nmy_rast\n\nplot(my_rast)\n\n\n\n\n\n\n\n\nWe can also create rasters from scratch using the rast() function. Here we create 36 cells centerd around (0,0). By default the CRS is set to WGS84, but we could change this with the crs argument.\nBecause we are working in WGS84, the resolution is in units of degrees. rast() fills the values of the cells row-wise starting in the upper left corner.\n\nnew_raster &lt;- rast(nrows = 6, ncols = 6, resolution = 0.5,\n                  xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n                  vals = 1:36)\n\ntm_shape(new_raster) +\n  tm_raster()\n\n\n\n\n\n\n\n\nThe SpatRaster class can also handle multiple layers.\n\nmulti_raster_file &lt;- system.file(\n  \"raster/landsat.tif\", package = \"spDataLarge\")\nmulti_rast &lt;- rast(multi_raster_file)\nmulti_rast\n\nnlyr(multi_rast) # test number of layers in raster object\n\nWe can subset layers using either the layer number or name:\n\nmulti_rast3 &lt;- subset(multi_rast, 3)\nmulti_rast4 &lt;- subset(multi_rast, \"landsat_4\")\n\nWe can combine SpatRaster objects into one, using c():\n\nmulti_rast34 &lt;- c(multi_rast3, multi_rast4)\n\nLet’s create an example raster for elevation:\n\nelev &lt;- rast(nrows = 6, ncols = 6, resolution = 0.5,\n            xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n            vals = 1:36)\n\nplot(elev)\n\n\n\n\n\n\n\n\nRasters can also hold categorical data. Let’s create an example raster for soil types:\n\ngrain_order &lt;- c(\"clay\", \"silt\", \"sand\") # set soil types\ngrain_char &lt;- sample(grain_order, 36, replace = TRUE) # randomly create character string of soil types\ngrain_fact &lt;- factor(grain_char, levels = grain_order) # convert to factors\n\ngrain &lt;- rast(nrows = 6, ncols = 6, resolution = 0.5,\n             xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n             vals = grain_fact)\n\nplot(grain)"
  },
  {
    "objectID": "course-materials/labs/week4.html#raster-subsetting",
    "href": "course-materials/labs/week4.html#raster-subsetting",
    "title": "Week 4: Lab",
    "section": "Raster Subsetting",
    "text": "Raster Subsetting\nWe can index rasters using, row-column indexing, cell IDs, coordinates, other spatial objects.\n\n# row 1, column 1\nelev[1, 1]\n\n  lyr.1\n1     1\n\n# cell ID 1\nelev[1]\n\n  lyr.1\n1     1\n\n\nIf we had a two layered raster, subsetting would return the values in both layers.\n\ntwo_layers &lt;- c(grain, elev)\ntwo_layers[1]\n\nWe can also modify/overwrite cell values.\n\nelev[1, 1] &lt;- 0\nelev[]\n\nReplacing values in multi-layer rasters requires a matrix with as many columns as layers and rows as replaceable cells.\n\ntwo_layers[1] &lt;- cbind(c(1), c(4))\ntwo_layers[]"
  },
  {
    "objectID": "course-materials/labs/week4.html#summarizing-raster-objects",
    "href": "course-materials/labs/week4.html#summarizing-raster-objects",
    "title": "Week 4: Lab",
    "section": "Summarizing Raster Objects",
    "text": "Summarizing Raster Objects\nWe can get info on raster values just by typing the name or using the summary function.\n\nelev\n\nclass       : SpatRaster \ndimensions  : 6, 6, 1  (nrow, ncol, nlyr)\nresolution  : 0.5, 0.5  (x, y)\nextent      : -1.5, 1.5, -1.5, 1.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        : lyr.1 \nmin value   :     0 \nmax value   :    36 \n\nsummary(elev)\n\n     lyr.1      \n Min.   : 0.00  \n 1st Qu.: 9.75  \n Median :18.50  \n Mean   :18.47  \n 3rd Qu.:27.25  \n Max.   :36.00  \n\n\nWe can get global summaries, such as standard deviation.\n\nglobal(elev, sd)\n\n            sd\nlyr.1 10.58432\n\n\nOr we can use freq() to get the counts with categories.\n\nfreq(grain)\n\n  layer value count\n1     1  clay    11\n2     1  silt     9\n3     1  sand    16\n\nhist(elev)"
  },
  {
    "objectID": "course-materials/labs/week4.html#spatial-subsetting",
    "href": "course-materials/labs/week4.html#spatial-subsetting",
    "title": "Week 4: Lab",
    "section": "Spatial Subsetting",
    "text": "Spatial Subsetting\nWe can move from subsetting based on specific cell IDs to extract info based on spatial objects.\nTo use coordinates for subsetting, we can “translate” coordinates into a cell ID with the terra function cellFromXY() or terra::extract().\n\nid &lt;- cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))\nelev[id]\n\n  lyr.1\n1    16\n\n# the same as\nterra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))\n\n  lyr.1\n1    16\n\n\nRaster objects can also subset with another raster object. Here we extract the values of our elevation raster that fall within the extent of a masking raster.\n\nclip &lt;- rast(xmin = 0.9, xmax = 1.8, ymin = -0.45, ymax = 0.45,\n            resolution = 0.3, vals = rep(1, 9))\n\nelev[clip]\n\n  lyr.1\n1    18\n2    24\n\n# we can also use extract\nterra::extract(elev, ext(clip))\n\nIn the previous example, we just got the values back. In some cases, we might want the output to be the raster cells themselves.\nWe can do this use the “[” operator and setting “drop = FALSE”.\nThis example returns the first 2 cells of the first row of the “elev” raster.\n\nelev[1:2, drop = FALSE]\n\nAnother common use of spatial subsetting is when we use one raster with the same extent and resolution to mask the another. In this case, the masking raster needs to be composed of logicals or NAs.\n\n# create raster mask of the same resolution and extent\nrmask &lt;- elev \n# randomly replace values with NA and TRUE to use as a mask\nvalues(rmask) &lt;- sample(c(NA, TRUE), 36, replace = TRUE) \n\n# spatial subsetting\nelev[rmask, drop = FALSE]   # with [ operator\nmask(elev, rmask)           # with mask()\n\nWe can also use a similar approach to replace values that we suspect are incorrect.\n\nelev[elev &lt; 20] = NA"
  },
  {
    "objectID": "course-materials/labs/week4.html#map-algebra",
    "href": "course-materials/labs/week4.html#map-algebra",
    "title": "Week 4: Lab",
    "section": "Map Algebra",
    "text": "Map Algebra\nHere we define map algebra as the set of operations that modify or summarize raster cell values with reference to surrounding cells, zones, or statistical functions that apply to every cell.\n\nLocal Operations\nLocal operations are computed on each cell individually. We can use oridinary arithemetic or logical statements.\n\nelev + elev\nelev^2\nlog(elev)\nelev &gt; 5\n\nWe can also classify intervals of values into groups. For example, we could classify a DEM into low, middle, and high elevation cells:\n\nFirst, we need to construct a reclassification matrix:\n\nThe first column corresponds to the lower end of the class\nThe second column corresponds to the upper end of the class\nThe third column corresponds to the new value for the specified ranges in columns 1 and 2\n\n\n\nrcl &lt;- matrix(c(0, 12, 1, 12, 24, 2, 24, 36, 3), ncol = 3, byrow = TRUE)\nrcl\n\n# We then use this matrix to reclassify our elevation matrix\nrecl &lt;- classify(elev, rcl = rcl)\nrecl\n\nFor more efficient processing, we can use a set of map algebra functions:\n\napp() applies a function to each cell of a raster to summarize the values of multiple layers into one layer\ntapp() is an extension of app() that allows us to apply on operation on a subset of layers\nlapp() allows us to apply a function to each cell using layers as arguments\n\nWe can use the lapp()function to compute the Normalized Difference Vegetation Index (NDVI).\nLet’s calculate NDVI for Zion National Park using multispectral satellite data.\n\nmulti_raster_file &lt;- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nmulti_rast &lt;- rast(multi_raster_file)\n\nWe need to define a function to calculate NDVI.\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\nSo now we can use lapp() to calculate NDVI in each raster cell. To do so, we just need the NIR and red bands.\n\nndvi_rast &lt;- lapp(multi_rast[[c(4, 3)]], fun = ndvi_fun)\n\ntm_shape(ndvi_rast) +\n  tm_raster()\n\n\n\n\n\n\n\n\n\n\nFocal Operations\nLocal operations operate on one cell, though from multiple layers. Focal operations take into account a central (focal) cell and its neighbors. The neighborhood (or kernel, moving window, filter) can take any size or shape. A focal operation applies an aggregation function to all cells in the neighborhood and updates the value of the central cell before moving on to the next central cell\nWe can use the focal() function to perform spatial filtering. We define the size, shape, and weights of the moving window using a matrix. Here we find the minimum.\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\n\nr_focal &lt;- focal(elev, w = matrix(1, nrow = 3, ncol = 3), fun = min)\n\nplot(elev)\n\n\n\n\n\n\n\nplot(r_focal)\n\n\n\n\n\n\n\n\n\n\nZonal Operations\nSimilar to focal operations, zonal operations apply an aggregation function to multiple cells. However, instead of applying operations to neighbors, zonal operations aggregate based on “zones”. Zones can are defined using a categorical raster and do not necessarily have to be neighbors\nFor example, we could find the average elevation for different soil grain sizes.\n\nzonal(elev, grain, fun = \"mean\")\n\n  lyr.1     elev\n1  clay 16.54545\n2  silt 20.66667\n3  sand 18.62500"
  },
  {
    "objectID": "course-materials/labs/week4.html#merging-rasters",
    "href": "course-materials/labs/week4.html#merging-rasters",
    "title": "Week 4: Lab",
    "section": "Merging Rasters",
    "text": "Merging Rasters\nIn some cases, data for a region will be stored in multiple, contiguous files. To use them as a single raster, we need to merge them.\nIn this example, we download elevation data for Austria and Switzerland and merge the two rasters into one.\n\naut &lt;- geodata::elevation_30s(country = \"AUT\", path = tempdir())\nch &lt;- geodata::elevation_30s(country = \"CHE\", path = tempdir())\naut_ch &lt;- merge(aut, ch)"
  },
  {
    "objectID": "course-materials/labs/week4.html#geometric-operations",
    "href": "course-materials/labs/week4.html#geometric-operations",
    "title": "Week 4: Lab",
    "section": "Geometric Operations",
    "text": "Geometric Operations\nWhen merging or performing map algebra, rasters need to match in their resolution, projection, origin, and/or extent\nIn the simplest case, two images differ only in their extent. Let’s start by increasing the extent of a elevation raster.\n\nelev &lt;- rast(system.file(\"raster/elev.tif\", package = \"spData\"))\nelev_2 &lt;- extend(elev, c(1, 2)) # add one row and two columns\n\nplot(elev)\n\n\n\n\n\n\n\nplot(elev_2)\n\n\n\n\n\n\n\n\nPerforming algebraic operations on objects with different extents doesn’t work.\n\nelev + elev_2\n\nWe can align the extent of the 2 rasters using the extend() function. Here we extend the elev object to the extent of elev_2 by adding NAs.\n\nelev_4 &lt;- extend(elev, elev_2)\n\nthe origin function returns the coordinates of the cell corner closes to the coordinates (0,0). We can also manually change the origin.\n\norigin(elev_4)\n\n[1] 0 0\n\norigin(elev_4) &lt;- c(0.25, 0.25)\norigin(elev_4)\n\n[1] 0.25 0.25"
  },
  {
    "objectID": "course-materials/labs/week4.html#aggregation-and-disaggregation",
    "href": "course-materials/labs/week4.html#aggregation-and-disaggregation",
    "title": "Week 4: Lab",
    "section": "Aggregation and Disaggregation",
    "text": "Aggregation and Disaggregation\nFaster datasets can also differ in their resolution to match resolutions we can decrease the resolution by aggregating or increase the resolution by disaggregating.\nLet’s start by changing the resolution of a DEM by a factor of 5, by taking the mean.\n\ndem &lt;- rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndem_agg &lt;-  aggregate(dem, fact = 5, fun = mean)\n\nplot(dem)\n\n\n\n\n\n\n\nplot(dem_agg)\n\n\n\n\n\n\n\n\nWe have some choices when increasing the resolution. Here, we try the bilinear method.\n\ndem_disagg &lt;- disagg(dem_agg, fact = 5, method = \"bilinear\")\nidentical(dem, dem_disagg)\n\n[1] FALSE\n\nplot(dem_disagg)"
  },
  {
    "objectID": "course-materials/labs/week4.html#resampling",
    "href": "course-materials/labs/week4.html#resampling",
    "title": "Week 4: Lab",
    "section": "Resampling",
    "text": "Resampling\nAggregation/disaggregation work when both rasters have the same origins.\nBut what do we do in the case where we have two or more rasters with different origins and resolutions? Resampling computes values for new pixel locations based on custom resolutions and origins.\nIn most cases, the target raster would be an object you are already working with, but here we define a target raster.\n\ntarget_rast &lt;- rast(xmin = 794600, xmax = 798200,\n                   ymin = 8931800, ymax = 8935400,\n                   resolution = 150, crs = \"EPSG:32717\")\n\ndem_resampl &lt;- resample(dem, y = target_rast, method = \"bilinear\")\n\nplot(dem)\n\n\n\n\n\n\n\nplot(dem_resampl)"
  },
  {
    "objectID": "course-materials/labs/week2.html",
    "href": "course-materials/labs/week2.html",
    "title": "Week 2: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of manipulating vector data in R using the sf package. The following materials are modified from Chapter 3 of Geocomputation with R by Rovin Lovelace."
  },
  {
    "objectID": "course-materials/labs/week2.html#set-up",
    "href": "course-materials/labs/week2.html#set-up",
    "title": "Week 2: Lab",
    "section": "Set Up",
    "text": "Set Up\nLet’s load all necessary packages:\n\nrm(list = ls())\nlibrary(sf)\nlibrary(spData)\nlibrary(tmap)\nlibrary(tidyverse)"
  },
  {
    "objectID": "course-materials/labs/week2.html#handling-sf-objects",
    "href": "course-materials/labs/week2.html#handling-sf-objects",
    "title": "Week 2: Lab",
    "section": "Handling sf Objects",
    "text": "Handling sf Objects\nLet’s start by looking at how we can construct a sf object. First, we create a geometry for London by supplying a point and CRS. Then, we supply some non-geographic attributes!\n\nlnd_point &lt;- st_point(c(0.1, 51.5))\n\nlnd_geom &lt;- st_sfc(lnd_point, crs = 4326)\n\nlnd_attrib &lt;- data.frame(\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nlnd_sf &lt;- st_sf(lnd_attrib, geometry = lnd_geom)\n\nclass(lnd_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can also check out what the CRS looks like:\n\nst_crs(lnd_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nst_crs(lnd_sf)$IsGeographic\n\n[1] TRUE\n\nst_crs(lnd_sf)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\nNow let’s look at an existing sf object representing countries of the world:\n\nclass(world)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(world)\n\n[1] 177  11\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nWe can see that this object contains both spatial data (“geom” column) and attributes about those geometries. We can perform operations on the attribute data, just like we would with a normal data frame.\n\nsummary(world$lifeExp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  50.62   64.96   72.87   70.85   76.78   83.59      10 \n\n\nThe geometry column is “sticky”, meaning it will stick around unless we explicitly get rid of it. To convert this object into a data frame, we need to drop the geometry column.\n\nworld_df &lt;- st_drop_geometry(world)\nclass(world_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_df)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\"\n\nncol(world)\n\n[1] 11\n\nncol(world_df)\n\n[1] 10"
  },
  {
    "objectID": "course-materials/labs/week2.html#vector-attribute-subsetting",
    "href": "course-materials/labs/week2.html#vector-attribute-subsetting",
    "title": "Week 2: Lab",
    "section": "Vector Attribute Subsetting",
    "text": "Vector Attribute Subsetting\nThe especially great things about sf objects is that we can use tidyverse functions on them!\nWe can select columns…\n\nworld %&gt;%\n  select(name_long, pop)\n\nOr remove columns…\n\nworld %&gt;%\n  select(-subregion, -area_km2)\n\nOr select AND rename columns\n\nworld %&gt;%\n  select(name = name_long, population = pop)\n\nOr filter observations based on variables\n\nworld1 &lt;- world %&gt;%\n  filter(area_km2 &lt; 10000)\n\nsummary(world1$area_km2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2417    4412    6207    5986    7614    9225 \n\nworld2 &lt;- world %&gt;%\n  filter(lifeExp &gt;= 80)\n\nnrow(world2)\n\n[1] 24"
  },
  {
    "objectID": "course-materials/labs/week2.html#chaining-commands-with-pipes",
    "href": "course-materials/labs/week2.html#chaining-commands-with-pipes",
    "title": "Week 2: Lab",
    "section": "Chaining Commands with Pipes",
    "text": "Chaining Commands with Pipes\nBecause we can use dplyr functions with sf objects, we can chain together commands using the pipe operator.\nLet’s try to find the country in Asia with the highest life expectancy\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;%\n  select(name_long, continent, lifeExp) %&gt;%\n  slice_max(lifeExp)"
  },
  {
    "objectID": "course-materials/labs/week2.html#vector-attribute-aggregation",
    "href": "course-materials/labs/week2.html#vector-attribute-aggregation",
    "title": "Week 2: Lab",
    "section": "Vector Attribute Aggregation",
    "text": "Vector Attribute Aggregation\nAggregation is the process of summarizing data with one or more ‘grouping’ variables. For example, using the ‘world’ which provides information on countries of the world, we might want to aggregate to the level of continents. It is important to note that aggregating data attributes is a different process from aggregating geographic data, which we will cover later.\nLet’s try to find the total population within each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE))\n\nLet’s also find the total area and number of countries in each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n())\n\nBuilding on this, let’s find the population density of each continent, find the continent’s with highest density and arrange by the number of countries. We’ll drop the geometry column to speed things up.\n\nworld %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  mutate(density = round(population/area_km2)) %&gt;%\n  slice_max(density, n = 3) %&gt;%\n  arrange(desc(n_countries))"
  },
  {
    "objectID": "course-materials/labs/week2.html#vector-attribute-joining",
    "href": "course-materials/labs/week2.html#vector-attribute-joining",
    "title": "Week 2: Lab",
    "section": "Vector Attribute Joining",
    "text": "Vector Attribute Joining\nA critical part of many data science workflows is combining data sets based on common attributes. In R, we do this using multiple join functions, which follow SQL conventions.\nLet’s start by looking a data set on national coffee production:\n\nhead(coffee_data)\n\n\nnrow(coffee_data)\n\n[1] 47\n\nnrow(world)\n\n[1] 177\n\n\nWe can combine this with the world data set, but joining based on country’s names:\n\nworld_coffee &lt;- left_join(world, coffee_data, by = \"name_long\")\n\nnames(world_coffee)\n\n [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n [4] \"region_un\"              \"subregion\"              \"type\"                  \n [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n[10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n[13] \"coffee_production_2017\"\n\n\nAnd plot what this looks like…\n\ntm_shape(world_coffee) +\n  tm_polygons(fill = \"coffee_production_2017\")\n\n\n\n\n\n\n\n\nIf we just wanted to keep countries that do have coffee data, we could use an inner join:\n\nworld_coffee_inner &lt;- inner_join(world, coffee_data)\nnrow(world_coffee_inner)\n\n[1] 45\n\n\nIt looks like we lost some countries with coffee data, so let’s figure out what’s going on. We can find rows that didn’t match using the setdiff function.\n\nsetdiff(coffee_data$name_long, world$name_long)\n\nWe see that one of the issues is that the two data sets use different naming conventions for the Democratic Republic of the Congo. We can use a string matching function to figure out what the DRC is called in the world data set.\n\ndrc &lt;- stringr::str_subset(world$name_long, \"Dem*.+Congo\")\n\nNow we can update the coffee data set with the matching name for the DRC:\n\ncoffee_data$name_long[grepl(\"Congo,\", coffee_data$name_long)] -&gt; drc\n\nAnd we can try the inner join again and hopefully the DRC now matches:\n\nworld_coffee_inner &lt;- inner_join(world, coffee_data)\nnrow(world_coffee_inner)\n\n[1] 45\n\n\nLet’s visualize what a the inner join did to our spatial object:\n\ntm_shape(world_coffee_inner) +\n  tm_polygons(fill = \"coffee_production_2017\",\n              title = \"Coffee production (2017)\") +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\nAnd let’s test what would happen if we flipped the order of the data sets in the join:\n\ncoffee_world &lt;- left_join(coffee_data, world, by = \"name_long\")\nclass(coffee_world)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(coffee_world)\n\n [1] \"name_long\"              \"coffee_production_2016\" \"coffee_production_2017\"\n [4] \"iso_a2\"                 \"continent\"              \"region_un\"             \n [7] \"subregion\"              \"type\"                   \"area_km2\"              \n[10] \"pop\"                    \"lifeExp\"                \"gdpPercap\"             \n[13] \"geom\""
  },
  {
    "objectID": "course-materials/labs/week1.html",
    "href": "course-materials/labs/week1.html",
    "title": "Week 1: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of map-making in R using the tmap package. The following materials are modified from Chapter 9 of Geocomputation with R by Rovin Lovelace."
  },
  {
    "objectID": "course-materials/labs/week1.html#set-up",
    "href": "course-materials/labs/week1.html#set-up",
    "title": "Week 1: Lab",
    "section": "Set Up",
    "text": "Set Up\nLet’s load all necessary packages:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\ninstall.packages(\"spData\")\ninstall.packages(\"spDataLarge\", repos = \"https://geocompr.r-universe.dev\")\nremotes::install_github(\"r-tmap/tmap@v4\")\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tmap) # for static and interactive maps\n\nLet’s also read in some data from the spDataLarge package to work with later.\n\nnz_elev &lt;- rast(system.file(\"raster/nz_elev.tif\", package = \"spDataLarge\"))"
  },
  {
    "objectID": "course-materials/labs/week1.html#map-making-basics",
    "href": "course-materials/labs/week1.html#map-making-basics",
    "title": "Week 1: Lab",
    "section": "Map-Making Basics",
    "text": "Map-Making Basics\nLet’s start with a pre-loaded spatial object representing the states of New Zealand\n\nnz\n\nWe’re going to start by using the tmap package to make some basic maps.\n\ntmap can work with spatial objects of a variety of classes, meaning it’s highly versatile\nThis approach relies on a series of functions that typically start with “tm_”\nThe first element is always tm_shape()\n\n\ntm_shape(nz) +\n  tm_fill()\n\n\n\n\n\n\n\n\nNow let’s plot just the boundaries:\n\ntm_shape(nz) +\n  tm_borders()\n\n\n\n\n\n\n\n\n…and the shapes and boundaries together:\n\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders()"
  },
  {
    "objectID": "course-materials/labs/week1.html#map-objects",
    "href": "course-materials/labs/week1.html#map-objects",
    "title": "Week 1: Lab",
    "section": "Map Objects",
    "text": "Map Objects\ntmap can store maps as objects. This means that we store a base map and add additional layers later.\n\nmap_nz &lt;- tm_shape(nz) +\n  tm_polygons()\n\n\nclass(map_nz)\n\n\nWe can add new shapes on top of the base map\nWhen we add a new shape, all subsequent aesthetic functions refer to it, until we add a new shape\n\nIn this case, we’re adding a layer with information on elevation and this layer to have 70% transparency.\n\nmap_nz1 &lt;- map_nz +\n  tm_shape(nz_elev) +\n  tm_raster(col_alpha = 0.7)\n\nmap_nz1 \n\n\n\n\n\n\n\n\nWe can add points designating high points in the country:\n\nmap_nz2 &lt;- map_nz1 +\n  tm_shape(nz_height) +\n  tm_dots()\n\nmap_nz2"
  },
  {
    "objectID": "course-materials/labs/week1.html#aesthetic-basics",
    "href": "course-materials/labs/week1.html#aesthetic-basics",
    "title": "Week 1: Lab",
    "section": "Aesthetic Basics",
    "text": "Aesthetic Basics\nUp until now, we’ve been working with the default aesthetics. There are 2 types of aesthetics: - Fixed aesthetics - Aesthetics that change with the value of a variable\nNote: tmap works differently than ggplot2 and doesn’t use the “aes()” function.\nLet’s start by changing some fixed aesthetics…First, let’s change the color used to fill the NZ shapes.\n\ntm_shape(nz) +\n  tm_polygons(fill = \"red\")\n\n\n\n\n\n\n\n\nNow change the color of the boundaries…\n\ntm_shape(nz) + \n  tm_polygons(col = \"blue\")\n\n\n\n\n\n\n\n\n…and the width of the boundary lines…\n\ntm_shape(nz) + \n  tm_polygons(lwd = 3)\n\n\n\n\n\n\n\n\n…and the line type of the boundary lines…\n\ntm_shape(nz) + \n  tm_polygons(lty = 2)\n\n\n\n\n\n\n\n\n…all together now!\n\ntm_shape(nz) + \n  tm_polygons(fill = \"red\", \n              fill_alpha = 0.3,\n              col = \"blue\", \n              lwd = 3, \n              lty = 2)\n\n\n\n\n\n\n\n\nNow let’s change the colors based on a value.\nFor example, the New Zealand dataset has a column with each state’s land area.\n\nnz\n\nLet’s try to plot the Land_area column. We might think that the following works, but it doesn’t!\n\ntm_shape(nz) +\n  tm_fill(col = nz$Land_area)\n\nInstead, tmap is expecting a character string naming the attribute associated with the geometry:\n\ntm_shape(nz) +\n  tm_fill(fill = \"Land_area\")\n\n\n\n\n\n\n\n\nWe can also add an argument that updates the title of the legend:\n\ntm_shape(nz) +\n  tm_fill(fill = \"Land_area\", title = \"Area\")\n\n\n\n\n\n\n\n\nWe can even make it more precise using the “expression” function:\n\ntm_shape(nz) +\n  tm_fill(fill = \"Land_area\", title = expression(\"Area (km\"^2*\")\")) +\n  tm_borders()"
  },
  {
    "objectID": "course-materials/labs/week1.html#color-settings",
    "href": "course-materials/labs/week1.html#color-settings",
    "title": "Week 1: Lab",
    "section": "Color Settings",
    "text": "Color Settings\nNote, how we set and define colors can radically change the interpretation of our map.\n\nThe style argument has several options for breaking data into bins:\n\nstyle = “pretty” (default) rounds breaks into evenly spaced whole numbers, where possible\nstyle = “equal” divides input values into bins with equal range (best for uniform distributions)\nstyle = “quantile” puts the same number of observations into each bin\nstyle = “jenks” identifies groups of similar values and maximizes differences between bins\n\n\nNotice how the following maps display the same data, but look quite different:\n\ntm_shape(nz) + \n  tm_polygons(fill = \"Median_income\", style = \"pretty\")\n\n\n\n\n\n\n\n\n\ntm_shape(nz) + \n  tm_polygons(fill = \"Median_income\", style = \"equal\")\n\n\n\n\n\n\n\n\n\ntm_shape(nz) + \n  tm_polygons(fill = \"Median_income\", style = \"quantile\")\n\n\n\n\n\n\n\n\n\ntm_shape(nz) + \n  tm_polygons(fill = \"Median_income\", style = \"jenks\")\n\n\n\n\n\n\n\n\nWe can also define custom bins:\n\nbreaks &lt;- c(0, 3, 4, 5) * 10000\n\ntm_shape(nz) + \n  tm_polygons(fill = \"Median_income\", breaks = breaks)\n\n\n\n\n\n\n\n\nBut in some cases we might not want to use bins:\n\nstyle = “cont” displays colors as a continuous spectrum\nstyle = “cat” uses a unique vale for each categorical value\n\n\nmap_nz +\n  tm_shape(nz_elev) +\n  tm_raster(col_alpha = 0.7) +\n  tm_scale_continuous()\n\n\n\n\n\n\n\n\n\nmap_nz +\n  tm_shape(nz) +\n  tm_polygons(fill = \"Island\")"
  },
  {
    "objectID": "course-materials/labs/week1.html#map-layout",
    "href": "course-materials/labs/week1.html#map-layout",
    "title": "Week 1: Lab",
    "section": "Map Layout",
    "text": "Map Layout\nNow that we have the basics, we can turn to all the other elements that make a cohesive map…tmap has lots of options, but we will explore just a few!\nTo clearly give readers the context of our map, we can include a compass and scale bar:\n\nmap_nz +\n  tm_compass(type = \"4star\", position = c(\"left\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), text.size = 1)\n\n\n\n\n\n\n\n\nInstead of using a compass and scale bar, we could add latitude/longitudes graticules:\n\nmap_nz +\n  tm_graticules()\n\n\n\n\n\n\n\n\nWe can also update the background color:\n\nmap_nz +\n  tm_graticules() +\n  tm_layout(bg.color = \"lightblue\")"
  },
  {
    "objectID": "course-materials/labs/week1.html#faceted-and-animated-maps",
    "href": "course-materials/labs/week1.html#faceted-and-animated-maps",
    "title": "Week 1: Lab",
    "section": "Faceted and Animated Maps",
    "text": "Faceted and Animated Maps\nWe might have data that varies over time and we want to look at the how it changes. One approach is by using faceted plots, a series of plots:\n\nurb_1970_2030 &lt;- urban_agglomerations %&gt;% \n  filter(year %in% c(1970, 1990, 2010, 2030))\n\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week1.html#interactive-maps",
    "href": "course-materials/labs/week1.html#interactive-maps",
    "title": "Week 1: Lab",
    "section": "Interactive Maps",
    "text": "Interactive Maps\ntmap is especially powerful because it allows us to make interactive maps using the same syntax.\nLet’s enter the interactive mode:\n\ntmap_mode(\"view\")\n\n\nmap_nz\n\n\n\n\n\n To go back to regular plotting, we just need enter plotting mode:\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "course-materials/discussions/week7-discussion.html",
    "href": "course-materials/discussions/week7-discussion.html",
    "title": "Discssuion Week 7",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ─────────────────── tidyverse 2.0.0.9000 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(terra)\n\nterra 1.7.78\n\nAttaching package: 'terra'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\n# practice filtering a raster\nelev &lt;- rast(nrows = 6, ncols = 6, resolution = 0.5,\n             xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n             vals = 1:36)\n\nplot(elev)\n\n\n\n\n\n\n\n# set all cells to NA based on condition\nelev[elev &lt; 20] &lt;- NA\n\nplot(elev)\n\n\n\n\n\n\n\n# let's practice combining (or unioning) geometries\n# read in shapefile of the counties of North Carolina\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\nplot(nc[\"AREA\"])\n\n\n\n\n\n\n\n# combines all geometries without resolving borders\nnc_combine &lt;- st_combine(nc)\n\nplot(nc_combine)\n\n\n\n\n\n\n\n# finds the union of all geometries\nnc_union &lt;- st_union(nc)\n\nplot(nc_union)\n\n\n\n\n\n\n\n# let's exploring removing geometries\n# pick a few counties to remove\ncounties &lt;- nc %&gt;%\n  filter(NAME %in% c(\"Ashe\", \"Alleghany\", \"Surry\")) %&gt;%\n  st_union()\n\n# plot counties on top of the unioned version of NC\nggplot() +\n  geom_sf(data = nc_union, fill = \"grey\", color = \"transparent\") +\n  geom_sf(data = counties, fill = \"black\", color = \"transparent\")\n\n\n\n\n\n\n\n# create a new geometry that is the difference between the unioned version of NC and the counties\n\nnc_difference &lt;- st_difference(nc_union, counties)\nnc_disjoint &lt;- st_disjoint(nc_union, counties)\nnc_intersection &lt;- st_intersection(nc_union, counties)\n\n# plot the difference on top of the unioned version of NC\n# counties should be missing!\nggplot() + \n  geom_sf(data = nc_difference, fill = \"grey\", color = \"transparent\")\n\n\n\n\n\n\n\nggplot() + \n  geom_sf(data = nc_intersection, fill = \"grey\", color = \"transparent\")"
  },
  {
    "objectID": "course-materials/discussions/week7-discussion.html#prerequsites",
    "href": "course-materials/discussions/week7-discussion.html#prerequsites",
    "title": "Discssuion Week 7",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ─────────────────── tidyverse 2.0.0.9000 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(terra)\n\nterra 1.7.78\n\nAttaching package: 'terra'\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\n# practice filtering a raster\nelev &lt;- rast(nrows = 6, ncols = 6, resolution = 0.5,\n             xmin = -1.5, xmax = 1.5, ymin = -1.5, ymax = 1.5,\n             vals = 1:36)\n\nplot(elev)\n\n\n\n\n\n\n\n# set all cells to NA based on condition\nelev[elev &lt; 20] &lt;- NA\n\nplot(elev)\n\n\n\n\n\n\n\n# let's practice combining (or unioning) geometries\n# read in shapefile of the counties of North Carolina\nnc = st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\nplot(nc[\"AREA\"])\n\n\n\n\n\n\n\n# combines all geometries without resolving borders\nnc_combine &lt;- st_combine(nc)\n\nplot(nc_combine)\n\n\n\n\n\n\n\n# finds the union of all geometries\nnc_union &lt;- st_union(nc)\n\nplot(nc_union)\n\n\n\n\n\n\n\n# let's exploring removing geometries\n# pick a few counties to remove\ncounties &lt;- nc %&gt;%\n  filter(NAME %in% c(\"Ashe\", \"Alleghany\", \"Surry\")) %&gt;%\n  st_union()\n\n# plot counties on top of the unioned version of NC\nggplot() +\n  geom_sf(data = nc_union, fill = \"grey\", color = \"transparent\") +\n  geom_sf(data = counties, fill = \"black\", color = \"transparent\")\n\n\n\n\n\n\n\n# create a new geometry that is the difference between the unioned version of NC and the counties\n\nnc_difference &lt;- st_difference(nc_union, counties)\nnc_disjoint &lt;- st_disjoint(nc_union, counties)\nnc_intersection &lt;- st_intersection(nc_union, counties)\n\n# plot the difference on top of the unioned version of NC\n# counties should be missing!\nggplot() + \n  geom_sf(data = nc_difference, fill = \"grey\", color = \"transparent\")\n\n\n\n\n\n\n\nggplot() + \n  geom_sf(data = nc_intersection, fill = \"grey\", color = \"transparent\")"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html",
    "href": "course-materials/discussions/week4-discussion.html",
    "title": "Discssuion Week 4: Raster Operation Basics",
    "section": "",
    "text": "The following exercises are modified from Chapters 3, 4, 5 of Geocomputation with R by Rovin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#prerequisites",
    "href": "course-materials/discussions/week4-discussion.html#prerequisites",
    "title": "Discssuion Week 4: Raster Operation Basics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(geodata)\nlibrary(spDataLarge)\nlibrary(tidyverse)\n\n\ndem &lt;- rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nperu_dem &lt;- geodata::elevation_30s(country = \"Peru\", path = \".\", mask = FALSE)\nsrtm &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#exercise-1",
    "href": "course-materials/discussions/week4-discussion.html#exercise-1",
    "title": "Discssuion Week 4: Raster Operation Basics",
    "section": "Exercise 1",
    "text": "Exercise 1\nPlot the histogram and the boxplot of the dem.tif file from the spDataLarge package:\n\nhist(dem)"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#exercise-2",
    "href": "course-materials/discussions/week4-discussion.html#exercise-2",
    "title": "Discssuion Week 4: Raster Operation Basics",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s manipulate rasters by eclassifying the elevation in three classes:\n\nLow (less than 300)\nMedium\nHigh (greater than 500)\n\n\nplot(dem)\n\n\n\n\n\n\n\nrcl &lt;- matrix(c(-Inf, 300, 0, 300, 500, 1, 500, Inf, 2), ncol = 3, byrow = TRUE)\n\ndem_rcl &lt;- terra::classify(dem, rcl = rcl)\n\nlevels(dem_rcl) &lt;- tibble::tibble(id = 0:2, cats = c(\"low\", \"medium\", \"high\"))\n\nplot(dem_rcl)\n\n\n\n\n\n\n\n\nAnd compute the mean elevation for each altitudinal class:\n\nelevation_mean &lt;- terra::zonal(dem, dem_rcl, fun = \"mean\")\nelevation_mean\n\n    cats      dem\n1    low 274.3910\n2 medium 392.0486\n3   high 765.2197\n\n\nNext, let’s calculate the Normalized Difference Water Index (NDWI) of a Landsat image:\n\nNDWI = (green - NIR)/(green + NIR)\n\n\nndwi_fun &lt;- function(green, nir){\n    (green - nir) / (green + nir)\n}\n\nndwi_rast &lt;- terra::lapp(landsat[[c(2, 4)]], fun = ndwi_fun)\nplot(ndwi_rast)\n\n\n\n\n\n\n\n\nCalculate a correlation between NDVI and NDWI for this area:\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\nndvi_rast &lt;- terra::lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n\n\ncombine &lt;- c(ndvi_rast, ndwi_rast)\n\nplot(combine)\n\n\n\n\n\n\n\nterra::layerCor(combine, fun = cor)\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.9132838\n[2,] -0.9132838  1.0000000\n\n\nUse terra::distance() to compute distances from all cells of Peru to it’s nearest coastline. According to the documentation, terra::distance() will calculate distance for all cells that are NA to the nearest cell that are not NA.\n\nperu_dem &lt;- terra::aggregate(peru_dem, fact = 20)\nplot(peru_dem)\n\n\n\n\n\n\n\nwater_mask &lt;- is.na(peru_dem)\nwater_mask[water_mask == 0] &lt;- NA\nplot(water_mask)\n\n\n\n\n\n\n\ndistance_to_coast &lt;- terra::distance(water_mask)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\ndistance_to_coast_km &lt;- distance_to_coast / 1000\n\nplot(distance_to_coast_km, main = \"Distance to the coast (km)\")\n\n\n\n\n\n\n\n\nTry to modify the approach used in the above exercise by weighting the distance raster with the elevation raster:\n\nEvery 100 altitudinal meters should increase the distance to the coast by 10 km\nNext, compute and visualize the difference between the raster created using the Euclidean distance (E7) and the raster weighted by elevation\n\n\ndistance_to_coast_km2 &lt;- distance_to_coast_km + ((peru_dem /100) * 10)\nplot(distance_to_coast_km2)"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#exercise-3",
    "href": "course-materials/discussions/week4-discussion.html#exercise-3",
    "title": "Discssuion Week 4: Raster Operation Basics",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet’s try some geometry operations with rasters!\nThe srtm raster has a resolution of 0.00083 by 0.00083 degrees.\nYour task: change its resolution to 0.01 by 0.01 degrees using all of the method available in the terra package. And plot the results!\n\nplot(srtm)\n\n\n\n\n\n\n\nrast_template &lt;- terra::rast(terra::ext(srtm), res = 0.01)\n\nsrtm_resampl1 &lt;- terra::resample(srtm, y = rast_template, method = \"bilinear\")\nsrtm_resampl2 &lt;- terra::resample(srtm, y = rast_template, method = \"near\")\nsrtm_resampl3 &lt;- terra::resample(srtm, y = rast_template, method = \"cubic\")\nsrtm_resampl4 &lt;- terra::resample(srtm, y = rast_template, method = \"cubicspline\")\nsrtm_resampl5 &lt;- terra::resample(srtm, y = rast_template, method = \"lanczos\")\n\n\nsrtm_resampl_all &lt;- c(srtm_resampl1, srtm_resampl2, srtm_resampl3, srtm_resampl4, srtm_resampl5)\nplot(srtm_resampl_all)"
  },
  {
    "objectID": "course-materials/discussions/week10-discussion.html",
    "href": "course-materials/discussions/week10-discussion.html",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggspatial)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the ghm.tif files. ghm.tif Transform both to EPSG:4326.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")"
  },
  {
    "objectID": "course-materials/discussions/week10-discussion.html#prerequsites",
    "href": "course-materials/discussions/week10-discussion.html#prerequsites",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggspatial)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the ghm.tif files. ghm.tif Transform both to EPSG:4326.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")"
  },
  {
    "objectID": "course-materials/discussions/week10-discussion.html#exercises",
    "href": "course-materials/discussions/week10-discussion.html#exercises",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s make nice plots of the California Protected areas by access level\n\n\np1 &lt;- ggplot2::ggplot(data = cpad_super) +\n  geom_sf(aes(color = access_typ, fill = access_typ)) +\n  theme_bw() +\n  labs(\n    color = \"Access Type\",\n    fill = \"Access Type\"\n  ) +\n  annotation_scale(plot_unit = \"km\") +\n  annotation_north_arrow(\n    location = \"tr\",\n    pad_x = unit(0.2, \"in\"),\n    pad_y = unit(0.2, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill =\n        c(\"grey40\", \"white\"),\n      line_col = \"grey20\"\n    )\n  ) +\n  coord_sf() +\n  scale_color_viridis_d() +\n  scale_fill_viridis_d()\n\n\np1 +\n  facet_wrap(~access_typ) +\n  theme(strip.background = element_rect(fill = \"transparent\"))\n\n\n\n\n\n\n\n\n\nLet’s try plotting the ghm layers nicely too!\n\n\nggplot() +\n    geom_stars(data = st_as_stars(ghm)) +\n  coord_equal() +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"\",\n    fill = \"Global Human Modification\"\n  ) +\n  scale_fill_viridis_c() +\n  annotation_scale(plot_unit = \"km\") +\n  annotation_north_arrow(\n    location = \"tr\",\n    pad_x = unit(0.2, \"in\"),\n    pad_y = unit(0.2, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill =\n        c(\"grey40\", \"white\"),\n      line_col = \"grey20\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nCreate a function to take 2 data sets (1 polygon and 1 raster) and create a boxplot of the values based on a specific layer\n\n\nsummary_boxplot &lt;- function(polygon, raster, my_layer, my_label) {\n  \n  # rasterize polygon by layer\n  id_rast &lt;- rasterize(polygon, raster, field = \"suid_nma\")\n  \n  #do mean zonal statistics\n  zonal_layer &lt;- zonal(raster, id_rast, fun = \"mean\", na.rm = TRUE)\n  \n  #join with polygon database\n  poly_join &lt;- full_join(polygon, zonal_layer) %&gt;% \n    select(suid_nma, gHM, my_layer)\n  \n  #create boxplot based on your layer\n  p1 &lt;- ggplot(poly_join) +\n    geom_boxplot(aes(gHM, .data[[my_layer]])) +\n    theme_bw() +\n    labs(x = \"Human Modification Index\", \n         y = my_label)\n  \n  return(p1)\n}\n\n\nLet’s select some layers and use our new function!\n\n\nnames(cpad_super)\n\n [1] \"suid_nma\"   \"access_typ\" \"park_name\"  \"park_url\"   \"mng_ag_id\" \n [6] \"mng_agncy\"  \"mng_ag_lev\" \"mng_ag_typ\" \"agncy_web\"  \"layer\"     \n[11] \"acres\"      \"label_name\" \"yr_est\"     \"gap1_acres\" \"gap2_acres\"\n[16] \"gap3_acres\" \"gap4_acres\" \"gap_tot_ac\" \"shape_leng\" \"shape_area\"\n[21] \"geometry\"   \"ID\"        \n\n\n\naccess &lt;- summary_boxplot(cpad_super, ghm, \"access_typ\", \"Access Type\")\n\naccess\n\n\n\n\n\n\n\n\n\nlayer &lt;- summary_boxplot(cpad_super, ghm, \"layer\", \"Management Agency Type\")\n\nlayer"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html",
    "href": "course-materials/discussions/week1-discussion.html",
    "title": "Discssuion Week 1: Making Maps",
    "section": "",
    "text": "The following exercises are modified from Chapter 9 of Geocomputation with R by Rovin Lovelace"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#prerequisites",
    "href": "course-materials/discussions/week1-discussion.html#prerequisites",
    "title": "Discssuion Week 1: Making Maps",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(raster)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tidyverse)\nlibrary(ggspatial)\nlibrary(patchwork)"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#exercise-1",
    "href": "course-materials/discussions/week1-discussion.html#exercise-1",
    "title": "Discssuion Week 1: Making Maps",
    "section": "Exercise 1",
    "text": "Exercise 1\nThese exercises rely on a new data object based on the world and worldbank_df datasets from the spData package.\n\nafrica &lt;- world %&gt;% \n  dplyr::filter(continent == \"Africa\", !is.na(iso_a2)) %&gt;% \n  dplyr::left_join(worldbank_df, by = \"iso_a2\") %&gt;% \n  dplyr::select(name, subregion, gdpPercap, HDI, pop_growth) %&gt;% \n  sf::st_transform(\"+proj=aea +lat_1=20 +lat_2=-23 +lat_0=0 +lon_0=25\")\n\n\nMap 1\nCreate a map showing the geographic distribution of the Human Development Index (HDI) across Africa.\n\nm1 &lt;- ggplot2::ggplot(africa) +\n  ggplot2::geom_sf(aes(fill = HDI)) +\n  ggplot2::theme_bw() \n\n\nm1\n\n\n\n\n\n\n\n\n\n\nMap 2\nNow, let’s update the map so the legend has three bins: - High where HDI is above 0.7 - Medium where HDI is between 0.55 and 0.7 - Low where HDI is below 0.55\n\nm2 &lt;- m1 + \n  ggplot2::scale_fill_binned(breaks = c(.55, 0.7))\n\n\nm2\n\n\n\n\n\n\n\n\n\n\nMap 3\nNext, let’s represent Africa’s sub regions on the map: - Change the color palette - Change the legend title\n\nm3 &lt;- ggplot2::ggplot(africa) +\n  ggplot2::geom_sf(aes(fill = subregion)) +\n  ggplot2::theme_bw() +\n  ggplot2::scale_fill_viridis_d() +\n  ggplot2::labs(fill = \"Subregion\")\n\n\nm3\n\n\n\n\n\n\n\n\nLet’s use patchwork to combine this map with the previous map into a single plot:\n\nm2 + m3"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#exercise-2",
    "href": "course-materials/discussions/week1-discussion.html#exercise-2",
    "title": "Discssuion Week 1: Making Maps",
    "section": "Exercise 2",
    "text": "Exercise 2\nHere, we will use the zion and nlcd datasets from the spDataLarge package:\n\nzion &lt;- sf::st_read((system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")))\n\n\n# For spDataLarge version 2.0.X\ndata(nlcd, package = \"spDataLarge\")\nforce(nlcd)\n\n\n# For spDataLarge version 2.1.1\nnlcd &lt;- terra::rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\n\nFirst, we need to convert the raster into a dataframe\n\nnlcd_df &lt;- as.data.frame(nlcd, xy = TRUE)\n\n\nMap 4\nLet’s create a land cover map of Zion National Park:\n\nChange the default colors to match your perception of land cover categories\n\nMove the map legend outside of the map to improve readability\nAdd a scale bar and north arrow\nChange the position of the scale bar and north arrow to improve the maps aesthetics\nAdd the park boundaries on top of the land cover map\n\n\nm4 &lt;- ggplot2::ggplot() +\n  ggplot2::geom_raster(data = nlcd_df,\n                       aes(x = x, y = y, fill = levels)) + # layer_levels in spDataLarge 2.0.9\n  ggplot2::geom_sf(data = zion, aes(fill = NA), alpha = 0.2, linewidth = 1, color = \"black\") +\n  ggplot2::labs(fill = \"Land Cover Type\") +\n  ggplot2::theme_bw() +\n  ggplot2::scale_fill_manual(values = c(\"tan\", \"lightgreen\", \"darkgrey\", \"darkgreen\", \"green\", \"purple\", \"blue\", \"brown\")) +\n  ggspatial::annotation_scale(plot_unit = \"km\") +\n  ggspatial::annotation_north_arrow(location = \"br\",\n                         pad_x = unit(0.2, \"in\"),\n                         pad_y = unit(0.2, \"in\"),\n                         style = ggspatial::north_arrow_nautical(fill = c(\"grey40\", \"white\"),\n                                                                 line_col = \"grey20\"))\n\n\nm4\n\n\n\n\n\n\n\n\n\n\nMap 5\nNow, let’s add an inset of Zion’s location in the context of the state of Utah. Hint: an object representing Utah can be subset from the us_states dataset.\n\nstates &lt;- us_states %&gt;% \n  janitor::clean_names() %&gt;% \n  dplyr::filter(name == \"Utah\")\n\n\nm5 &lt;- ggplot2::ggplot() +\n  ggplot2::geom_sf(data = states) + \n  ggplot2::geom_sf(data = zion) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(axis.text = element_blank(),\n                 axis.ticks = element_blank())\n\n\nm5\n\n\n\n\n\n\n\n\nLet’s use patchwork again to combine elements:\n\nm4 + patchwork::inset_element(m5, 0.7, 0.7, 1, 1) + patchwork::plot_annotation(tag_levels = \"A\")"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html",
    "href": "course-materials/discussions/week3-discussion.html",
    "title": "Discssuion Week 3: Assignment 2",
    "section": "",
    "text": "The following exercises are modified from Chapters 3, 4, 5 of Geocomputation with R by Rovin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#prerequisites",
    "href": "course-materials/discussions/week3-discussion.html#prerequisites",
    "title": "Discssuion Week 3: Assignment 2",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(spData)\nlibrary(tidyverse)\n\nLet’s load all necessary data frames:\n\nus_states &lt;- us_states %&gt;% \n  janitor::clean_names()\n\nus_states_df &lt;- us_states_df %&gt;% \n  janitor::clean_names()\n\nnz_height &lt;- nz_height %&gt;%  \n  janitor::clean_names()\n\nnz &lt;- nz %&gt;%  \n  janitor::clean_names()"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#united-states",
    "href": "course-materials/discussions/week3-discussion.html#united-states",
    "title": "Discssuion Week 3: Assignment 2",
    "section": "United States",
    "text": "United States\nLet’s find the states that belong to the West region:\n\nwest_states &lt;- dplyr::filter(us_states, region == \"West\")\ndplyr::pull(west_states, name)\n\n [1] \"Arizona\"    \"Colorado\"   \"Idaho\"      \"Montana\"    \"Nevada\"    \n [6] \"California\" \"New Mexico\" \"Oregon\"     \"Utah\"       \"Washington\"\n[11] \"Wyoming\"   \n\n\nAnd from these states, the states with more than 5,000,000 residents in 2015:\n\nus_states %&gt;% \n  dplyr::filter(total_pop_15 &gt; 5000000) %&gt;% \n  dplyr::pull(name)\n\n [1] \"Arizona\"        \"Colorado\"       \"Florida\"        \"Georgia\"       \n [5] \"Indiana\"        \"Massachusetts\"  \"Minnesota\"      \"Missouri\"      \n [9] \"New Jersey\"     \"New York\"       \"Pennsylvania\"   \"Texas\"         \n[13] \"California\"     \"Illinois\"       \"Maryland\"       \"Michigan\"      \n[17] \"North Carolina\" \"Ohio\"           \"Tennessee\"      \"Virginia\"      \n[21] \"Washington\"     \"Wisconsin\"     \n\n\nNext, what was the total population of the US in 2015?\n\nus_states %&gt;% \n  dplyr::summarise(pop_2015 = sum(total_pop_15, na.rm = TRUE)) %&gt;% \n  dplyr::pull(pop_2015)\n\n[1] 314375347\n\n\nLet’s now create a new variable named us_states_stats by adding variables from us_states_df to us_states:\n\nclass(us_states_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(us_states_df)\n\n[1] \"state\"            \"median_income_10\" \"median_income_15\" \"poverty_level_10\"\n[5] \"poverty_level_15\"\n\n\n\nus_states_df &lt;- us_states_df %&gt;% \n  dplyr::rename(name = state)\n\nus_states_stats &lt;- dplyr::full_join(us_states, us_states_df)\n\nWhat is the class of the new object?\n\n“sf”\n“data.frame”\n\nNow, calculate the percent change in population density between 2010 and 2015 in each state:\n\npop_change &lt;- mutate(us_states_stats, percent_change_pop = \n                       (total_pop_15 - total_pop_10)/total_pop_10 * 100)\n\nIn how many states did population density decrease?\n\npop_change %&gt;% \n  dplyr::filter(percent_change_pop &lt; 0) %&gt;% \n  nrow()\n\n[1] 2\n\n\nCreate a new object representing all of the states the geographically intersect with Colorado.\n\nHint: use the us_states dataset. The most concise way to do this is with the subsetting method “[”.\n\n\ncolorado &lt;- us_states %&gt;% \n  dplyr::filter(name == \"Colorado\")\n\ncolorado_intersect &lt;- us_states[colorado, , op = st_intersects]\n\nCreate another object representing all the objects that touch (have a shared boundary with) Colorado and plot the result.\n\nHint: remember you can use the argument op = st_intersects and other spatial relations during spatial subsetting operations in base R.\n\n\ncolorado_touches &lt;- us_states[colorado, , op = st_touches]\n\n\nplot(colorado_intersect[\"name\"])\n\n\n\n\n\n\n\nplot(colorado_touches[\"name\"])"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#new-zealand",
    "href": "course-materials/discussions/week3-discussion.html#new-zealand",
    "title": "Discssuion Week 3: Assignment 2",
    "section": "New Zealand",
    "text": "New Zealand\nHow many of New Zealand’s high points are in the Canterbury region?\n\nnames(nz)\n\n[1] \"name\"          \"island\"        \"land_area\"     \"population\"   \n[5] \"median_income\" \"sex_ratio\"     \"geom\"         \n\nnames(nz_height)\n\n[1] \"t50_fid\"   \"elevation\" \"geometry\" \n\ncanterbury &lt;- nz  %&gt;% \n  dplyr::filter(name == \"Canterbury\")\n\ncanterbury_height &lt;- nz_height[canterbury, ]\nnrow(canterbury_height)\n\n[1] 70\n\n\nThis could also be achieved by spatial joining then filter\n\nsf::st_join(nz, nz_height) %&gt;% \n  dplyr::filter(name == \"Canterbury\") %&gt;% \n  nrow()\n\n[1] 70\n\n\nWhich region has the second highest number of nz_height points? And how many does it have?\n\nnz_join &lt;- sf::st_join(nz, nz_height) \n\nnz_join %&gt;% \n  dplyr::group_by(name) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  dplyr::arrange(dplyr::desc(n))\n\nSimple feature collection with 16 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1090144 ymin: 4748537 xmax: 2089533 ymax: 6191874\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n# A tibble: 16 × 3\n   name                  n                                                  geom\n   &lt;chr&gt;             &lt;int&gt;                                    &lt;MULTIPOLYGON [m]&gt;\n 1 Canterbury           70 (((1686902 5353233, 1679996 5344809, 1673699 5328829…\n 2 West Coast           22 (((1557042 5319333, 1554239 5309440, 1546356 5306561…\n 3 Waikato               3 (((1860345 5859665, 1857808 5853929, 1850511 5849040…\n 4 Manawatu-Wanganui     2 (((1866732 5664323, 1868949 5654440, 1865829 5649938…\n 5 Otago                 2 (((1335205 5126878, 1336956 5118634, 1325903 5102723…\n 6 Auckland              1 (((1803822 5900006, 1791443 5900571, 1790082 5883500…\n 7 Bay of Plenty         1 (((2049387 5832785, 2051016 5826423, 2040276 5825884…\n 8 Gisborne              1 (((2024489 5674920, 2019037 5677334, 2016277 5683982…\n 9 Hawke's Bay           1 (((2024489 5674920, 2024126 5663676, 2032576 5659653…\n10 Marlborough           1 (((1686902 5353233, 1679241 5359478, 1667754 5357340…\n11 Nelson                1 (((1624866 5417556, 1616643 5424521, 1618569 5428691…\n12 Northland             1 (((1745493 6001802, 1740539 5995066, 1733165 5989714…\n13 Southland             1 (((1229078 5062352, 1221427 5056736, 1217551 5038521…\n14 Taranaki              1 (((1740438 5714538, 1743867 5711520, 1755759 5711464…\n15 Tasman                1 (((1616643 5424521, 1624866 5417556, 1620946 5409416…\n16 Wellington            1 (((1881590 5489434, 1875693 5479987, 1871588 5466618…\n\n\nGenerate simplified versions of the nz dataset. Experiment with different values of keep (ranging from 0.5 to 0.00005) for ms_simplify() and dTolerance (from 100 to 100,000) for st_simplify().\n\nnz_simp1 &lt;- st_simplify(nz, dTolerance = 100)\nnz_simp4 &lt;- st_simplify(nz, dTolerance = 100000)\nnz_simp2 &lt;- st_simplify(nz, dTolerance = 1000)\nnz_simp3 &lt;- st_simplify(nz, dTolerance = 10000)\n\nMap the results to show how the simplification changes as you change values.\n\nplot(nz_simp1[\"name\"])\n\n\n\n\n\n\n\nplot(nz_simp2[\"name\"])\n\n\n\n\n\n\n\nplot(nz_simp3[\"name\"])\n\n\n\n\n\n\n\nplot(nz_simp4[\"name\"])\n\n\n\n\n\n\n\n\nHow many points from the nz_height dataset are within 100km of the Canterbury region?\n\ncant_buffer &lt;- sf::st_buffer(canterbury, dist = 100000)\n\ncant_buffer_height &lt;- nz_height[cant_buffer, ]\nnrow(cant_buffer_height)\n\n[1] 95\n\n\nFind the geographic centroid of the country of New Zealand. How far is it from the geographic centroid of Canterbury?\n\nnz_agg &lt;- nz %&gt;% \n  dplyr::summarize(population = sum(population))\n\nnz_centroid &lt;- sf::st_centroid(nz_agg)\n\ncant_centroid &lt;- sf::st_centroid(canterbury) \n\nsf::st_distance(nz_centroid, cant_centroid)\n\nUnits: [m]\n         [,1]\n[1,] 234192.6"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html",
    "href": "course-materials/discussions/week6-discussion.html",
    "title": "Discssuion Week 6: Bringing Vector and Raster Together",
    "section": "",
    "text": "The following exercises are modified from Chapter 6 of Geocomputation with R by Rovin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#prerequisites",
    "href": "course-materials/discussions/week6-discussion.html#prerequisites",
    "title": "Discssuion Week 6: Bringing Vector and Raster Together",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(tidyverse)\n\n\nzion_points &lt;- read_sf(system.file(\"vector/zion_points.gpkg\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\nch &lt;- sf::st_combine(zion_points) %&gt;%\n  sf::st_convex_hull() %&gt;% \n  sf::st_as_sf()\n\ngrain &lt;- terra::rast(system.file(\"raster/grain.tif\", package = \"spData\"))\n\n\nplot(srtm)\nplot(st_geometry(zion_points), add = TRUE)\nplot(ch, add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#exercises",
    "href": "course-materials/discussions/week6-discussion.html#exercises",
    "title": "Discssuion Week 6: Bringing Vector and Raster Together",
    "section": "Exercises",
    "text": "Exercises\n\nCrop the srtm raster using (1) the zion_points dataset and (2) the ch dataset. Are there any differences in the output maps? Next, mask srtm using these two datasets. Can you see any difference now?\n\n\nsrtm_crop1 &lt;- terra::crop(srtm, zion_points)\nsrtm_crop2 &lt;- terra::crop(srtm, ch)\nplot(srtm_crop1)\nplot(srtm_crop2)\n\n\n\n\n\n\n\nsrtm_mask1 &lt;- terra::mask(srtm, zion_points)\nsrtm_mask2 &lt;- terra::mask(srtm, ch)\nplot(srtm_mask1)\n\n\n\n\n\n\n\nplot(srtm_mask2)\n\n\n\n\n\n\n\n\n\nSubset points higher than 3100 meters in New Zealand (the nz_height object) and create a template raster with a resolution of 3 km for the extent of the new point dataset. Using these two new objects:\n\n\nnz_height3100 &lt;- nz_height %&gt;% \n  filter(elevation &gt; 3100)\n\nnz_template &lt;- terra::rast(ext(nz_height3100), resolution = 3000, crs = crs(nz_height3100))\n\n2a. Count numbers of the highest points in each grid cell.\n\nnz_raster &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = \"length\")\nplot(nz_raster)\nplot(st_geometry(nz_height3100), add = TRUE)\n\n\n\n\n\n\n\n\n2b. Find the maximum elevation in each grid cell.\n\nnz_raster2 = rasterize(nz_height3100, nz_template, \n                       field = \"elevation\", fun = max)\nplot(nz_raster2)\nplot(st_geometry(nz_height3100), add = TRUE)\n\n\n\n\n\n\n\n\n\nAggregate the raster counting high points in New Zealand (created in the previous exercise), reduce its geographic resolution by half (so cells are 6 by 6 km) and plot the result. Resample the lower resolution raster back to the original resolution of 3 km.\n\n\nnz_raster_low = raster::aggregate(nz_raster, fact = 2, fun = sum, na.rm = TRUE)\n\nnz_resample = resample(nz_raster_low, nz_raster)\nplot(nz_raster_low)\n\n\n\n\n\n\n\nplot(nz_resample) # the results are spread over a greater area and there are border issues\n\n\n\n\n\n\n\nplot(nz_raster)\n\n\n\n\n\n\n\n\n\nPolygonize the grain dataset and filter all squares representing clay.\n\n\ngrain_poly = as.polygons(grain) %&gt;% \n  st_as_sf()\n\nplot(grain)\n\n\n\n\n\n\n\nplot(grain_poly)\n\n\n\n\n\n\n\nclay = grain_poly %&gt;% \n  filter(grain == \"clay\")\nplot(clay)"
  },
  {
    "objectID": "course-materials/discussions/week9-discussion.html",
    "href": "course-materials/discussions/week9-discussion.html",
    "title": "Discssuion Week 9: Functions for Assignment 4",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(kableExtra)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the gHM_masked.tif files.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")\n\n\nplot(cpad_super[\"suid_nma\"])\n\n\n\n\n\n\n\nplot(ghm)"
  },
  {
    "objectID": "course-materials/discussions/week9-discussion.html#prerequsites",
    "href": "course-materials/discussions/week9-discussion.html#prerequsites",
    "title": "Discssuion Week 9: Functions for Assignment 4",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(kableExtra)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the gHM_masked.tif files.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")\n\n\nplot(cpad_super[\"suid_nma\"])\n\n\n\n\n\n\n\nplot(ghm)"
  },
  {
    "objectID": "course-materials/discussions/week9-discussion.html#exercises",
    "href": "course-materials/discussions/week9-discussion.html#exercises",
    "title": "Discssuion Week 9: Functions for Assignment 4",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s rasterize the cpad dataset a few times! This is necessary to do zonal statistics!\n\n\nid_rast = rasterize(cpad_super, ghm, field = \"suid_nma\") #rasterize by PA id\nplot(id_rast)\n\n\n\n\n\n\n\naccess_rast = rasterize(cpad_super, ghm, field = \"access_typ\") #rasterize by PA access type\nplot(access_rast)\n\n\n\n\n\n\n\n\n\nLet’s say you’re interested in finding protected areas with intermediate levels of human modification. How could you do this?\n\n\nmasking and zonal statistics\n\n\nrcl = matrix(c(-Inf, 0.4, NA, #create reclassification matrix \n                 0.4, 0.6, 1, \n                 0.6, Inf, NA), ncol = 3, byrow = TRUE)\n\nmod_dis &lt;- classify(ghm, rcl = rcl) #reclassify ghm\n\nplot(mod_dis) #plot to check\n\n\n\n\n\n\n\n\nSay you want to make this more complicated, and you want two groups?\n\nrcl2 = matrix(c(-Inf, 0.4, NA, #create reclassification matrix \n                 0.4, 0.5, 1,\n                 0.5, 0.6, 2,\n                 0.6, Inf, NA), ncol = 3, byrow = TRUE)\n\nmod_dis2 &lt;- classify(ghm, rcl = rcl2) #reclassify ghm\n\nplot(mod_dis2) #plot to check\n\n\n\n\n\n\n\n\nOr you just want to group them generally!\n\nrcl3 = matrix(c(-Inf, 0.25, 1, #create reclassification matrix \n                 0.25, 0.5, 2,\n                 0.5, 0.75, 3,\n                 0.75, Inf, 4), ncol = 3, byrow = TRUE)\n\nmod_dis3 &lt;- classify(ghm, rcl = rcl3) #reclassify ghm\n\nplot(mod_dis3) #plot to check\n\n\n\n\n\n\n\n\nNow let’s use zonal statistics to get a mean\n\nghm_zonal &lt;- zonal(mod_dis, id_rast, fun = \"mean\", na.rm = TRUE) %&gt;% #calculate the \"mean\" - this is just an indicator of if 1 is present or not\n  filter(!is.na(gHM)) %&gt;% #remove NAs \n  mutate(method = \"Method 1\") #add a method for comparison\n\nhead(ghm_zonal)\n\n  suid_nma gHM   method\n1      295   1 Method 1\n2      442   1 Method 1\n3      744   1 Method 1\n4      760   1 Method 1\n5      835   1 Method 1\n6     1372   1 Method 1\n\n\n\ncombining and filtering\n\n\ncpad_ghm_values &lt;- terra::extract(x = ghm, y = cpad_super) #extract all ghm values from cpad\n\nSummarize the mean of human modification within each protected area\n\ncpad_ghm_summary &lt;- cpad_ghm_values %&gt;%\n  group_by(ID) %&gt;%\n  summarize(gHM_mean = mean(gHM)) #calculate mean by protected area\n\nJoin this summary with the protected area database. Plot these values\n\ncpad_ghm &lt;- full_join(cpad_super, cpad_ghm_summary) #join summary back with\n\nNow we can filter!\n\ncpad_ghm_sub &lt;- cpad_ghm %&gt;% \n  st_drop_geometry() %&gt;%  #remove geometry for speed\n  filter(gHM_mean &lt; 0.6 & gHM_mean &gt; 0.4) %&gt;% #filter \n  distinct() %&gt;% \n  select(suid_nma, gHM_mean) %&gt;% \n  mutate(method = \"Method 2\") #set method for comparison\n\n\nThese do give slightly different results however… can you identify the difference?\n\n\nmethod_sum &lt;- full_join(ghm_zonal, cpad_ghm_sub) #join two methods\n\nmethod_count &lt;- method_sum %&gt;% \n  group_by(suid_nma) %&gt;% \n  summarize(count = as.factor(n())) %&gt;%  #deterine which PAs are selected in both\n  full_join(method_sum) #rejoin with full method data\n\n\nggplot(method_count) +\n  geom_jitter(aes(method, suid_nma, color = count, alpha = count), height = 0) +\n  theme_bw() +\n  scale_color_manual(values = c(\"red\", \"darkgrey\")) +\n  scale_alpha_manual(values = c(1, 0.5))\n\n\n\n\n\n\n\n\n\nLet’s use zonal statistics to summarize human modification by access type!\n\n\naccess_zonal &lt;- zonal(ghm, access_rast, fun = \"mean\", na.rm = TRUE) #get mean of ghm within access level types\n\naccess_zonal %&gt;% #create nice table\n  kable(digits = 2, col.names = c(\"Access Level\", \"Human Modification Index\")) %&gt;% #adjust digits and column names\n  kable_minimal()\n\n\n\n\nAccess Level\nHuman Modification Index\n\n\n\n\nNo Public Access\n0.21\n\n\nOpen Access\n0.06\n\n\nRestricted Access\n0.22\n\n\nUnknown Access\n0.27"
  },
  {
    "objectID": "course-materials/labs/week10.html",
    "href": "course-materials/labs/week10.html",
    "title": "Week 10: Lab",
    "section": "",
    "text": "The National Science Foundation’s National Ecological Observatory Network (NEON) collects standardized, open-access ecological data at 81 freshwater and terrestrial field sites across the country. In addition to an amazing array of on-the-ground surveys, they also periodically collect Lidar data at the sites. All data is publicly available through the NEON Data Portal.\nFor this exercise, we will imagine that we are interested in studying canopy structure (tree height) at the San Joaquin Experimental Range in California. We’re interested in figuring out if we can rely on the Lidar data NEON is collecting by comparing tree height estimates to on-the-ground field surveys. If the estimates between the two methods are similar, we could save ourselves a lot of time and effort measuring trees!\nThis lab is based on materials developed by Edmund Hart, Leah Wasser, and Donal O’Leary for NEON."
  },
  {
    "objectID": "course-materials/labs/week10.html#task",
    "href": "course-materials/labs/week10.html#task",
    "title": "Week 10: Lab",
    "section": "Task",
    "text": "Task\nTo estimate tree height from Lidar data, we will create a canopy height model (CHM) from Lidar-derived digital surface and terrain models. We will then extract tree height estimates within the locations of on-the-ground surveys and compare Lidar estimates to measured tree height in each plot."
  },
  {
    "objectID": "course-materials/labs/week10.html#data",
    "href": "course-materials/labs/week10.html#data",
    "title": "Week 10: Lab",
    "section": "Data",
    "text": "Data\nLidar data\n\nSJER2013_DSM.tif, digital surface model (DSM)\nSJER2013_DTM.tif, digital terrain model (DTM)\nDSMs represent the elevation of the top of all objects\nDTMs represent the elevation of the ground (or terrain)\n\nVegetation plot geometries\n\nSJERPlotCentroids_Buffer.shp\nContains locations of vegetation surveys\nPolygons representing 20m buffer around plot centroids\n\nVegetation surveys\n\nD17_2013_vegStr.csv\nMeasurements for individual trees in each plot\nMetadata available in D17_2013_vegStr_metadata_desc.csv"
  },
  {
    "objectID": "course-materials/labs/week10.html#prerequisites",
    "href": "course-materials/labs/week10.html#prerequisites",
    "title": "Week 10: Lab",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s load all necessary packages:\n\nlibrary(terra)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(here)"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-lidar-data",
    "href": "course-materials/labs/week10.html#load-lidar-data",
    "title": "Week 10: Lab",
    "section": "Load Lidar data",
    "text": "Load Lidar data\n\n# digital surface model (DSM)\ndsm &lt;- rast(here::here(\"course-materials\", \"data\", \"week10\", \"SJER2013_DSM.tif\"))\n\n# digital terrain model (DTM)\ndtm &lt;- rast(here::here(\"course-materials\", \"data\", \"week10\", \"SJER2013_DTM.tif\"))\n\nLet’s check if the DSM and DTM have the same resolution, position, and extent by creating a raster stack:\n\ntest_raster &lt;- c(dsm, dtm)\n\nCreate the canopy height model (CHM) or the height of all objects by finding the difference between the DSM and DTM:\n\nchm &lt;- dsm - dtm"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-vegetation-plot-geometries",
    "href": "course-materials/labs/week10.html#load-vegetation-plot-geometries",
    "title": "Week 10: Lab",
    "section": "Load vegetation plot geometries",
    "text": "Load vegetation plot geometries\nThis includes the locations of study plots and the surveys of individual trees in each plot.\n\n# read in plot centroids\nplot_centroids &lt;- st_read(here::here(\"course-materials\", \"data\", \"week10\", \"PlotCentroids\", \"SJERPlotCentroids_Buffer.shp\"))\n\n# test if the plot CRS matches the Lidar CRS\nst_crs(plot_centroids) == st_crs(chm)\n\n\ntm_shape(chm) +\n  tm_raster() +\n  tm_shape(plot_centroids) +\n  tm_polygons()"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-vegetation-survey-data",
    "href": "course-materials/labs/week10.html#load-vegetation-survey-data",
    "title": "Week 10: Lab",
    "section": "Load vegetation survey data",
    "text": "Load vegetation survey data\nLet’s find the maximum tree height in each plot:\n\n# read in the vegetation surveys, which include the height of each tree\n\n# setting this option will keep all character strings as characters\noptions(stringsAsFactors=FALSE)\n\n# read in survey data and find the maximum tree height in each plot\nveg_surveys &lt;- read.csv(here::here(\"course-materials\", \"data\", \"week10\", \"VegetationData\", \"D17_2013_vegStr.csv\")) %&gt;%\n  group_by(plotid) %&gt;%\n  summarise(\"survey_height\" = max(stemheight, na.rm = TRUE))\n\nNow find the maximum tree height in each plot as determined by the CHM:\n\nextract_chm_height &lt;- terra::extract(chm, plot_centroids, fun = max) %&gt;%\n  rename(chm_height = SJER2013_DSM) %&gt;%\n  select(chm_height)\n\nCombine tree height estimates from the Lidar and plot surveys:\n\nplot_centroids &lt;- cbind(plot_centroids, extract_chm_height) %&gt;%\n  left_join(.,veg_surveys, by = c(\"Plot_ID\" = \"plotid\"))"
  },
  {
    "objectID": "course-materials/labs/week10.html#plot-results",
    "href": "course-materials/labs/week10.html#plot-results",
    "title": "Week 10: Lab",
    "section": "Plot results",
    "text": "Plot results\nLet’s compare the estimates between the two methods: Lidar and on-the-ground surveys\n\nTo make the comparison, we’ll add a 1:1 line\n\nIf all the points fall along this line it means that both methods give the same answer\n\nLet’s also add a regression line with confidence intervals to compare how the overall fit between methods compares to the 1:1 line\n\n\nggplot(plot_centroids, aes(y=chm_height, x= survey_height)) +\n  geom_abline(slope=1, intercept=0, alpha=.5, lty=2) + #plotting our \"1:1\" line\n  geom_point() +\n  geom_smooth(method = lm) + # add regression line and confidence interval\n  ggtitle(\"Validating Lidar measurements\") +\n  xlab(\"Maximum Measured Height (m)\") +\n  ylab(\"Maximum Lidar Height (m)\")\n\n\n\n\n\n\n\n\nWe’ve now compared Lidar estimates of tree height to on-the-ground measurements!\nIt looks like the Lidar estimates tend to underestimate tree height for shorter trees and overestimates tree height for taller trees. Or maybe human observers underestimate the height of tall trees because they’re challenging to measure? Or maybe the digital terrain model misjudged the elevation of the ground? There could be many reasons that the answers don’t line up! It’s then up to the researcher to figure out if the mismatch is important for their problem."
  },
  {
    "objectID": "course-materials/labs/week3.html",
    "href": "course-materials/labs/week3.html",
    "title": "Week 3: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of spatial and geometry operations on vector data in R using the sf package. The following materials are modified from Chapter 4 and Chapter 5 of Geocomputation with R by Robin Lovelace."
  },
  {
    "objectID": "course-materials/labs/week3.html#set-up",
    "href": "course-materials/labs/week3.html#set-up",
    "title": "Week 3: Lab",
    "section": "Set Up",
    "text": "Set Up\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(rmapshaper)\nlibrary(smoothr)\nlibrary(spData)"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-subsetting",
    "href": "course-materials/labs/week3.html#spatial-subsetting",
    "title": "Week 3: Lab",
    "section": "Spatial Subsetting",
    "text": "Spatial Subsetting\nSpatial subsetting is the process of converting a spatial object into a new object containing only the features that relate in space to another object. This is analogous the attribute subsetting that we covered last week. There are many ways to spatially subset in R, so we will explore a few.\nLet’s start by going back to the New Zealand datasets and find all the high points in the state of Canterbury.\n\ncanterbury &lt;- nz %&gt;%\n  filter(Name == \"Canterbury\")\n\n# subsets nz_heights to just the features that intersect Canterbury\nc_height &lt;- nz_height[canterbury, ]\n\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(nz_height) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_polygons(fill = \"blue\") +\n  tm_shape(c_height) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\nThe default is to subset to features that intersect, but we can use other operations, including finding features that do not intersect.\n\noutside_height &lt;- nz_height[canterbury, , op = st_disjoint]\n\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_polygons(fill = \"blue\") +\n  tm_shape(outside_height) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\nWe can perform the same operations using topological operators. These operators return matrices testing the relationship between features.\n\nsel_sgbp &lt;- st_intersects(x = nz_height, y = canterbury)\n\nsel_logical &lt;- lengths(sel_sgbp) &gt; 0\n\nc_height2 &lt;- nz_height[sel_logical, ]\n\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_polygons(fill = \"blue\") +\n  tm_shape(c_height2) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\nWe can also use the st_filter function in sf:\n\nc_height3 &lt;- nz_height %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects)\n\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_polygons(fill = \"blue\") +\n  tm_shape(c_height3) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\nWe can change the predicate option to test subset to features that don’t intersect:\n\noutside_height2 &lt;- nz_height %&gt;%\n  st_filter(y = canterbury, .predicate = st_disjoint)\n\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_polygons(fill = \"blue\") +\n  tm_shape(outside_height2) +\n  tm_dots(fill = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week3.html#buffers",
    "href": "course-materials/labs/week3.html#buffers",
    "title": "Week 3: Lab",
    "section": "Buffers",
    "text": "Buffers\nBuffers create polygons representing a set distance from a feature.\n\nseine_buffer &lt;- st_buffer(seine, dist = 5000)\n\ntm_shape(seine_buffer) +\n  tm_polygons()"
  },
  {
    "objectID": "course-materials/labs/week3.html#unions",
    "href": "course-materials/labs/week3.html#unions",
    "title": "Week 3: Lab",
    "section": "Unions",
    "text": "Unions\nAs we saw in the last lab, we can spatially aggregate without explicitly asking R to do so.\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE))\n\nWhat is going on here? Behind the scenes, summarize() is using st_union() to dissolve the boundaries.\n\nus_west &lt;- us_states %&gt;%\n  filter(REGION == \"West\")\n\nus_west_union &lt;- st_union(us_west)\n\ntm_shape(us_west_union) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nst_union() can also take 2 geometries and unite them.\n\ntexas &lt;-  us_states %&gt;%\n  filter(NAME == \"Texas\")\n\ntexas_union = st_union(us_west_union, texas)\n\ntm_shape(texas_union) +\n  tm_polygons()"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-joining",
    "href": "course-materials/labs/week3.html#spatial-joining",
    "title": "Week 3: Lab",
    "section": "Spatial Joining",
    "text": "Spatial Joining\nWhere attribute joining depends on both data sets sharing a ‘key’ variable, spatial joining uses the same concept but depends on spatial relationships between data sets.\nLet’s test this out by creating 50 points randomly distributed across the world and finding out what countries they call in.\n\nset.seed(2018)\nbb &lt;- st_bbox(world)\n\nrandom_df &lt;- data.frame(\n  x = runif(n = 10, min = bb[1], max = bb[3]),\n  y = runif(n = 10, min = bb[2], max = bb[4])\n)\n\nrandom_points &lt;- random_df %&gt;%\n  st_as_sf(coords = c(\"x\", \"y\")) %&gt;%\n  st_set_crs(\"EPSG:4326\")\n\ntm_shape(world) +\n  tm_fill() +\n  tm_shape(random_points) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\nLet’s first use spatial subsetting to find just the countries that contain random points.\n\nworld_random &lt;- world[random_points, ]\n\ntm_shape(world) +\n  tm_fill() +\n  tm_shape(world_random) +\n  tm_fill(fill = \"red\")\n\n\n\n\n\n\n\n\nNow let’s perform a spatial join to add the info from each country that a point falls into onto the point dataset.\n\nrandom_joined  &lt;- st_join(random_points, world)\n\ntm_shape(world) +\n  tm_fill() +\n  tm_shape(random_joined) +\n  tm_dots(fill = \"name_long\")\n\n\n\n\n\n\n\n\nBy default, st_join performs a left join. We change this and instead perform an inner join.\n\nrandom_joined_inner &lt;- st_join(random_points, world, left = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week3.html#non-overlapping-joins",
    "href": "course-materials/labs/week3.html#non-overlapping-joins",
    "title": "Week 3: Lab",
    "section": "Non-Overlapping Joins",
    "text": "Non-Overlapping Joins\nSometimes we might want join geographic datasets that are strongly related, but do not have overlapping geometries. To demonstrate this, let’s look at data on cycle hire points in London.\n\ntmap_mode(\"view\")\n\ntm_shape(cycle_hire) +\n  tm_dots(col = \"blue\", alpha = 0.5) +\n  tm_shape(cycle_hire_osm) +\n  tm_dots(col = \"red\", alpha = 0.5)\n\n\n\n\n\n We can check if any of these points overlap.\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n\n[1] FALSE\n\n\nLet’s say we need to join the ‘capacity’ variable in cycle_hire_osm onto the official ‘target’ data in cycle_hire. The simplest method is using the topological operator st_is_within_distance().\n\nsel &lt;- st_is_within_distance(cycle_hire, cycle_hire_osm, dist = 20)\n\nsummary(lengths(sel) &gt; 0) #summarizes the number of points within 20 meters\n\n   Mode   FALSE    TRUE \nlogical     304     438 \n\n\nNow, we’d like to add the values from cycle_hire_osm onto the cycle_hire points.\n\nz &lt;- st_join(cycle_hire, cycle_hire_osm, st_is_within_distance, dist = 20)\n\nnrow(cycle_hire)\n\n[1] 742\n\nnrow(z)\n\n[1] 762\n\n\nNote: the number of rows of the join is larger than the number of rows in the original dataset. Why? Because some points in ‘cycle_hire’ were within 20 meters of multiple points in ‘cycle_hire_osm’. If we wanted to aggregate so we have just one value per original point, we can use the aggregation methods from last week.\n\nz &lt;- z %&gt;%\n  group_by(id) %&gt;%\n  summarise(capacity = mean(capacity))"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-aggregation",
    "href": "course-materials/labs/week3.html#spatial-aggregation",
    "title": "Week 3: Lab",
    "section": "Spatial Aggregation",
    "text": "Spatial Aggregation\nSimilar to attribute data aggregation, spatial data aggregation condenses data (we end up with fewer rows than we started with).\nLet’s say we wanted to find the average height of high point in each region of New Zealand. We could use the aggregate() function in base R.\n\nnz_agg &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\n\nThe result of this is an object with the same geometries as the aggregating feature data set (in this case ‘nz’).\n\nnz_agg\n\nWe could also use a sf/dplyr approach.\n\nnz_agg &lt;- st_join(nz, nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarise(elevation = mean(elevation, na.rm = TRUE))\n\nnz_agg"
  },
  {
    "objectID": "course-materials/labs/week3.html#joining-incongruent-layers",
    "href": "course-materials/labs/week3.html#joining-incongruent-layers",
    "title": "Week 3: Lab",
    "section": "Joining Incongruent Layers",
    "text": "Joining Incongruent Layers\nWe might want to aggregate data to geometries that are not congruent (i.e. their boundaries don’t line up). This causes issues when we think about how to summarize associated values.\n\n# head(incongruent)\n# head(aggregating_zones)\n\ntm_shape(incongruent) +\n  tm_polygons() +\n  tm_shape(aggregating_zones) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n The simplest method for dealing with this is using area weighted spatial interpolation which transfers values from the ‘incongruent’ object to a new column in ‘aggregating_zones’ in proportion with the area of overlap.\n\niv &lt;- incongruent[\"value\"]\nagg_aw &lt;- st_interpolate_aw(iv, aggregating_zones, extensive = TRUE)\n\ntm_shape(agg_aw) +\n  tm_fill(fill = \"value\")"
  },
  {
    "objectID": "course-materials/labs/week3.html#centroids",
    "href": "course-materials/labs/week3.html#centroids",
    "title": "Week 3: Lab",
    "section": "Centroids",
    "text": "Centroids\nCentroids identify the center of a spatial feature. Similar to taking an average, there are many ways to compute a centroid. The most common is the geographic centroid.\n\nnz_centroid &lt;- st_centroid(nz)\n\ntm_shape(nz) +\n  tm_fill() +\n  tm_shape(nz_centroid) +\n  tm_dots()\n\n\n\n\n\n Sometimes centroids fall outside of the boundaries of the objects they were created from. In the case where we need them to fall inside of the feature, we can use point on surface methods.\n\nnz_pos &lt;- st_point_on_surface(nz)\n\ntm_shape(nz) +\n  tm_fill() +\n  tm_shape(nz_centroid) +\n  tm_dots() +\n  tm_shape(nz_pos) +\n  tm_dots(fill = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week3.html#distance-relationships",
    "href": "course-materials/labs/week3.html#distance-relationships",
    "title": "Week 3: Lab",
    "section": "Distance Relationships",
    "text": "Distance Relationships\nWhile topological relationships are binary (features either intersect or don’t), distance relationships are continuous.\nFind the distance between the highest point in NZ and the centroid of the Canterbury region:\n\nnz_highest &lt;- nz_height %&gt;%\n  slice_max(n = 1, order_by = elevation)\n\ncanterbury_centroid = st_centroid(canterbury)\n\nst_distance(nz_highest, canterbury_centroid)\n\nUnits: [m]\n       [,1]\n[1,] 115540\n\n\nNote: this function returns distances with units (yay!) and as a matrix, meaning we could find the distance between many locations at once."
  },
  {
    "objectID": "course-materials/labs/week3.html#simplification",
    "href": "course-materials/labs/week3.html#simplification",
    "title": "Week 3: Lab",
    "section": "Simplification",
    "text": "Simplification\nSimplification generalizes vector data (polygons and lines) to assist with plotting and reducing the amount of memory, disk space, and network bandwidth to handle a dataset.\nLet’s try simplifying the US states using the Douglas-Peucker algorithm.\nGEOS assumes a projected CRS, so we first need to project the data, in this case into the US National Atlas Equal Area (EPSG = 2163):\n\nus_states2163 &lt;- st_transform(us_states, \"EPSG:2163\")\nus_states2163 &lt;- us_states2163\n\nus_states_simp1 &lt;- st_simplify(us_states2163, dTolerance = 100000)  # 100 km\n\ntm_shape(us_states_simp1) +\n  tm_polygons()\n\n\n\n\n\n To preserve the states’ topology let’s use a simplify function from rmapshaper which uses Visalingam’s algorithm.\n\nlibrary(rmapshaper)\n\n# proportion of points to retain (0-1; default 0.05)\nus_states_simp2 &lt;- rmapshaper::ms_simplify(us_states2163, keep = 0.01,\n                                          keep_shapes = TRUE)\n\ntm_shape(us_states_simp2) +\n  tm_polygons()\n\n\n\n\n\n Instead of simplifying, we could try smoothing using Gaussian kernel regression.\n\nus_states_simp3 &lt;- smoothr::smooth(us_states2163, method = 'ksmooth', smoothness = 6)\n\ntm_shape(us_states_simp3) +\n  tm_polygons()"
  },
  {
    "objectID": "course-materials/labs/week8.html",
    "href": "course-materials/labs/week8.html",
    "title": "Week 8: Lab",
    "section": "",
    "text": "Phenology is the timing of life history events. Important phenological events for plants involve the growth of leaves, flowering, and senescence (death of leaves). Plants species adapt the timing of these events to local climate conditions to ensure successful reproduction. Subsequently, animal species often adapt their phenology to take advantage of food availability. As the climate shifts this synchronization is being thrown out of whack. Shifts in phenology are therefore a common yardstick of understanding how and if ecosystems are adjusting to climate change.\nPlant species may employ the following phenological strategies:\nThis lab is based on a materials developed by Chris Kibler."
  },
  {
    "objectID": "course-materials/labs/week8.html#task",
    "href": "course-materials/labs/week8.html#task",
    "title": "Week 8: Lab",
    "section": "Task",
    "text": "Task\nIn this lab we are analyzing plant phenology near the Santa Clara River which flows from Santa Clarita to Ventura. We will investigate the phenology of the following plant communities:\n\nRiparian forests: grow along the river, dominated by winter deciduous cottonwood and willow trees\nGrasslands: grow in openspaces, dominated by drought deciduous grasses\nChaparral shrublands: grow in more arid habitats, dominated by evergreen shrubs\n\nTo investigate the phenology of these plant communities we will a time series of Landsat imagery and polygons identifying the locations of study sites within each plant community.\nOur goals in this lab are:\n\nConvert spectral reflectance into a measure of vegetation productivity (NDVI)\nCalculate NDVI throughout the year\nSummarize NDVI values within vegetation communities\nVisualize changes in NDVI within vegetation communities"
  },
  {
    "objectID": "course-materials/labs/week8.html#data",
    "href": "course-materials/labs/week8.html#data",
    "title": "Week 8: Lab",
    "section": "Data",
    "text": "Data\nLandsat’s Operational Land Imager (OLI)\n\n8 pre-processed scenes\n\nLevel 2 surface reflectance products\nErroneous values set to NA\nScale factor set to 100\nBands 2-7\nDates in filenname\n\n\nStudy sites\n\nPolygons representing sites\n\nstudy_site: character string with plant type"
  },
  {
    "objectID": "course-materials/labs/week8.html#prerequisites",
    "href": "course-materials/labs/week8.html#prerequisites",
    "title": "Week 8: Lab",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s load all necessary packages:\n\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(tmap)\nlibrary(cowplot)"
  },
  {
    "objectID": "course-materials/labs/week8.html#create-ndvi-function",
    "href": "course-materials/labs/week8.html#create-ndvi-function",
    "title": "Week 8: Lab",
    "section": "Create NDVI Function",
    "text": "Create NDVI Function\nLet’s start by defining a function to compute the NDVI.\n\nNDVI computes the difference in reflectance in the near infrared and red bands, normalized by their sum.\n\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}"
  },
  {
    "objectID": "course-materials/labs/week8.html#compute-ndvi-for-a-single-scene",
    "href": "course-materials/labs/week8.html#compute-ndvi-for-a-single-scene",
    "title": "Week 8: Lab",
    "section": "Compute NDVI for a Single Scene",
    "text": "Compute NDVI for a Single Scene\nWe have 8 scenes collected by Landsat’s OLI sensor on 8 different days throughout the year.\nLet’s start by loading in the first scene collected on June 12, 2018:\n\nlandsat_20180612 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20180612.tif\"))\nlandsat_20180612\n\nNow let’s update the names of the layers to match the spectral bands they correspond to:\n\nnames(landsat_20180612) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nlandsat_20180612\n\nNow we can apply the NDVI function we created to compute NDVI for this scene using the lapp() function.\n\nThe lapp() function applies a function to each cell using layers as arguments.\nTherefore, we need to tell lapp() which layers (or bands) to pass into the function.\n\nThe NIR band is the 4th layer and the red band is the 3rd layer in our raster. In this case, because we defined the NIR band as the first argument and the red band as the second argument in our function, we tell lapp() to use the 4th layer first and 3rd layer second.\n\nndvi_20180612 &lt;- lapp(landsat_20180612[[c(4, 3)]], fun = ndvi_fun)\nndvi_20180612"
  },
  {
    "objectID": "course-materials/labs/week8.html#attempt-1-compute-ndvi-for-all-scences",
    "href": "course-materials/labs/week8.html#attempt-1-compute-ndvi-for-all-scences",
    "title": "Week 8: Lab",
    "section": "Attempt 1: Compute NDVI for All Scences",
    "text": "Attempt 1: Compute NDVI for All Scences\nNow we want to repeat the same operations for all 8 scenes. Below is a possible solution, but it’s pretty clunky.\nLet’s load each layer:\n\nlandsat_20180612 &lt;-rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20180612.tif\"))\nlandsat_20180815 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20180815.tif\"))\nlandsat_20181018 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20181018.tif\"))\nlandsat_20181103 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20181103.tif\"))\nlandsat_20190122 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190122.tif\"))\nlandsat_20190223 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190223.tif\"))\nlandsat_20190412 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190412.tif\"))\nlandsat_20190701 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190701.tif\"))\n\nAnd rename each layer:\n\nnames(landsat_20180612) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20180815) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20181018) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20181103) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190122) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190223) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190412) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190701) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\nNext, compute NDVI for each layer:\n\nndvi_20180612 &lt;- lapp(landsat_20180612[[c(4, 3)]], fun = ndvi_fun)\nndvi_20180815 &lt;- lapp(landsat_20180815[[c(4, 3)]], fun = ndvi_fun)\nndvi_20181018 &lt;- lapp(landsat_20181018[[c(4, 3)]], fun = ndvi_fun)\nndvi_20181103 &lt;- lapp(landsat_20181103[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190122 &lt;- lapp(landsat_20190122[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190223 &lt;- lapp(landsat_20190223[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190412 &lt;- lapp(landsat_20190412[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190701 &lt;- lapp(landsat_20190701[[c(4, 3)]], fun = ndvi_fun)\n\nLet’s combine NDVI layers into a single raster stack.\n\nall_ndvi &lt;- c(ndvi_20180612, ndvi_20180815, ndvi_20181018, ndvi_20181103, ndvi_20190122, ndvi_20190223, ndvi_20190412, ndvi_20190701)\n\nNow, update the names of each layer to match the date of each image:\n\nnames(all_ndvi) &lt;- c(\"2018-06-12\", \"2018-08-15\", \"2018-10-18\", \"2018-11-03\", \"2019-01-22\", \"2019-02-23\", \"2019-04-12\", \"2019-07-01\")"
  },
  {
    "objectID": "course-materials/labs/week8.html#attempt-2-compute-ndvi-for-all-scenes",
    "href": "course-materials/labs/week8.html#attempt-2-compute-ndvi-for-all-scenes",
    "title": "Week 8: Lab",
    "section": "Attempt 2: Compute NDVI for All Scenes",
    "text": "Attempt 2: Compute NDVI for All Scenes\nThe first attempt was pretty clunky and required a lot of copy/pasting. Because we’re performing the same operations over and over again, this is a good opportunity to generalize our workflow into a function!\nLet’s start over and see how we could do this more efficiently.\nWe’ll clear our environment and redefine our function for NDVI:\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\nNext, let’s first sketch out what operations we want to perform so we can figure out what our function needs:\n\n# Note: this code is not meant to run! \n# We're just outlining the function we want to create\n\ncreate_ndvi_layer &lt;- function(){\n  # Read scene\n  landsat &lt;- rast(file)\n  # Rename layer\n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n  # Compute NDVI\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n}\n\n# What do we notice as what we need to pass into our function?\n\nWe want a list of the scenes so that we can tell our function to compute NDVI for each. To do that we look in our data folder for the relevant file.\n\nAsk for the names of all the files in the week8 folder\nSet the “pattern” option to return the names that end in .tif (\n\n.tif is the file extension for the landsat scenes\n\nSet the “full.names” option returns the full file path for each scene\n\n\nfiles &lt;- list.files(\n  here(\"course-materials\", \"data\", \"week8\"), pattern = \"*.tif\", \n  full.names = TRUE)\n\nNow let’s update our function to work with list of file names we created:\n\nPass function a number that will correspond to the index in the list of file names\n\n\ncreate_ndvi_layer &lt;- function(i){\n  landsat &lt;- rast(files[i])\n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n}\n\nLet’s test our function by asking it to read in the first file:\n\ntest &lt;- create_ndvi_layer(1)\n\nNow we can use our function to create a NDVI layer for each scene and stack them into a single rasterstack. And then update layer names to match date:\n\nall_ndvi &lt;- c(create_ndvi_layer(1), create_ndvi_layer(2), create_ndvi_layer(3), create_ndvi_layer(4), create_ndvi_layer(5), create_ndvi_layer(6), create_ndvi_layer(7), create_ndvi_layer(8))\n\nnames(all_ndvi) &lt;- c(\"2018-06-12\", \"2018-08-15\", \"2018-10-18\", \"2018-11-03\", \"2019-01-22\", \"2019-02-23\", \"2019-04-12\", \"2019-07-01\")"
  },
  {
    "objectID": "course-materials/labs/week8.html#compare-ndvi-across-vegetation-communities",
    "href": "course-materials/labs/week8.html#compare-ndvi-across-vegetation-communities",
    "title": "Week 8: Lab",
    "section": "Compare NDVI Across Vegetation Communities",
    "text": "Compare NDVI Across Vegetation Communities\nNow that we have computed NDVI for each of our scenes (days) we want to compare changes in NDVI values across different vegetation communities.\nFirst, we’ll read in a shapefile of study sites:\n\nsites &lt;- st_read(here(\"course-materials\", \"data\",\"week8\",\"study_sites.shp\"))\n\nAnd plot study sites on a single NDVI layer:\n\ntm_shape(all_ndvi[[1]]) +\n  tm_raster() +\n  tm_shape(sites) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\nExtract NDVI at Study Sites\nHere, we find the average NDVI within each study site. The output of extract is a data frame with rows that match the study site dataset, so we bind the results to the original dataset.\n\nsites_ndvi &lt;- terra::extract(all_ndvi, sites, fun = \"mean\")\n\nsites_annotated &lt;- cbind(sites, sites_ndvi)\n\nWe’re done! Except our data is very untidy… Let’s tidy it up!\n\nConvert to data frame\nTurn from wide to long format\nTurn layer names into date format\n\n\nsites_clean &lt;- sites_annotated %&gt;%\n  st_drop_geometry() %&gt;%\n  select(-ID) %&gt;%\n  pivot_longer(!study_site) %&gt;%\n  rename(\"NDVI\" = value) %&gt;%\n  mutate(\"year\" = str_sub(name, 2, 5),\n         \"month\" = str_sub(name, 7, 8),\n         \"day\" = str_sub(name, -2, -1)) %&gt;%\n  unite(\"date\", 4:6, sep = \"-\") %&gt;%\n  mutate(\"date\" = lubridate::as_date(date))"
  },
  {
    "objectID": "course-materials/labs/week8.html#plot-results",
    "href": "course-materials/labs/week8.html#plot-results",
    "title": "Week 8: Lab",
    "section": "Plot Results",
    "text": "Plot Results\nLet’s plot the results:\n\nggplot(sites_clean,\n       aes(x = date, y = NDVI,\n           group = study_site, col = study_site)) +\n  scale_color_manual(values = c(\"#EAAC8B\", \"#315C2B\", \"#315C2B\", \"#315C2B\",\"#9EA93F\")) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Normalized Difference Vegetation Index (NDVI)\", col = \"Vegetation type\",\n       title = \"Seasonal cycles of vegetation productivity\")"
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Week 1: Intro to Spatial Data",
    "section": "",
    "text": "Reminder\n\n\n\nAssignment 1 posted Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week1.html#class-materials",
    "href": "course-materials/week1.html#class-materials",
    "title": "Week 1: Intro to Spatial Data",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nWelcome to EDS 223\nTemplate  Answer key\n\n\nDiscussion\nMaking maps\nTemplate  Answer key"
  },
  {
    "objectID": "course-materials/week1.html#background-reading",
    "href": "course-materials/week1.html#background-reading",
    "title": "Week 1: Intro to Spatial Data",
    "section": "Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 9\nGIS Fundamentals, Chapter 2\nGIS Fundamentals, Chapter 3\nA Gentle Introduction to GIS, Chapter 8\nGeographic vs projected coordinate systems (Esri)"
  },
  {
    "objectID": "course-materials/week1.html#technical-background",
    "href": "course-materials/week1.html#technical-background",
    "title": "Week 1: Intro to Spatial Data",
    "section": "Technical Background",
    "text": "Technical Background\n\ntmap: thematic maps in R documentation\ntmap overview\nCreating thematic maps in R"
  },
  {
    "objectID": "course-materials/week1.html#additional-resources",
    "href": "course-materials/week1.html#additional-resources",
    "title": "Week 1: Intro to Spatial Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nJohn Snow: A Legacy of Disease Detectives (CDC)\nHow the north ended up on top of the map (Al Jazeera)\nWhy maps point North on top? (Geospatial World)\nWhy all world maps are wrong (Vox)"
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Week 2: Intro to Vector Data",
    "section": "",
    "text": "Reminder\n\n\n\nAssignment 1 due Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week2.html#class-materials",
    "href": "course-materials/week2.html#class-materials",
    "title": "Week 2: Intro to Vector Data",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nIntro to spatial/vector data models and sf\nTemplate  Answer key\n\n\nDiscussion"
  },
  {
    "objectID": "course-materials/week2.html#background-reading",
    "href": "course-materials/week2.html#background-reading",
    "title": "Week 2: Intro to Vector Data",
    "section": "Background Reading",
    "text": "Background Reading\n\nGIS Fundamentals, Chapter 2 Part 2\nGeocomputation with R, Chapter 2\nGeocomputation with R, Chapter 3\nGeocomputation with R, Chapter 7"
  },
  {
    "objectID": "course-materials/week2.html#technical-background",
    "href": "course-materials/week2.html#technical-background",
    "title": "Week 2: Intro to Vector Data",
    "section": "Technical Background",
    "text": "Technical Background\n\nsf overview\nsf cheatsheet\nIntroduction to sf and stars"
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "Week 4: Raster Operations",
    "section": "",
    "text": "Reminder\n\n\n\nCheck-in Quiz 1 due Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week4.html#class-materials",
    "href": "course-materials/week4.html#class-materials",
    "title": "Week 4: Raster Operations",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nIntro to raster data model\nTemplate  Answer key\n\n\nDiscussion\nRaster operation basics\nTemplate  Answer key"
  },
  {
    "objectID": "course-materials/week4.html#background-reading",
    "href": "course-materials/week4.html#background-reading",
    "title": "Week 4: Raster Operations",
    "section": "Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 3\nGeocomputation with R, Chapter 4\nGeocomputation with R, Chapter 5"
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "Week 6: Intro to Remote Sensing",
    "section": "",
    "text": "Reminder\n\n\n\nAssignment 3 posted Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week6.html#class-materials",
    "href": "course-materials/week6.html#class-materials",
    "title": "Week 6: Intro to Remote Sensing",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nRemote sensing basics and raster geometry operations\nTemplate  Answer key\n\n\nDiscussion\nBringing vector and raster together\nTemplate  Answer key"
  },
  {
    "objectID": "course-materials/week6.html#background-reading",
    "href": "course-materials/week6.html#background-reading",
    "title": "Week 6: Intro to Remote Sensing",
    "section": "Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2"
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Week 8: Remote Sensing of Vegetation",
    "section": "",
    "text": "Reminder\n\n\n\nCheck-in Quiz 3 due Weekday, DD-MM-YYYY at HH:MM AM/PM."
  },
  {
    "objectID": "course-materials/week8.html#class-materials",
    "href": "course-materials/week8.html#class-materials",
    "title": "Week 8: Remote Sensing of Vegetation",
    "section": "Class Materials",
    "text": "Class Materials\n\n\n\n\n\n\n\n\n Session\n Topic\n Lab\n\n\n\n\nLecture\nRemote sensing of vegetation (leaf, canopy, and landscape)\nDownload data  Template  Answer key\n\n\nDiscussion"
  },
  {
    "objectID": "course-materials/week8.html#background-reading",
    "href": "course-materials/week8.html#background-reading",
    "title": "Week 8: Remote Sensing of Vegetation",
    "section": "Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 10"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the spatial modeling and analytic techniques of geographic information science to data science students. The emphasis is on deep understanding of spatial data models and the analytic operations they enable. Recognizing remotely sensed data as a key data type within environmental data science, this course will also introduce fundamental concepts and applications of remote sensing. In addition to this theoretical background, students will become familiar with libraries, packages, and APIs that support spatial analysis in R."
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nLecture: Monday 12:30-3:15 PM at Bren Hall 1424\nDiscussion Section 1: Thursday 1:00-1:50 PM at Bren Hall 3526\nDiscussion Section 2: Thursday 2:00-2:50 PM at Bren Hall 3526"
  },
  {
    "objectID": "index.html#readings-and-references",
    "href": "index.html#readings-and-references",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Readings and References",
    "text": "Readings and References\n\nGeocompuation with R\nSpatial Data Science with Applications in R\nA Gentle Introduction to GIS"
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nMeet minimum MEDS device requirements\nInstall or update to R version 4.40\nInstall or update RStudio\nCreate a GitHub account"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nRuth Oliver\nEmail: rutholiver@ucsb.edu\nLearn more: ryoliver-lab.github.io\n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\n\n\nAlessandra Vidal Meza\nEmail: avidalmeza@ucsb.edu\nLearn more: avidalmeza.github.io"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis website was designed by Sam Csik."
  }
]