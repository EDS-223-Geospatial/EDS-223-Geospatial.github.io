[
  {
    "objectID": "resources.html#r-programming",
    "href": "resources.html#r-programming",
    "title": "Resources",
    "section": "R Programming",
    "text": "R Programming\n\nTidyverse style guide\nTidy design principles\nr-spatial\nSpatial Statistics for Data Science: Theory and Practice with R. Paula Moraga, 2023.\nA Guide to Reproducible Code in Ecology and Evolution. British Ecological Society, 2017"
  },
  {
    "objectID": "resources.html#coordinate-systems-and-projections",
    "href": "resources.html#coordinate-systems-and-projections",
    "title": "Resources",
    "section": "Coordinate Systems and Projections",
    "text": "Coordinate Systems and Projections\n\nGeographic vs projected coordinate systems (Esri)\nCoordinate Reference System and Spatial Projection (Earth Lab, CU Boulder)\nGuide to map projections (Axis Maps)\nChoosing a projection (Penn State)\nDiscover coordinate systems"
  },
  {
    "objectID": "resources.html#mapmaking",
    "href": "resources.html#mapmaking",
    "title": "Resources",
    "section": "Mapmaking",
    "text": "Mapmaking\n\nPlotting Geospatial Data in R: comparing across packages\nColor palette finder with paletteer\nColorBrewer 2.0\nIntro to Color Visualization (NASA)\nGIS icons\nGuide to common errors in map production (Journal of Maps)"
  },
  {
    "objectID": "course-materials/week5.html#class-materials",
    "href": "course-materials/week5.html#class-materials",
    "title": "Intro to remote sensing & electromagnetism",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nFundamentals of electromagnetic radiation\n\n\n Lab\nVector and raster interactions\n\n\n Discussion\nPractice raster operations with vectors (Answer Key)"
  },
  {
    "objectID": "course-materials/week5.html#assignment-reminders",
    "href": "course-materials/week5.html#assignment-reminders",
    "title": "Intro to remote sensing & electromagnetism",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 5)\n10/28/2024\n10/28/2024\n\n\nSR\nMid-course Self Reflection (SR#2)\n10/28/2024\n11/02/2024\n\n\nHW\nHomework Assignment #3\n10/21/2024\n11/09/2024"
  },
  {
    "objectID": "course-materials/week5.html#background-reading",
    "href": "course-materials/week5.html#background-reading",
    "title": "Intro to remote sensing & electromagnetism",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2"
  },
  {
    "objectID": "course-materials/week6.html#class-materials",
    "href": "course-materials/week6.html#class-materials",
    "title": "Remote sensing data collection",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nResolutions: spatial, temporal, spectral, radiometric\n\n\n Lab\nVector and raster interactions cont.; False color images\n\n\n Discussion\nWorking with false color images (Answer Key)"
  },
  {
    "objectID": "course-materials/week6.html#assignment-reminders",
    "href": "course-materials/week6.html#assignment-reminders",
    "title": "Remote sensing data collection",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 6)\n11/04/2024\n11/04/2024\n\n\nHW\nHomework Assignment #3\n10/21/2024\n11/09/2024"
  },
  {
    "objectID": "course-materials/week6.html#background-reading",
    "href": "course-materials/week6.html#background-reading",
    "title": "Remote sensing data collection",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 1\nRemote Sensing of the Environment, Chapter 2\nHow to Interpret a False-Color Satellite Image"
  },
  {
    "objectID": "course-materials/week6.html#additional-resources",
    "href": "course-materials/week6.html#additional-resources",
    "title": "Remote sensing data collection",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nHow raster functions map to stars functions\nSpatiotemporal raster data handling with stars\nA comparison of terra and stars packages"
  },
  {
    "objectID": "course-materials/week2.html#class-materials",
    "href": "course-materials/week2.html#class-materials",
    "title": "Intro to spatial data models",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nIntro to spatial/vector data models\n\n\n Lab\nSpatial operations with vector data\n\n\n Discussion\nWorking with multiple vector types (Answer Key)"
  },
  {
    "objectID": "course-materials/week2.html#assignment-reminders",
    "href": "course-materials/week2.html#assignment-reminders",
    "title": "Intro to spatial data models",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 2)\n10/07/2024\n10/07/2024\n\n\nHW\nHomework Assignment #2\n10/07/2024\n10/19/2024"
  },
  {
    "objectID": "course-materials/week2.html#background-reading",
    "href": "course-materials/week2.html#background-reading",
    "title": "Intro to spatial data models",
    "section": " Background Reading",
    "text": "Background Reading\n\nGIS Fundamentals, Chapter 2 Part 2\nGeocomputation with R, Chapter 2\nGeocomputation with R, Chapter 3\nGeocomputation with R, Chapter 7"
  },
  {
    "objectID": "course-materials/week2.html#additional-resources",
    "href": "course-materials/week2.html#additional-resources",
    "title": "Intro to spatial data models",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nsf overview\nsf cheatsheet\nIntroduction to sf and stars"
  },
  {
    "objectID": "course-materials/week1.html#class-materials",
    "href": "course-materials/week1.html#class-materials",
    "title": "Intro to EDS 223 and map making",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nWelcome to EDS 223, why spatial?, and fundamentals of good maps\n\n\n Lab\nMaking maps in R with tmap\n\n\n Discussion\nMaking maps with tmap and beyond (Answer Key)"
  },
  {
    "objectID": "course-materials/week1.html#assignment-reminders",
    "href": "course-materials/week1.html#assignment-reminders",
    "title": "Intro to EDS 223 and map making",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 1)\n09/30/2024\n09/30/2024\n\n\nSR\nPre-course Self Reflection (SR#1)\n09/30/2024\n10/05/2024\n\n\nHW\nHomework Assignment #1\n09/30/2024\n10/05/2024"
  },
  {
    "objectID": "course-materials/week1.html#background-reading",
    "href": "course-materials/week1.html#background-reading",
    "title": "Intro to EDS 223 and map making",
    "section": " Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 9\nGIS Fundamentals, Chapter 2\nGIS Fundamentals, Chapter 3\nA Gentle Introduction to GIS, Chapter 8\nGeographic vs projected coordinate systems (Esri)"
  },
  {
    "objectID": "course-materials/week1.html#technical-background",
    "href": "course-materials/week1.html#technical-background",
    "title": "Intro to EDS 223 and map making",
    "section": " Technical Background",
    "text": "Technical Background\n\ntmap: thematic maps in R documentation\ntmap overview\nCreating thematic maps in R"
  },
  {
    "objectID": "course-materials/week1.html#additional-resources",
    "href": "course-materials/week1.html#additional-resources",
    "title": "Intro to EDS 223 and map making",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nJohn Snow: A Legacy of Disease Detectives (CDC)\nHow the north ended up on top of the map (Al Jazeera)\nWhy maps point North on top? (Geospatial World)\nWhy all world maps are wrong (Vox)"
  },
  {
    "objectID": "course-materials/resources/good-bad-example/good_example.html",
    "href": "course-materials/resources/good-bad-example/good_example.html",
    "title": "Professional Output Example",
    "section": "",
    "text": "Professional documents should be easy to follow and get the point across to the reader in a concise manner.\nHelpful components include:\n\ndocument header with title, name, and date\n\nideally, the date reflects the day the document is most recently rendered\n\nall packages are loaded together at the top of the document\ninclude code comments when appropriate\nclean code indentation for lists, parameters, functions within functions, etc.\ndata visualizations such as maps and plots contain all necessary components such as a legend, axes labels, units, and appropriate palette (divergent, colorblind friendly, etc.)\nfolding code or sourcing separate scripts when appropriate to direct reader’s attention to the important output or to condense code\nhiding unnecessary output such as warning messages or loaded packages\nsuccinct documentation between steps, such as section headers, descriptions for an analysis step, and map/plot interpretation\ncomplete and detailed data citations\nfunction definitions should include warnings and errors that ensure the intended operations are executed\n\n\n\nElephants are intelligent and gregarious animals that tend to travel as a herd and seek out resources such as shrubs and water. Elephants in Krugar National Park were collared with GPS tracking devices from August 2007 through August 2009 (Slotow et al. 2019).\nObjective: Use GPS point locations of individual elephants to visualize their movement throughout Krugar National Park in South Africa between 2007 and 2009.\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(tmap)\nlibrary(rnaturalearth)\nlibrary(osmdata)\n\n# source external script that defines custom functions\nsource(list.files(path = here(\"course-materials\"),\n                  pattern = \"functions.R\",\n                  recursive = TRUE,\n                  full.names = TRUE))\n\n\n\n\n\n\nImport the elephant movement data and configure its spatial components\n\n\nmetadata_df &lt;- load_csv(filename = \"elephants_metadata.csv\",\n                        directory = \"course-materials\",\n                        communicate = FALSE)\n\nelephants_df &lt;- load_csv(filename = \"elephants.csv\",\n                         directory = \"course-materials\",\n                         communicate = FALSE)\n\n# convert the elephant observations to sf objects\nelephants &lt;- to_spatial_points(data = elephants_df,\n                               latitude_col = \"location-lat\",\n                               longitude_col = \"location-long\")\n\n\n\n\n\nImport the spatial boundaries for Kruger National Park to serve as a basemap for the elephant tracks\n\n\n# set bbox to approximate boundaries of Kruger NP\nkruger &lt;- osmdata::opq(bbox = c(16, -35, 33, -22)) %&gt;%\n          add_osm_feature(key = \"boundary\", \n                          value = \"protected_area\") %&gt;%\n          add_osm_feature(key = \"name\", \n                          value = \"Kruger National Park\") %&gt;%\n          osmdata_sf()\n\n# subset imported list of elements to just the geodataframe\nkruger_bounds &lt;- kruger$osm_multipolygons\n\n\nProduce a map of elephant observations to visualize the individual movement of elephants\n\n\n\nShow the code\ntitle = \"Elephant Observations\\nKruger National Park\\n2007-2009\"\n\n# basemap: Kruger National Park geometry\ntm_shape(kruger_bounds) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"black\") +\n  # overlay all elephant GPS locations \n  tm_shape(elephants) +\n  tm_dots(col = \"individual-local-identifier\",\n          palette = \"Set3\",\n          # small dots to distinguish between individuals\n          size = 0.001,\n          border.col = \"black\",\n          title = \"Individual\") +\n  tm_layout(bg.color = \"darkgrey\",\n            legend.bg.color = \"white\",\n            title = title,\n            frame = TRUE,\n            title.position = c(\"center\", \"top\"),\n            title.size = 1.2,\n            title.fontface = \"bold\",\n            legend.frame = TRUE,\n            legend.outside = TRUE,\n            legend.position = c(0.08, 0.3),\n            legend.title.size = 0.8,\n            legend.text.size = 0.6) +\n  # add compass with only North arrow\n  tm_compass(position = c(0.0, 0.01),\n             show.labels = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nElephants in Kruger National Park fitted with GPS collars are primarily located in the southern part of the park. The clustered tracks imply that this species is highly gregarious. Tracks that expand beyond park boundaries, notably on the western side, imply that the elephants could be seeking resources such as food or water outside of the park. Alternatively, error in the GPS location for these points could explain this perceived movement.\nFuther analysis steps could include:\n\nzooming in further to the realized niche\npartitioning the data temporally\nusing metadata to advise how to subset the data to view fewer elephant locations at a time\noverlaying other vector or raster data layers such as surface water and vegetation to understand if the elephants’ movement is driven by these resources\n\n\n\n\n\n\n\nData\nCitation\nLink\n\n\n\n\nElephant observations\nSlotow R, Thaker M, Vanak AT. 2019. Data from: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water. Movebank Data Repository. https://doi.org/10.5441/001/1.403h24q5\nMoveBank Repository: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water"
  },
  {
    "objectID": "course-materials/resources/good-bad-example/good_example.html#elephant-tracking-in-krugar-national-park",
    "href": "course-materials/resources/good-bad-example/good_example.html#elephant-tracking-in-krugar-national-park",
    "title": "Professional Output Example",
    "section": "",
    "text": "Elephants are intelligent and gregarious animals that tend to travel as a herd and seek out resources such as shrubs and water. Elephants in Krugar National Park were collared with GPS tracking devices from August 2007 through August 2009 (Slotow et al. 2019).\nObjective: Use GPS point locations of individual elephants to visualize their movement throughout Krugar National Park in South Africa between 2007 and 2009.\n\n\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(tmap)\nlibrary(rnaturalearth)\nlibrary(osmdata)\n\n# source external script that defines custom functions\nsource(list.files(path = here(\"course-materials\"),\n                  pattern = \"functions.R\",\n                  recursive = TRUE,\n                  full.names = TRUE))\n\n\n\n\n\n\nImport the elephant movement data and configure its spatial components\n\n\nmetadata_df &lt;- load_csv(filename = \"elephants_metadata.csv\",\n                        directory = \"course-materials\",\n                        communicate = FALSE)\n\nelephants_df &lt;- load_csv(filename = \"elephants.csv\",\n                         directory = \"course-materials\",\n                         communicate = FALSE)\n\n# convert the elephant observations to sf objects\nelephants &lt;- to_spatial_points(data = elephants_df,\n                               latitude_col = \"location-lat\",\n                               longitude_col = \"location-long\")\n\n\n\n\n\nImport the spatial boundaries for Kruger National Park to serve as a basemap for the elephant tracks\n\n\n# set bbox to approximate boundaries of Kruger NP\nkruger &lt;- osmdata::opq(bbox = c(16, -35, 33, -22)) %&gt;%\n          add_osm_feature(key = \"boundary\", \n                          value = \"protected_area\") %&gt;%\n          add_osm_feature(key = \"name\", \n                          value = \"Kruger National Park\") %&gt;%\n          osmdata_sf()\n\n# subset imported list of elements to just the geodataframe\nkruger_bounds &lt;- kruger$osm_multipolygons\n\n\nProduce a map of elephant observations to visualize the individual movement of elephants\n\n\n\nShow the code\ntitle = \"Elephant Observations\\nKruger National Park\\n2007-2009\"\n\n# basemap: Kruger National Park geometry\ntm_shape(kruger_bounds) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"black\") +\n  # overlay all elephant GPS locations \n  tm_shape(elephants) +\n  tm_dots(col = \"individual-local-identifier\",\n          palette = \"Set3\",\n          # small dots to distinguish between individuals\n          size = 0.001,\n          border.col = \"black\",\n          title = \"Individual\") +\n  tm_layout(bg.color = \"darkgrey\",\n            legend.bg.color = \"white\",\n            title = title,\n            frame = TRUE,\n            title.position = c(\"center\", \"top\"),\n            title.size = 1.2,\n            title.fontface = \"bold\",\n            legend.frame = TRUE,\n            legend.outside = TRUE,\n            legend.position = c(0.08, 0.3),\n            legend.title.size = 0.8,\n            legend.text.size = 0.6) +\n  # add compass with only North arrow\n  tm_compass(position = c(0.0, 0.01),\n             show.labels = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nElephants in Kruger National Park fitted with GPS collars are primarily located in the southern part of the park. The clustered tracks imply that this species is highly gregarious. Tracks that expand beyond park boundaries, notably on the western side, imply that the elephants could be seeking resources such as food or water outside of the park. Alternatively, error in the GPS location for these points could explain this perceived movement.\nFuther analysis steps could include:\n\nzooming in further to the realized niche\npartitioning the data temporally\nusing metadata to advise how to subset the data to view fewer elephant locations at a time\noverlaying other vector or raster data layers such as surface water and vegetation to understand if the elephants’ movement is driven by these resources\n\n\n\n\n\n\n\nData\nCitation\nLink\n\n\n\n\nElephant observations\nSlotow R, Thaker M, Vanak AT. 2019. Data from: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water. Movebank Data Repository. https://doi.org/10.5441/001/1.403h24q5\nMoveBank Repository: Fine-scale tracking of ambient temperature and movement reveals shuttling behavior of elephants to water"
  },
  {
    "objectID": "course-materials/week9.html#class-materials",
    "href": "course-materials/week9.html#class-materials",
    "title": "Week 9: Land Cover Classification",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nImage classifiction: unsupervised and supervised approaches\n\n\n Lab\nLandcover classification with random forests\n\n\n No Discussion Section"
  },
  {
    "objectID": "course-materials/week9.html#assignment-reminders",
    "href": "course-materials/week9.html#assignment-reminders",
    "title": "Week 9: Land Cover Classification",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 9)\n11/25/2024\n11/25/2024\n\n\nHW\nHomework Assignment #4\n11/11/2024\n11/30/2024\n\n\nPR\nPortfolio Repository\n11/11/2024\n12/07/2024"
  },
  {
    "objectID": "course-materials/discussions/week10-discussion.html",
    "href": "course-materials/discussions/week10-discussion.html",
    "title": "Week 10: Discussion Section",
    "section": "",
    "text": "Let’s load the necessary packages:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(rspatial)\n\n\ndf &lt;- sp_data('precipitation')"
  },
  {
    "objectID": "course-materials/discussions/week10-discussion.html#get-started",
    "href": "course-materials/discussions/week10-discussion.html#get-started",
    "title": "Week 10: Discussion Section",
    "section": "",
    "text": "Let’s load the necessary packages:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(rspatial)\n\n\ndf &lt;- sp_data('precipitation')"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n# Read Santa Barbara PA vectors\nsb_protected_areas &lt;- read_sf(here::here(\"data\", \"cpad_super_units_sb.shp\")) %&gt;% \n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's city boundaries vector\nsb_city_boundaries &lt;- read_sf(here::here(\"data\", \"sb_city_boundaries_2003.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's county boundaries vector\nsb_county_boundary &lt;- read_sf(here::here(\"data\", \"sb_county_boundary_2020.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read iNaturalist bird observations from 2020-2024\naves &lt;- read_sf(here::here(\"data\", \"aves_observations_2020_2024.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\nApproach with spatial subset:\n\nSpatially subset the PA geometries to only those with bird observations\n\n\naves_PA_subset &lt;- sb_protected_areas[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_subset) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_subset)\n\n[1] 35\n\n\nApproach with a spatial join:\n\nAppend the Protected Area geometries to the bird observation geometries\n\n\naves_PA_join &lt;- st_join(aves, sb_protected_areas)\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_join) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_join)\n\n[1] 500\n\n\nAnd try adding a 5 km buffer around the protected areas:\n\n# Check if units are in meters\nst_crs(sb_protected_areas)$units\n\n[1] \"m\"\n\n# Create 5000m buffer around PAs\nPA_buffer_5km &lt;- st_buffer(sb_protected_areas, dist = 5000)\n\n# Subset the buffered PA geoms to only those with bird observations\naves_buffer_subset &lt;- PA_buffer_5km[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_buffer_subset) +\n  tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_buffer_subset)\n\n[1] 327"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-bird-observations-within-santa-barbaras-pas",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-bird-observations-within-santa-barbaras-pas",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n# Read Santa Barbara PA vectors\nsb_protected_areas &lt;- read_sf(here::here(\"data\", \"cpad_super_units_sb.shp\")) %&gt;% \n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's city boundaries vector\nsb_city_boundaries &lt;- read_sf(here::here(\"data\", \"sb_city_boundaries_2003.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read Santa Barbara's county boundaries vector\nsb_county_boundary &lt;- read_sf(here::here(\"data\", \"sb_county_boundary_2020.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\n# Read iNaturalist bird observations from 2020-2024\naves &lt;- read_sf(here::here(\"data\", \"aves_observations_2020_2024.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\nApproach with spatial subset:\n\nSpatially subset the PA geometries to only those with bird observations\n\n\naves_PA_subset &lt;- sb_protected_areas[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_subset) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_subset)\n\n[1] 35\n\n\nApproach with a spatial join:\n\nAppend the Protected Area geometries to the bird observation geometries\n\n\naves_PA_join &lt;- st_join(aves, sb_protected_areas)\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_PA_join) +\n    tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_PA_join)\n\n[1] 500\n\n\nAnd try adding a 5 km buffer around the protected areas:\n\n# Check if units are in meters\nst_crs(sb_protected_areas)$units\n\n[1] \"m\"\n\n# Create 5000m buffer around PAs\nPA_buffer_5km &lt;- st_buffer(sb_protected_areas, dist = 5000)\n\n# Subset the buffered PA geoms to only those with bird observations\naves_buffer_subset &lt;- PA_buffer_5km[aves, ]\n\ntm_shape(sb_county_boundary) +\n  tm_fill() +\n  tm_shape(sb_protected_areas) +\n    tm_borders(lwd = 1, col = \"#fb8500\") +\n    tm_fill(col = \"#fb8500\", alpha = 0.2) +\n  tm_shape(aves_buffer_subset) +\n  tm_dots(col = \"#023047\") +\n  tm_layout(main.title = \"Birds in Santa Barbara\\nCounty Protected Areas\",\n            main.title.size = 1)\n\n\n\n\n\n\n\nnrow(aves_buffer_subset)\n\n[1] 327"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-pas-within-15-km-of-goleta",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-pas-within-15-km-of-goleta",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "Find PAs within 15 km of Goleta",
    "text": "Find PAs within 15 km of Goleta\n\n# Subset SB county to Goleta\ngoleta &lt;- sb_city_boundaries %&gt;%\n  dplyr::filter(NAME == \"Goleta\")\n\n# Create 15km buffer around Goleta\nst_crs(goleta)$units\n\n[1] \"m\"\n\ngoleta_buffer_15km &lt;- st_buffer(goleta, dist = 15000)\n\n# Explore the different outputs with different spatial operations\ngoleta_PAs_within &lt;- st_within(sb_protected_areas, goleta_buffer_15km)\ngoleta_PAs_intersect &lt;- st_intersects(sb_protected_areas, goleta_buffer_15km)\ngoleta_PAs_intersection &lt;- st_intersection(sb_protected_areas, goleta_buffer_15km)\n\n# Check class\nclass(goleta_PAs_intersect) == class(goleta_PAs_intersection)\n\n[1] FALSE FALSE FALSE FALSE\n\n# Compute distance-based join\ngoleta_PAs_join &lt;- st_join(sb_protected_areas, goleta, st_is_within_distance, dist = 15000)\n\n# Print the number of observations included in outputs\nnrow(goleta_PAs_intersection)\n\n[1] 185\n\nnrow(goleta_PAs_join)\n\n[1] 369"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-distance-between-goleta-and-dangermond-preserve",
    "href": "course-materials/discussions/answer-keys/week3-discussion-answerKey.html#find-distance-between-goleta-and-dangermond-preserve",
    "title": "Week 3: Discussion Section - Answer Key",
    "section": "Find Distance between Goleta and Dangermond Preserve",
    "text": "Find Distance between Goleta and Dangermond Preserve\n\n# Subset PA to Dangermond Preserve\ndangermond &lt;- sb_protected_areas %&gt;%\n  dplyr::filter(UNIT_NAME == \"Jack and Laura Dangermond Preserve\")\n\n# Compute the distance between geometries edges, output as a matrix\ndanger_dist &lt;- st_distance(goleta, dangermond)\n\n# Calculate the geometric center\ndangermond_centroid &lt;- st_centroid(dangermond)\ngoleta_centroid &lt;- st_centroid(goleta)\n\ndanger_dist_centroid &lt;- st_distance(goleta_centroid, dangermond_centroid)\n\n# Check if the distance matrices are equal\ndanger_dist == danger_dist_centroid\n\n      [,1]\n[1,] FALSE"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\n# import all data files for Easter Island (3 vectors, 1 raster)\nei_points &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_points.gpkg\"))\nvolcanoes &lt;- subset(ei_points, type == \"volcano\")\nei_elev &lt;- stars::read_stars(here::here(\"data\", \"easter_island\", \"ei_elev.tif\"))\nei_borders &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\nei_roads &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))\n\n\n# first plot the elevation raster\ntm_shape(ei_elev) +\n  tm_raster(style = \"cont\",                # continuous values for this layer\n            palette = \"-RdYlGn\",           # reversed red-yellow-green continuous palette\n            title = \"Elevation (m asl)\") + # legend title with units\n  # add vector: Easter Island vector in default gray\n  tm_shape(ei_borders) +\n    tm_borders() +\n  # add vector: road lines in default black\n  tm_shape(ei_roads) +\n    tm_lines(lwd = \"strokelwd\", # line width depends on attribute value \n           legend.lwd.show = FALSE) +\n  # add vector: volcano points in default gray\n  tm_shape(volcanoes) +\n    tm_symbols(shape = 24,                       # triangle\n             size = \"elevation\",                 # symbol size depends on attribute value \n             title.size = \"Volcanoes (m asl)\") + # legend title with units\n  # general map layout options\n  tm_layout(main.title = \"Easter Island\",\n            bg.color = \"lightblue\") +            # background color for map (ocean)\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(show.labels = 1)                    # only show North label on compass"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-tmap",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-tmap",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\n# import all data files for Easter Island (3 vectors, 1 raster)\nei_points &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_points.gpkg\"))\nvolcanoes &lt;- subset(ei_points, type == \"volcano\")\nei_elev &lt;- stars::read_stars(here::here(\"data\", \"easter_island\", \"ei_elev.tif\"))\nei_borders &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\nei_roads &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))\n\n\n# first plot the elevation raster\ntm_shape(ei_elev) +\n  tm_raster(style = \"cont\",                # continuous values for this layer\n            palette = \"-RdYlGn\",           # reversed red-yellow-green continuous palette\n            title = \"Elevation (m asl)\") + # legend title with units\n  # add vector: Easter Island vector in default gray\n  tm_shape(ei_borders) +\n    tm_borders() +\n  # add vector: road lines in default black\n  tm_shape(ei_roads) +\n    tm_lines(lwd = \"strokelwd\", # line width depends on attribute value \n           legend.lwd.show = FALSE) +\n  # add vector: volcano points in default gray\n  tm_shape(volcanoes) +\n    tm_symbols(shape = 24,                       # triangle\n             size = \"elevation\",                 # symbol size depends on attribute value \n             title.size = \"Volcanoes (m asl)\") + # legend title with units\n  # general map layout options\n  tm_layout(main.title = \"Easter Island\",\n            bg.color = \"lightblue\") +            # background color for map (ocean)\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(show.labels = 1)                    # only show North label on compass"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-an-interactive-map-of-easter-island-with-tmap",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-an-interactive-map-of-easter-island-with-tmap",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "Create an Interactive Map of Easter Island with tmap",
    "text": "Create an Interactive Map of Easter Island with tmap\n\n# same code as static map above\nei_map &lt;- tm_shape(ei_elev) +\n  tm_raster(style = \"cont\",\n            palette = \"-RdYlGn\",\n            title = \"Elevation (m asl)\") +\n  tm_shape(ei_borders) + \n  tm_borders() +\n  tm_shape(ei_roads) + \n  tm_lines(lwd = \"strokelwd\", \n           legend.lwd.show = FALSE) +\n  tm_shape(volcanoes) +\n  tm_symbols(shape = 24, \n             size = \"elevation\",\n             title.size = \"Volcanoes (m asl)\") +\n  tm_layout(main.title = \"Easter Island\",\n            bg.color = \"lightblue\") +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(show.labels = 1)\n\n# convert default static map to interactive map \ntmap_mode(\"view\")\n\nei_map"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-ggplot2",
    "href": "course-materials/discussions/answer-keys/week1-discussion-answerKey.html#create-a-map-of-easter-island-with-ggplot2",
    "title": "Week 1: Discussion Section - Answer Key",
    "section": "Create a Map of Easter Island with ggplot2",
    "text": "Create a Map of Easter Island with ggplot2\n\n# extract lat & long from geom column \nvolcanoes_point &lt;- volcanoes %&gt;%\n  mutate(lon = unlist(map(volcanoes$geom,1)), # longitude = first component (x)\n         lat = unlist(map(volcanoes$geom,2))) # latitude = second component (y)\n\nggplot() +\n  # first add Easter Isand borders\n  geom_sf(data = ei_borders, color = \"#212529\") +\n  # add elevation raster\n  geom_stars(data = ei_elev) +\n  scale_fill_distiller(name = \"Elevation (m asl)\", # legend title with units\n                       palette = \"RdYlGn\",         # red-yellow-green palette (no need to reverse)\n                       na.value = \"lightblue\") +  # set NA color (background ocean)\n  # add road vector\n  geom_sf(data = ei_roads, color = \"#343a40\") +\n  # add volcano vector\n  geom_point(data = volcanoes_point, \n             aes(x = lon, y = lat, \n                 size = elevation), # point size depends on attribute value\n             shape = 17,            # triangle\n             color = \"#22577a\") +\n  scale_size_continuous(name = \"Volcanoes (m asl)\") +       # legend title with units\n  ggspatial::annotation_north_arrow(location = \"br\",        # bottom right\n                                    which_north = \"true\") + # point to north pole\n  ggspatial::annotation_scale(location = \"bl\",              # bottom left\n                              width_hint = 0.5) +           # proportion of map area the scalebar should occupy\n  labs(title = \"Easter Island\") +\n  theme_minimal()"
  },
  {
    "objectID": "course-materials/discussions/week8-discussion.html",
    "href": "course-materials/discussions/week8-discussion.html",
    "title": "Week 8: Discussion Section",
    "section": "",
    "text": "Let’s load the necessary packages:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\n\n\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nAnd create a raster:\n\nelev &lt;- terra::rast(nrows = 6, \n                    ncols = 6, \n                    resolution = 0.5,\n                    xmin = -1.5, xmax = 1.5, \n                    ymin = -1.5, ymax = 1.5,\n                    vals = 1:36)\n\nNow, to meet our learning objectives, your task:\n\nPractice filtering a raster\n\n\nSet all cells to NA where elev &lt; 20\n\n\nPractice combining (or unioning) geometries\n\n\nCombine all geometries without resolving borders of nc\nFind union of all geometries\nRemove geometries\n\n\nCreate a new geometry that is the difference of two geometries\n\n\nHint: Plot the difference! Counties should be missing!"
  },
  {
    "objectID": "course-materials/discussions/week8-discussion.html#get-started",
    "href": "course-materials/discussions/week8-discussion.html#get-started",
    "title": "Week 8: Discussion Section",
    "section": "",
    "text": "Let’s load the necessary packages:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\n\n\nnc &lt;- sf::st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nAnd create a raster:\n\nelev &lt;- terra::rast(nrows = 6, \n                    ncols = 6, \n                    resolution = 0.5,\n                    xmin = -1.5, xmax = 1.5, \n                    ymin = -1.5, ymax = 1.5,\n                    vals = 1:36)\n\nNow, to meet our learning objectives, your task:\n\nPractice filtering a raster\n\n\nSet all cells to NA where elev &lt; 20\n\n\nPractice combining (or unioning) geometries\n\n\nCombine all geometries without resolving borders of nc\nFind union of all geometries\nRemove geometries\n\n\nCreate a new geometry that is the difference of two geometries\n\n\nHint: Plot the difference! Counties should be missing!"
  },
  {
    "objectID": "course-materials/discussions/week8-discussion.html#answer-key",
    "href": "course-materials/discussions/week8-discussion.html#answer-key",
    "title": "Week 8: Discussion Section",
    "section": "Answer Key",
    "text": "Answer Key\n\nFilter a Raster\n\nelev[elev &lt; 20] &lt;- NA\n\n\n\nCombine Geometries\n\nnc_combine &lt;- sf::st_combine(nc)\nplot(nc_combine)\n\nnc_union &lt;- sf::st_union(nc)\nplot(nc_union)\n\ncounties &lt;- nc %&gt;%\n  dplyr::filter(NAME %in% c(\"Ashe\", \"Alleghany\", \"Surry\")) %&gt;%\n  sf::st_union()\n\n# Plot counties on top of NC unioned\nggplot() +\n  geom_sf(data = nc_union, fill = \"grey\", color = \"transparent\") +\n  geom_sf(data = counties, fill = \"black\", color = \"transparent\")\n\n\n\nCreate a New Geometry\n\nnc_difference &lt;- sf::st_difference(nc_union, counties)\nnc_disjoint &lt;- sf::st_disjoint(nc_union, counties)\nnc_intersection &lt;- sf::st_intersection(nc_union, counties)\n\nggplot() + \n  geom_sf(data = nc_difference, fill = \"grey\", color = \"transparent\") +\n  geom_sf(data = nc_intersection, fill = \"grey\", color = \"transparent\")"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html",
    "href": "course-materials/discussions/week3-discussion.html",
    "title": "Week 3: Discussion Section",
    "section": "",
    "text": "Explore topological relationships with sf functions: st_intersects(), st_intersection(), st_within(), etc.\nExplore distance relationships with sf functions: st_distance(), st_within_distance(), and st_buffer()\nLearn about spatial and distance-based joins\nPractice writing error/warning messages and unit tests to diagnose outputs"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week3-discussion.html#learning-objectives",
    "title": "Week 3: Discussion Section",
    "section": "",
    "text": "Explore topological relationships with sf functions: st_intersects(), st_intersection(), st_within(), etc.\nExplore distance relationships with sf functions: st_distance(), st_within_distance(), and st_buffer()\nLearn about spatial and distance-based joins\nPractice writing error/warning messages and unit tests to diagnose outputs"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#get-started",
    "href": "course-materials/discussions/week3-discussion.html#get-started",
    "title": "Week 3: Discussion Section",
    "section": "2. Get Started",
    "text": "2. Get Started\n\nCreate an .Rproj as your version controlled project for Week 3\nCreate a Quarto document inside your .Rproj\nDownload this data folder from Google Drive and move it inside your .Rproj\nLoad all necessary packages and read spatial objects\n\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\nsb_protected_areas &lt;- read_sf(here::here(\"data\", \"cpad_super_units_sb.shp\")) %&gt;% \n  st_transform(\"ESRI:102009\")\n\nsb_city_boundaries &lt;- read_sf(here::here(\"data\", \"sb_city_boundaries_2003.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\nsb_county_boundary &lt;- read_sf(here::here(\"data\", \"sb_county_boundary_2020.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")\n\naves &lt;- read_sf(here::here(\"data\", \"aves_observations_2020_2024.shp\")) %&gt;%\n  st_transform(\"ESRI:102009\")"
  },
  {
    "objectID": "course-materials/discussions/week3-discussion.html#your-task",
    "href": "course-materials/discussions/week3-discussion.html#your-task",
    "title": "Week 3: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nYou will be wokring with the following datasets:\n\nSanta Barbara County’s City Boundaries (Santa Barbara County)\nCalifornia Protected Areas Database (CPAD)\niNaturalist Research-grade Observations, 2020-2024 (via rinat)\n\nNow, to meet this week’s learning objectives, your task:\n\nFind how many bird observations are within protected areas in Santa Barbara County\n\n\nShow the different outputs from a spatial subset and a spatial join\nBonus Challenge: Try it out with a 5 km buffer around the protected areas too!\n\n\nFind the protected areas within 15 km of a city in Santa Barbara County\n\n\nHint: Use dplyr::filter() to select a city from sb_city_boundaries\nExplore the different outputs with st_intersects(), st_intersection(), and st_within()\nPractice a distance-based join with st_is_within_distance()\n\n\nFind the distance between your city of choice and a protected area of your choice\n\n\nNote: st_distance() finds the distance between the geometries’ edges"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html",
    "href": "course-materials/discussions/week5-discussion.html",
    "title": "Week 5: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from Geocomputation with R by Robin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week5-discussion.html#learning-objectives",
    "title": "Week 5: Discussion Section",
    "section": "1. Learning Objectives",
    "text": "1. Learning Objectives\n\nUse terra functions aggregate() and resample() to create a new raster\nUse terra functions as.polygons() to convert a raster to a vector of polygons"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#get-started",
    "href": "course-materials/discussions/week5-discussion.html#get-started",
    "title": "Week 5: Discussion Section",
    "section": "2. Get Started",
    "text": "2. Get Started\n\nCreate an .Rproj as your version controlled project for Week 5\nCreate a Quarto document inside your .Rproj\nLoad all necessary packages and read spatial objects\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\n\n\n# Load raster data representing grain sizes with the three classes clay, silt and sand\ngrain &lt;- terra::rast(system.file(\"raster/grain.tif\", package = \"spData\"))"
  },
  {
    "objectID": "course-materials/discussions/week5-discussion.html#your-task",
    "href": "course-materials/discussions/week5-discussion.html#your-task",
    "title": "Week 5: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nSubset points higher than 3100 meters in nz_height (from spData) and create a template raster with rast(), where the resolution is 3 km x 3 km for the extent of the subset dataset\n\n\nCount numbers of the highest points in each grid cell\nFind the maximum elevation in each grid cell\n\n\nWith the previous raster, complete the following:\n\n\nAggregate the raster that counts the highest points in New Zealand/Aotearoa\nReduce its geographic resolution by half, such that cells are 6 x 6 km\nPlot the result\nResample back to the original resolution of 3 km x 3 km\n\n\nPolygonize grain and filter to only keep squares that represent clay"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html",
    "href": "course-materials/discussions/week2-discussion.html",
    "title": "Week 2: Discussion Section",
    "section": "",
    "text": "Use sf::st_read() to read multiple vector data types\nRetreive the CRS of a vector object with sf::st_crs()\nTransform CRS and match across all vector data types with sf::st_transform()\nPerform dplyr attribute manipulations (e.g. filter(), mutate(), select())\n\n\n\n\n\n\n\nMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week2-discussion.html#learning-objectives",
    "title": "Week 2: Discussion Section",
    "section": "",
    "text": "Use sf::st_read() to read multiple vector data types\nRetreive the CRS of a vector object with sf::st_crs()\nTransform CRS and match across all vector data types with sf::st_transform()\nPerform dplyr attribute manipulations (e.g. filter(), mutate(), select())\n\n\n\n\n\n\n\nMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#get-started",
    "href": "course-materials/discussions/week2-discussion.html#get-started",
    "title": "Week 2: Discussion Section",
    "section": "2. Get Started",
    "text": "2. Get Started\n\nCreate an .Rproj as your version controlled project for Week 2\nCreate a Quarto document inside your .Rproj\nDownload this folder from Google Drive and move it inside your .Rproj\nLoad all necessary packages\n\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)"
  },
  {
    "objectID": "course-materials/discussions/week2-discussion.html#your-task",
    "href": "course-materials/discussions/week2-discussion.html#your-task",
    "title": "Week 2: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nYou will be wokring with the following datasets:\n\nColombia’s Terrestrial Ecosystems (The Nature Conservancy/NatureServe)\nColombia’s Roads (Esri)\nBird Observations (DATAVES)\n\nNow, to meet this week’s learning objectives, your task:\n\nRead in the data for Colombia’s ecoregions, roads, and bird observations\n\n\nUse st_read() to read vector data (e.g., .shp, .gdb)\nUse rename() or mutate() to rename the columns decimal_longitude and decimal_latitude to long and lat in aves and sf::st_as_sf() to convert it into an sf object\n\nHint: To convert a table into a vector object, you can use st_as_sf() but remember to check the class() of an object first!\n\n\n\nCheck class() of all vector objects (including the converted aves) and use sf::st_geometry_type() to check the geometry type\nUse filter() to select a macro region of interest from N1_MacroBi in Colombia’s ecoregions dataset and save as a new vector data\n\n\nCheck class() of the new vector data\nPlot the new vector data using tmap\n\n\nUse st_crs() to retrieve CRS of all vector objects and assign a new CRS\n\n\nBonus Challenge: Check units of your object with st_crs()$units\nCheck CRS of all vector objects with st_crs()\nst_crs() &lt;- NA is a brute force way to remove a CRS, instead:\n\nFor the bird observations dataset, extract the longitude and latitude from the geometry column and use sf::st_drop_geometry()\nConvert long and lat into a geometry again with st_as_sf() to obtain a proper sf data frame\n\n\n\nLet’s bring all vector data types together\n\n\nCheck that the CRS of the ecoregions and roads datasets match\nTransform CRS of the bird observations data using sf::st_transform() to match with the other datasets\nUse tmap to plot the ecoregions, roads, and bird observations together"
  },
  {
    "objectID": "course-materials/labs/week5.html",
    "href": "course-materials/labs/week5.html",
    "title": "Week 5: Lab",
    "section": "",
    "text": "In this lab we’ll explore operations that rely on interactions between vector and raster datasets, including how to convert raster data into vector data."
  },
  {
    "objectID": "course-materials/labs/week5.html#set-up",
    "href": "course-materials/labs/week5.html#set-up",
    "title": "Week 5: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nFirst, we’ll load all relevant packages.\n\nlibrary(sf) # vector handling\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data\nlibrary(viridisLite)\n\nToday we’re heading back to Zion National Park in Utah to explore the interactions between vector and raster data.\n\n\n\n\n\n\n\n\n\n\nPhoto from Unsplash\n\nWe’ll load the following data from the {spDataLarge} package:\n\nsrtm.tif: remotely sensed elevation estimates (raster data)\nzion.gpkg: boundary of Zion National Park (vector data)\n\n\n# load raster dataset\nelevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n# load vector dataset\nboundary &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\n\n\n\n\n\n\n\nRemember to check the CRS!\n\n\n\nWhenever we work with multiple spatial datasets, we need check that the coordinate reference systems match. If they don’t, we need to transform one to match the other.\n\n# check if coordinate reference systems match\nif(crs(elevation) == crs(boundary)) {\n  print(\"Coordinate reference systems match\")\n} else{\n  warning(\"Updating coordinate reference systems to match\")\n  # transform data to match\n  boundary &lt;- st_transform(boundary, st_crs(elevation))\n}\n\nWarning: Updating coordinate reference systems to match\n\n\n\n\n\n\nCode\ntm_shape(elevation) +\n  tm_raster(title = \"Elevation (meters)\") +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "course-materials/labs/week5.html#raster-cropping",
    "href": "course-materials/labs/week5.html#raster-cropping",
    "title": "Week 5: Lab",
    "section": "2. Raster cropping",
    "text": "2. Raster cropping\nMany geographic data projects involve integrating data from many different sources, such as remote sensing images (rasters) and administrative boundaries (vectors). Often the extent of input raster datasets is larger than the area of interest. In this case, raster cropping and masking are useful for unifying the spatial extent of input data. Both operations reduce object memory use and associated computational resources for subsequent analysis steps and may be a necessary preprocessing step when creating attractive maps involving raster data.\nFirst, let’s crop the extent of the elevation raster to match the extent of Zion’s boundaries. Through this process, we eliminate grid cells that fall outside the extent of the park and reduce the size of the raster. To do so, we use the terra::crop() function.\n\n# crop raster to extent of vector object\nelevation_cropped &lt;- crop(elevation, boundary)\n\nBeyond matching the extent, we can also set the values of raster cells outside of the boundaries or the park to NA using terra::mask().\n\n# mask raster based on vector object\n# (cells outside of vector are converted to NA)\nelevation_masked &lt;- mask(elevation, boundary)\n\nOften, we will want to combine both cropping and masking to reduce the size of the raster as much as possible.\n\n# crop and mask raster\nelevation_final &lt;- mask(elevation_cropped, boundary)\n\nIn some cases, we may want to mask the raster cells inside of the boundaries (i.e. assign cells inside the park to NA). We can do so with terra::mask() by setting the argument inverse = TRUE.\n\n# mask raster based on vector object\n# (cells inside of vector are converted to NA)\nelevation_inv_masked &lt;- mask(elevation_cropped, boundary, inverse = TRUE)\n\n\n\nCode\nmap1 &lt;- tm_shape(elevation) +\n  tm_raster(legend.show = FALSE) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_layout(main.title = \"original\")\n\nmap2 &lt;- tm_shape(elevation_cropped) +\n  tm_raster(legend.show = FALSE) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_layout(main.title = \"cropped\")\n\nmap3 &lt;- tm_shape(elevation_masked) +\n  tm_raster(legend.show = FALSE) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_layout(main.title = \"masked\")\n\nmap4 &lt;- tm_shape(elevation_final) +\n  tm_raster(legend.show = FALSE) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_layout(main.title = \"cropped & masked\")\n\nmap5 &lt;- tm_shape(elevation_inv_masked) +\n  tm_raster(legend.show = FALSE) +\ntm_shape(boundary) +\n  tm_borders(lwd = 2) +\ntm_layout(main.title = \"inverse mask\")\n\ntmap_arrange(map1, map2, map3, map4, map5, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week5.html#raster-vectorization",
    "href": "course-materials/labs/week5.html#raster-vectorization",
    "title": "Week 5: Lab",
    "section": "3. Raster vectorization",
    "text": "3. Raster vectorization\nThere are several ways to convert raster data into vector. The most common, and straightforward, is converting raster grid cells into polygons. For more examples, check out Geocomputation with R.\nWe could simply convert all grid cells into polygons, but it may be more helpful to create polygons based on some condition\n\n\n\n\n\n\nTip for HW3\n\n\n\nThe following example is relevant to homework assignment 3!\n\n\nIn this example, we’ll select grid cells higher than 2000 meters by masking the elevation raster. We’ll then convert these grid cells into polygons using the terra::as.polygons() function and turn this into a sf object.\n\nelevation_mask &lt;- elevation_final\nelevation_mask[elevation_mask &lt; 2000] &lt;- NA\n\n\n\nelevation_mask_poly &lt;- as.polygons(elevation_mask) %&gt;% \n  st_as_sf()\n\n\n\nCode\nmap1 &lt;- tm_shape(elevation_mask) +\n  tm_raster() +\n  tm_layout(legend.outside = TRUE,\n            main.title = \"masked raster\")\n\nmap2 &lt;- tm_shape(elevation_mask_poly) +\n  tm_polygons() +\n  tm_layout(main.title = \"vectorized raster\")\n\ntmap_arrange(map1, map2, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week3.html",
    "href": "course-materials/labs/week3.html",
    "title": "Week 3: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of spatial and geometry operations on vector data in R using the sf package. We’ll be working with data representing the heigh points of New Zealand."
  },
  {
    "objectID": "course-materials/labs/week3.html#set-up",
    "href": "course-materials/labs/week3.html#set-up",
    "title": "Week 3: Lab",
    "section": "1. Set Up",
    "text": "1. Set Up\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(spData)"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-subsetting-filtering",
    "href": "course-materials/labs/week3.html#spatial-subsetting-filtering",
    "title": "Week 3: Lab",
    "section": "2. Spatial subsetting (filtering)",
    "text": "2. Spatial subsetting (filtering)\nWhen working with tabular data, we have frequently found it useful to subset the data.frame we are working with based on some condition using dplyr::filter().\n\n\n\n\n\n\n\n\n\n\nArtwork by Allison Horst\n\nFor example, last week we saw how we could filter to countries whose average life expectancy is greater than 80 years old using the following code:\n\nworld %&gt;%\n  filter(lifeExp &gt;= 80)\n\nSimilarly, we might want to filter data based on its spatial relationships. In this case, we use spatial subsetting which is the process of converting a spatial object into a new object containing only the spatial features that relate in space to another object. This is analogous the attribute subsetting that we covered last week (example above).\n\nTopological relationships\nWhen filtering based on attributes, we use conditions (for example, lifeExp &gt;= 80). In spatial subsetting, we use the relationships of objects to each other in space (topological relationships). These relationships are based on mathematical relationships, but can be more easily understood from visualizing them. The figure below shows how each relationship is satisfied.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\n\n\n\n\n\n\nst_intersects() and st_disjoint()\n\n\n\nNote that st_intersects()is a “catch-all” that contains the following relationships:\n\nst_touches()\nst_overlaps()\nst_contains() and st_contains_properly()\nst_covers() and st_covered_by()\nst_within()\n\nst_disjoint() is the opposite of st_intersects()\n\n\n\n\nExamples\nThere are many ways to spatially subset in R, so we will explore a few.\nAs an example we’ll work with the following two datasets from the spData package:\n\nnz: polygons representing the 16 regions of New Zealand\nnz_height: top 101 heighest points in New Zealand\n\nWe’ll explore by trying to find all the high points in the region of Canterbury (shown in dark grey).\n\n\nCode\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(col = \"darkgrey\") +\n  tm_shape(nz_height) +\n  tm_dots(col = \"red\")\n\n\n\n\n\n\n\n\n\n\nBracket subsetting\nLike attribute subsetting, the command x[y, ] (equivalent to nz_height[canterbury, ]) subsets features of a target x using the contents of a source object y. Instead of y being a vector of class logical or integer, however, for spatial subsetting both x and y must be geographic objects. Specifically, objects used for spatial subsetting in this way must have the class sf or sfc: both nz and nz_height are geographic vector data frames and have the class sf, and the result of the operation returns another sf object representing the features in the target nz_height object that intersect with (in this case high points that are located within) the Canterbury region.\n\n# first filter to the region of Canterbury\ncanterbury &lt;- nz %&gt;%\n  filter(Name == \"Canterbury\")\n\n# subset nz_heights to just the features that intersect Canterbury\nc_height1 &lt;- nz_height[canterbury, ]\n\nBy default bracket subsetting will filter to features in x that intersect features in y. However, we can use other topological relationships by changing options.\n\nnz_height[canterbury, , op = st_disjoint]\n\n\n\nst_filter()\nThe sf package also includes the function st_filter() which is analogous to dplyr::filter(). Using st_filter() we can perform spatial subsetting in the same format as using dplyr commands. The .predicate = argument allows us to define which topological relationship we would like to filter by (e.g. st_intersects(), st_disjoint()).\nThe results from this method are the identical to the method above.\n\n# subset to the features in Cantebury\nc_height2 &lt;- nz_height %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects) # define the topological relationship\n\n\n\nTopological operators (st_intersects())\nThe previous two methods either by default or explicitly use the argument st_intersects. All topological relationships have their own topological operators which are functions that evaluate whether or not features meet the specified condition (e.g. st_intersects()). These operators can be used for spatial subsetting, but are more complicated to use.\nThe output of st_intersects() and other topological operators is a sparse geometry binary predicate list (yikes!) that’s a list that defines whether or not each feature in x intersects y.\nThis can be converted into logical vector of TRUE and FALSE values which can then be used for filtering.\n\n# sparse binary predicate list\nnz_height_sgbp &lt;- st_intersects(x = nz_height, y = canterbury)\nnz_height_sgbp\n\nSparse geometry binary predicate list of length 101, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: 1\n 6: 1\n 7: 1\n 8: 1\n 9: 1\n 10: 1\n\n# convert to logical vector\nnz_height_logical &lt;- lengths(nz_height_sgbp) &gt; 0\n\n# filter based on logical vector\nc_height3 = nz_height[nz_height_logical, ]\n\nNow let’s plot results from all three methods to confirm they gave the same results.\n\n\nCode\nmap1 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(col = \"darkgrey\") +\n  tm_shape(c_height1) +\n  tm_dots(col = \"red\") +\ntm_layout(title = \"Bracket subsetting\")\n\nmap2 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(col = \"darkgrey\") +\n  tm_shape(c_height1) +\n  tm_dots(col = \"red\") +\ntm_layout(title = \"st_filter()\")\n\nmap3 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(col = \"darkgrey\") +\n  tm_shape(c_height3) +\n  tm_dots(col = \"red\") +\ntm_layout(title = \"st_intersects()\")\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nDistance relationships\nThe topological relationships we have been discussing are all binary (features either intersect or don’t). In some cases, it might be helpful to subset based on a distance to a feature. In these cases we can use the st_is_within_distance() to filter features. By default st_is_within_distance() will return a sparse geometry binary predicate list as in st_intersects() above. Instead, we can return a logical by setting sparse = FALSE.\n\n# find heights within 1000 km of Canterbury\nnz_height_logical &lt;- st_is_within_distance(nz_height, canterbury,\n                      dist = units::set_units(1000, \"km\"), # set distance\n                      sparse = FALSE) # return logical vector instead\n\nc_height4 &lt;- nz_height[nz_height_logical, ] # filter based on logical\n\nNow, we should see points appear that do not intersect Caterbury, but are within 1000 km.\n\n\nCode\n# additional high points should appear\ntm_shape(nz) +\n  tm_polygons() +\n  tm_shape(canterbury) +\n  tm_fill(col = \"darkgrey\") +\n  tm_shape(c_height4) +\n  tm_dots(col = \"red\")"
  },
  {
    "objectID": "course-materials/labs/week3.html#spatial-joins",
    "href": "course-materials/labs/week3.html#spatial-joins",
    "title": "Week 3: Lab",
    "section": "3. Spatial joins",
    "text": "3. Spatial joins\nJoins are a common way to link different data sources. Up until now, we have been performing joins using common attributes between data.frames. Last week, we saw that the same joins can be used on sf objects. However, we can also perform joins by using the spatial relationship of datasets.\nFirst, let’s remind ourselves of the different types of joins.\n\n\n\n\n\n\n\n\n\n\nSoftware Carpentry\n\n\nToplogical relationships\nWith spatial data, we can join based on the geometry columns using topological relationships using the st_join() function. By default st_join() will join based on geometries that intersect, but can accommodate other topological relationships by changing the join = argument. By default st_join() performs left joins, but can perform inner joins by setting left = FALSE.\n\n# specify join based on geometries x within y\nst_join(x, y, join = st_within)\n\n# specify inner join\nst_join(x, y, left = FALSE)\n\nLet’s consider the scenario where we would like to know which region each of the highest points is located in. We can left join the nz dataset (polygons of NZ’s regions) onto the nz_height dataset (points of highest points in the county).\n\nnz_height_left_join &lt;- st_join(nz_height, nz) %&gt;%\n  select(id = t50_fid, elevation, region = Name)\n\nhead(nz_height_left_join)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1204143 ymin: 5048309 xmax: 1389460 ymax: 5168749\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n       id elevation     region                geometry\n1 2353944      2723  Southland POINT (1204143 5049971)\n2 2354404      2820      Otago POINT (1234725 5048309)\n3 2354405      2830      Otago POINT (1235915 5048745)\n4 2369113      3033 West Coast POINT (1259702 5076570)\n5 2362630      2749 Canterbury POINT (1378170 5158491)\n6 2362814      2822 Canterbury POINT (1389460 5168749)\n\n\nNow we could use this data to summarize the number of highest point in each region!\n\nnz_height_left_join %&gt;%\n  group_by(region) %&gt;%\n  summarise(n_points = n()) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 7 × 2\n  region            n_points\n* &lt;chr&gt;                &lt;int&gt;\n1 Canterbury              70\n2 Manawatu-Wanganui        2\n3 Marlborough              1\n4 Otago                    2\n5 Southland                1\n6 Waikato                  3\n7 West Coast              22\n\n\n\n\nDistance-based joins\nSimilar to filtering, in some cases we may want to join datasets based on their proximity. Let’s see an example!\nWe’ll use the following two datasets from the spData package:\n\ncycle_hire: points representing cycle hire points across London with information on number of bikes available\ncycle_hire_osm: dataset downloaded from OpenStreetMaps representing cycle hire points across London with information on the capacity of the hire point\n\nIn this example, we would like join the capacity attribute from the cycle_hire_osm dataset to the cycle_hire dataset. Unfortunately it appears that the points from the two datasets do not perfectly align.\n\n# check whether or not points overlap\nif(any(st_intersects(cycle_hire, cycle_hire_osm, sparse = FALSE)) == TRUE){\n  print(\"points overlap\")\n} else{\n  warning(\"points don't overlap\")\n}\n\nWarning: points don't overlap\n\n\n\n\nCode\ntmap_mode(\"view\")\n\ntm_shape(cycle_hire) +\n  tm_symbols(col = \"red\", alpha = 0.2) +\ntm_shape(cycle_hire_osm) +\n  tm_symbols(col = \"blue\", alpha = 0.2)\n\n\n\n\n\n\nWe can join by again using st_join(), but this time including a distance threshold using st_is_within_distance.\n\ncycle_hire_join &lt;- st_join(cycle_hire, cycle_hire_osm,\n                           st_is_within_distance,\n                           dist = units::set_units(20, \"m\")) %&gt;%\n                   select(id, capacity)\n\nhead(cycle_hire_join)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -0.1975742 ymin: 51.49313 xmax: -0.08460569 ymax: 51.53006\nGeodetic CRS:  WGS 84\n  id capacity                     geometry\n1  1        9  POINT (-0.1099705 51.52916)\n2  2       27  POINT (-0.1975742 51.49961)\n3  3       NA POINT (-0.08460569 51.52128)\n4  4       NA  POINT (-0.1209737 51.53006)\n5  5       NA   POINT (-0.156876 51.49313)\n6  6        8  POINT (-0.1442289 51.51812)\n\n\nLet’s build some checks to diagnose the output.\n\nif(nrow(cycle_hire) == nrow(cycle_hire_join)){\n  print(\"join matches original data dimensions\")\n} else {\n  warning(\"join does not match orginal data dimensions\")\n  print(paste(\"cycle_hire has\", nrow(cycle_hire), \"rows\"))\n  print(paste(\"cycle_hire_join has\", nrow(cycle_hire_join), \"rows\"))\n}\n\nWarning: join does not match orginal data dimensions\n\n\n[1] \"cycle_hire has 742 rows\"\n[1] \"cycle_hire_join has 762 rows\"\n\n\nNote that the joined result has more rows than the target data. This is because some of the cycle hire stations in cycle_hire have multiple matches in cycle_hire_osm. Depending on your project, you would need to think about how to resolve this. In this case, we can aggregate the values for the overlapping points by taking the mean.\n\n# aggregate values for single points in cycle_hire\ncycle_hire_join &lt;- cycle_hire_join %&gt;%\n  group_by(id) %&gt;%\n  summarise(capacity = mean(capacity))\n\n# check results\nif(nrow(cycle_hire) == nrow(cycle_hire_join)){\n  print(\"join matches original data dimensions\")\n} else {\n  warning(\"join does not match orginal data dimensions\")\n  print(paste(\"cycle_hire has\", nrow(cycle_hire), \"rows\"))\n  print(paste(\"cycle_hire_join has\", nrow(cycle_hire_join), \"rows\"))\n}\n\n[1] \"join matches original data dimensions\"\n\n\n\n\n4. Spatial aggregation\nAs with aggregating attribute data, spatial data aggregation condenses data: outputs have few rows than inputs. Think about our friend group_by() %&gt;% summarise()! Spatial aggregation is the same.\nLet’s consider the example where we would like to make a map of the mean elevation of high points within each region of NZ. There are several ways to do this, but the first thing we should be thinking is that we will need to retain the geometry column of the NZ regions in order to make a map.\nThe first approach is by leveraging st_join() again. But in this example, we want the nz object to be the target to maintain the geometries we need.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nnz_elevation &lt;- st_join(x = nz, y = nz_height) %&gt;%\n  group_by(Name) %&gt;%\n  summarise(elevation = mean(elevation, na.rm = TRUE))\n\nThe second approah uses the aggregate() function. Although it doesn’t follow the dplyr piping convention we’re used to, aggregate() will come in handy later, so it’s nice to see how it works.\nThe syntax looks slightly different, in this case the argument x is the data we would like to aggregate (nz_height) and the by argument specifies the geometry that you would like to group by. The FUN argument defines the function that you would like to use to aggregate, in this case mean.\n\nnz_elevation &lt;- aggregate(x = nz_height, by = nz, FUN = mean)\n\n\n\nCode\nmap1 &lt;- tm_shape(nz_elevation) +\n  tm_polygons(col = \"elevation\",\n              title = \"Mean elevation (meters)\") +\n  tm_layout(title = \"group_by()\")\n\nmap2 &lt;- tm_shape(nz_elevation) +\n  tm_polygons(col = \"elevation\",\n              title = \"Mean elevation (meters)\") +\n  tm_layout(title = \"aggregate()\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJoining incongruent layers\n\n\n\nAn important consideration when aggregating spatial objects is that geometries are congruent, meaning aggregating zones align with the units being aggregating. This is often the case with administrative boundaries where units are sub-units of one another (e.g. countries &gt; states &gt; counties).\nHowever, in some cases the aggregating zones do not share common borders with the target. This is an issue because it’s not clear how to aggregate underlying data. Areal interpolation overcomes this issue by transferring values to another using algorithms, including simple area weighted approaches.\n\nincongruent &lt;- spData::incongruent\naggregating_zones &lt;- spData::aggregating_zones\n\ntm_shape(incongruent) +\n  tm_polygons(col = \"lightblue\",\n              border.col = \"blue\") +\n  tm_shape(aggregating_zones) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n\n\n\nThe simplest useful method for this is area weighted spatial interpolation, which transfers values from the incongruent object to a new column in aggregating_zones in proportion with the area of overlap: the larger the spatial intersection between input and output features, the larger the corresponding value. This is implemented in st_interpolate_aw(), as demonstrated in the code chunk below.\n\n# select just the value to be aggregated\nincongruent_2 &lt;- incongruent %&gt;%\n  select(value)\n\n# use area-weighted interpolation to aggregrate the \"value\" attribute\naggregating_zones_area_weighted &lt;- st_interpolate_aw(incongruent_2, aggregating_zones, extensive = TRUE)\n\nWarning in st_interpolate_aw.sf(incongruent_2, aggregating_zones, extensive =\nTRUE): st_interpolate_aw assumes attributes are constant or uniform over areas\nof x\n\naggregating_zones_area_weighted$value\n\n[1] 19.61613 25.66872"
  },
  {
    "objectID": "course-materials/labs/week3.html#aggregating",
    "href": "course-materials/labs/week3.html#aggregating",
    "title": "Week 3: Lab",
    "section": "1. Aggregating",
    "text": "1. Aggregating\n\nGeometry unions\nWe may come across situations where we would like to summarize data across several spatial units. For example, summarizing the population across states within regions of the US. Based on our experience with tabular data, we can summarize attributes by using group_by() %&gt;% summarize(). Alternatively we can use the aggregate() function. So far this should look familiar, but if plot the outputs we notice that these functions have also aggregated the underlying geometries.\n\n# load US states\nus_states &lt;- spData::us_states\n\n# summarize total population within each region\nregions1 &lt;- us_states %&gt;%\n  group_by(REGION) %&gt;%\n  summarise(population = sum(total_pop_15, na.rm = TRUE))\n\n# alternative approach\nregions2 &lt;- aggregate(x = us_states[, \"total_pop_15\"], # data and attribute to be aggregated\n                      by = list(us_states$REGION), # attribute to aggregate by\n                      FUN = sum, na.rm = TRUE) # aggregating function\n\n\n\nCode\nmap1 &lt;- tm_shape(us_states) +\n  tm_polygons(col = \"total_pop_15\",\n              title = \"Total population\") +\n  tm_layout(title = \"US States\")\n\nmap2 &lt;- tm_shape(regions1) +\n  tm_polygons(col = \"population\",\n              title = \"Total population\") +\n  tm_layout(title = \"group_by()\")\n\nmap3 &lt;- tm_shape(regions2) +\n  tm_polygons(col = \"total_pop_15\",\n              title = \"Total population\") +\n  tm_layout(title = \"aggregate()\")\n\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nWhat’s going on here? Behind the scenes, R is using st_union() to combine geometries within each group. We can also use st_union() to combine any pair of spatial objects.\n\n# combine geometries of western states\nus_west &lt;- us_states[us_states$REGION == \"West\", ]\nus_west_union &lt;- st_union(us_west)\n\n# combine geometries of Texas and western states\ntexas &lt;- us_states[us_states$NAME == \"Texas\", ]\ntexas_union &lt;- st_union(us_west_union, texas)\n\n\n\nCode\nmap1 &lt;- tm_shape(us_west) +\n  tm_polygons() +\n  tm_layout(main.title = \"western states\")\n\nmap2 &lt;- tm_shape(us_west_union) +\n  tm_polygons() +\n  tm_layout(main.title = \"western states union\")\n\nmap3 &lt;- tm_shape(texas) +\n  tm_polygons() +\n  tm_layout(main.title = \"TX\")\n\nmap4 &lt;- tm_shape(texas_union) +\n  tm_polygons() +\n  tm_layout(main.title = \"TX + western states union\")\n\ntmap_arrange(map1, map2, map3, map4, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week3.html#filtering",
    "href": "course-materials/labs/week3.html#filtering",
    "title": "Week 3: Lab",
    "section": "2. Filtering",
    "text": "2. Filtering\n\nBuffers\nIn the previous section, we saw that we can filter spatial objects based on their proximity using st_is_within_distance(). An alternative approach to finding items that are within a set distance would be to expand the geometry and then intersect with objects of interest. We can change the size of geometries by creating a “buffer” using st_buffer()\nIn this example, let’s create 5 km and 50 km buffers around the Seine.\n\nseine_buffer_5km &lt;- st_buffer(seine, dist = 5000)\nseine_buffer_50km = st_buffer(seine, dist = 50000)\n\n\n\nCode\nmap1 &lt;- tm_shape(seine_buffer_5km) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_layout(title = \"5km buffer\")\n\nmap2 &lt;- tm_shape(seine_buffer_50km) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_layout(title = \"50km buffer\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nThe Seine is actually comprised of multiple geometries. To make things simpler, and more look better on maps, we can combine geometries using our new friend st_union!\n\nseine_union &lt;- st_union(seine_buffer_50km)\n\n\n\nCode\ntm_shape(seine_union) +\n  tm_polygons() +\n  tm_shape(seine) +\n  tm_lines() +\n  tm_layout(title = \"50km buffer\")\n\n\n\n\n\n\n\n\n\nNow let’s see an example of using a buffer to find objects within a set distance. Here, we’ll repeat our previous example of finding points within 100 km of Canterbury. And check to see if the results match our previous approach!\n\n# create buffer around high points\nnz_height_buffer &lt;- st_buffer(nz_height, dist = 1000000)\n\n# filter buffered points with those that intersect Canterbury\nc_height5 &lt;- nz_height_buffer %&gt;%\n  st_filter(y = canterbury, .predicate = st_intersects)\n\n# check to see if results match previous approach\nif(nrow(c_height4) == nrow(c_height5)){\n  print(\"results from buffer approach match st_is_within_distance() approach\")\n} else{\n  warning(\"approaches giving different results\")\n}\n\n[1] \"results from buffer approach match st_is_within_distance() approach\"\n\n\n\n\nClipping\nBeyond filtering observations based on their spatial proximity, in some cases we might want to filter (or remove) portions of geometries. Spatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features.\nClipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents.\nThere are several options for clipping geometries:\n\nst_intersection(x, y) - portion of x intersecting y\nst_difference(x, y) - portion of x not intersecting y\nst_difference(y, x) - portion of y not intersecting x\nst_union(x, y) - portion either in x or y\nst_sym_difference(x, y) - portions of x and y that do not intersect\n\nTo illustrate the concept, we will start with a simple example: two overlapping circles with a center point one unit away from each other and a radius of one.\n\nx &lt;- st_sfc(st_point(c(0, 1))) %&gt;%\n  st_buffer(., dist = 1) %&gt;%\n  st_as_sf()\n\ny &lt;- st_sfc(st_point(c(1, 1))) %&gt;%\n  st_buffer(., dist = 1) %&gt;%\n  st_as_sf()\n\nintersection &lt;- st_intersection(x, y)\ndifference_x_y &lt;- st_difference(x, y)\ndifference_y_x &lt;- st_difference(y, x)\nunion &lt;- st_union(x, y)\nsym_difference &lt;- st_sym_difference(x, y)\n\n\n\nCode\nbbox &lt;- st_union(x, y)\n\nmap1 &lt;- tm_shape(x, bbox = bbox) +\n  tm_borders(col = \"red\") +\n  tm_shape(y) +\n  tm_borders(col = \"blue\")\n\nmap2 &lt;- map1 +\n  tm_shape(intersection, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_layout(title = \"st_intersection()\")\n\nmap3 &lt;- map1 +\n  tm_shape(difference_x_y, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_layout(title = \"st_difference(x,y)\")\n\nmap4 &lt;- map1 +\n  tm_shape(difference_y_x, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_layout(title = \"st_difference(y,x)\")\n\nmap5 &lt;- map1 +\n  tm_shape(union, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_layout(title = \"st_union()\")\n\nmap6 &lt;- map1 +\n  tm_shape(sym_difference, bbox = bbox) +\n  tm_fill(col = \"purple\") +\n  tm_layout(title = \"st_sym_diffference()\")\n\ntmap_arrange(map1, map2, map3, map4, map5, map6, nrow = 2)\n\n\n\n\n\n\n\n\n\nNow let’s see how we could use these updated geometries for filtering. Extending this simple example, we’ll create 100 random points. We want to find the points that intersect both x and y. We have a few different approaches that all produce the same results.\n\n# create random points\nbb &lt;- st_bbox(st_union(x, y)) # create bounding box of x and y\nbox &lt;- st_as_sfc(bb)\np &lt;- st_sample(x = box, size = 100) %&gt;% # randomly sample the bounding box\n  st_as_sf()\n\n# find intersection of x and y\nx_and_y &lt;- st_intersection(x, y)\n\n# filter points\n# first approach: bracket subsetting\np_xy1 = p[x_and_y, ]\n\n# second approach: st_filter()\np_xy2 &lt;- p %&gt;%\n  st_filter(., x_and_y)\n\n# third approach: st_intersection()\np_xy3 = st_intersection(p, x_and_y)\n\n\n\nCode\nmap2 &lt;- map1 +\n  tm_shape(p) +\n  tm_dots(alpha = 0.5) +\n  tm_layout(main.title = \"original\")\n\nmap3 &lt;- map2 +\n  tm_shape(p_xy1) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"bracket subsetting\")\n\nmap4 &lt;- map2 +\n  tm_shape(p_xy2) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"st_filter()\")\n\nmap5 &lt;- map2 +\n  tm_shape(p_xy3) +\n  tm_symbols(col = \"purple\", size = 0.2) +\n  tm_layout(main.title = \"st_intersection()\")\n\ntmap_arrange(map2, map3, map4, map5, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week3.html#making-life-easier",
    "href": "course-materials/labs/week3.html#making-life-easier",
    "title": "Week 3: Lab",
    "section": "3. Making life easier!",
    "text": "3. Making life easier!\nWorking with, and especially plotting, complex spatial objects can become quite cumbersome. This section we’ll see a few ways to create and manipulate geometries to make them easier to work with.\n\nCentroids\nCentroids are basically the center of spatial objects. They can be a handy way to display summary statistics (or we might actually use them for analysis – e.g. to find the distance between polygons).\nThere are many ways to that we might want to define the “center” of an object, but the most common is the geographic centroid which is the center of mass of a spatial object. The geographic centroid can be found using st_centroid().\nSometimes the geographic centroid may fall outside of the boundaries of the object (picture the centroid of a doughnut!). While correct, it might be confusing on a map, so we can use st_point_on_surface() to ensure that the centroid is placed onto the object.\nLet’s inspect a few examples!\n\nnz_centroid &lt;- st_centroid(nz)\nseine_centroid &lt;- st_centroid(seine)\n\nnz_pos &lt;- st_point_on_surface(nz)\nseine_pos &lt;- st_point_on_surface(seine)\n\n\n\nCode\nmap1 &lt;- tm_shape(nz) +\n  tm_polygons() +\n  tm_shape(nz_centroid) +\n  tm_symbols(col = \"red\", alpha = 0.5) +\n  tm_shape(nz_pos) +\n  tm_symbols(col = \"blue\", alpha = 0.5)\n\nmap2 &lt;- tm_shape(seine) +\n  tm_lines() +\n  tm_shape(seine_centroid) +\n  tm_symbols(col = \"red\", alpha = 0.5) +\n  tm_shape(seine_pos) +\n  tm_symbols(col = \"blue\", alpha = 0.5)\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\nSimplification\nWe may also want to simplify a geometry to make it easier to plot or take up less storage. There are several different algorithms for simplifying geometries, check out Geocomputation with R for more examples. The sf packages uses the Douglas-Peucker algorithm within the st_simplify() function.\nLet’s see an example!\n\nseine_simple &lt;- st_simplify(seine, dTolerance = 2000)  # 2000 m\n\n\n\nCode\nmap1 &lt;- tm_shape(seine) +\n  tm_lines() +\n  tm_layout(\"original\")\n\nmap2 &lt;- tm_shape(seine_simple) +\n  tm_lines() +\n  tm_layout(\"st_simplify()\")\n\ntmap_arrange(map1, map2, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week1.html",
    "href": "course-materials/labs/week1.html",
    "title": "Week 1: Lab",
    "section": "",
    "text": "In this lab, we’ll explore the basics of map-making in R using the tmap package."
  },
  {
    "objectID": "course-materials/labs/week1.html#why-tmap",
    "href": "course-materials/labs/week1.html#why-tmap",
    "title": "Week 1: Lab",
    "section": "1. Why tmap?",
    "text": "1. Why tmap?\n\nThere are MANY ways to make maps in R, but tmap or “thematic maps” offers the most flexibility.\ntmap can handle vector and raster objects from the sf, sp, raster, and stars packages.\nThe syntax of tmap is based on ggplot2 and the Grammar of Graphics\ntmap supports static AND interactive maps (yay!)\n\n\n\n\n\n\n\nMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/labs/week1.html#set-up",
    "href": "course-materials/labs/week1.html#set-up",
    "title": "Week 1: Lab",
    "section": "2. Set up",
    "text": "2. Set up\n\nFork and clone this repository to create a version controlled project for Week 1.\nCreate a Quarto doc\nInstall and load all necessary packages\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"stars\")\ninstall.packages(\"tmap\")\n\n\nlibrary(sf) # for vector data (more soon!)\nlibrary(stars) # for raster data (more soon!)\nlibrary(tmap) # for static and interactive maps\nlibrary(here)"
  },
  {
    "objectID": "course-materials/labs/week1.html#specifying-spatial-data",
    "href": "course-materials/labs/week1.html#specifying-spatial-data",
    "title": "Week 1: Lab",
    "section": "3. Specifying spatial data",
    "text": "3. Specifying spatial data\nSimilar to plotting in ggplot2, in order to plot spatial data, at least two aspects need to be specified:\n\nthe spatial data object to plot (similar to ggplot(data = ))\nand the plotting method (similar to geom_points())\n\nLet’s load three spatial data objects to plot:\n\na raster (more on this next week!) of elevations of the world\na vector dataset (again, more soon!) of country boundaries\na vector dataset of locations of major cities\n\n\n# raster of global elevations\nworld_elevation &lt;- read_stars(here(\"data\",\"worldelevation.tif\"))\n\n# country boundaries\nworld_vector &lt;- read_sf(here(\"data\",\"worldvector.gpkg\"))\n\n# major cities\nworld_cities &lt;- read_sf(here(\"data\",\"worldcities.gpkg\"))\n\n\nShapes and layers\nIn tmap, the spatial object to plot needs to be defined within the function tm_shape(). This is analogous to defining the data frame to plot in ggplot2 using ggplot(data = ).\nLet’s start by plotting the countries of the world.\n\n# plotting a single spatial object\n\ntm_shape(world_vector) + # defines the spatial object to plot\n  tm_polygons() # defines how to plot the object\n\n\n\n\n\n\n\n\n\n\nShapes hierarchy\nSimilar to ggplot2, we can plot multiple datasets by adding layers. When multiple spatial objects are being plotted, each has to be defined in a separate tm_shape() call.\nNow let’s plot the following two spatial objects:\n\ncountries of the world\nmajor cities of the world\n\nIn the next section we’ll unpack the difference between tm_polygons() and tm_dots(), but for now let’s just pay attention to the syntax of how we plot multiple spatial objects. Each spatial object needs to be specified using tm_shape() followed by a function for how to plot it.\n\n# plotting two spatial objects\n\ntm_shape(world_vector) + # defines the FIRST spatial object to plot\n  tm_polygons() + # defines how to plot the FIRST object\ntm_shape(world_cities) + # defines the SECOND objet to plot\n  tm_dots() # defines how to plot the SECOND object\n\n\n\n\n\n\n\n\nSo far, we’ve only tried plotting vector data (more on what this means next week!), but one of the major advantages of tmap is that it allows us to plot vector and raster on the same map.\nLet’s try on example of this by adding information on global elevations to our previous map.\n\n# plotting vector and raster spatial objects\n\ntm_shape(world_elevation) + # plot global elevations\n  tm_raster() + # tm_raster for raster data\ntm_shape(world_vector) +\n  tm_borders() +\ntm_shape(world_cities) +\n  tm_dots() +\n  tm_text(\"name\")\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nSimilar to ggplot2 the order of the “layers” matters! The order in which datasets are plotted defines how they are layered (think of this is as adding layers of paint). Spatial objects have extra features which additionally change this behavior: spatial extent and projection. When creating maps with tmap, whichever dataset is used in the first tm_shape() call sets the spatial extent and projection (more details next week!) for the entire map.\nFor example, if we swapped the order of tm_shape() calls in the previous example, we’d end up with a different map.\n\ntm_shape(world_cities) + # plot world_cities first\n  tm_dots() +\n  tm_text(\"name\") +\ntm_shape(world_elevation) +\n  tm_raster() +\ntm_shape(world_vector) +\n  tm_borders() \n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nSometimes this can present sticky issues! Imagine the case where we want to use the spatial extent and projection from the world_cities data, but want it plotted on top of the other datasets. We can do this by changing the main shape using the is.master argument.\n\ntm_shape(world_elevation) + \n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() +\ntm_shape(world_cities, is.master = TRUE) + # plot world_cities last, but set as main shape\n  tm_dots() +\n  tm_text(\"name\")\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\nMap extent\nOne thing to consider when making maps is what area we want to show on the map – the spatial extent of our map. This isn’t an issue when we want to map all of our data (spatial extent of our data matches our desired map extent). But often our data will represent a larger region than what we want to map.\nWe have two options:\n\nprocess our data to create a new spatial object for exactly what we want to map (fine, but annoying)\nchange the extent of a map\n\ntmap has a few options for changing the map extent. The first is by defining a bounding box that specifies the minimum and maximum coordinates in the x and y directions that we want to represent. The values need to be in the units of the original data or we can create a bounding box using st_bbox().\nFor example, let’s restrict our previous map to just Europe using a set of min/max values.\n\ntm_shape(world_elevation, bbox = c(-15, 35, 45, 65)) + # add bounding box to restrict extent\n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() \n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nWe can also restrict the extent of the map using the extent of a dataset. For example, we can restrict the map using the extent of the world_cities data.\n\ntm_shape(world_elevation, bbox = world_cities) + # bounding box = extent of world_cities\n  tm_raster() + \ntm_shape(world_vector) +\n  tm_borders() \n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonus Tip\n\n\n\n\n\nYou can also restrict the map extent using an OpenStreetMap tool called Nominatim to automatically generate minimum and maximum coordinates in the x and y directions based on the provided query.\n\ntm_shape(world_elevation, bbox = \"Europe\") + # query the region of Europe\n  tm_raster(palette = terrain.colors(8))"
  },
  {
    "objectID": "course-materials/labs/week1.html#layers",
    "href": "course-materials/labs/week1.html#layers",
    "title": "Week 1: Lab",
    "section": "4. Layers",
    "text": "4. Layers\nAgain following the syntax of ggplot2 which uses layers to plot data (e.g. geom_point()), tmap also uses layers! We’ve already used layers in our previous examples (e.g. tm_borders()), but now we’ll dig into them in more detail. All possible layer types can be found in the table below:\n\n\n\nMap layers.\n\n\nFunction\nElement\nGeometry\n\n\n\n\nBasic functions\n\n\ntm_polygons()\npolygons (borders and fill)\npolygons\n\n\ntm_symbols()\nsymbols\npoints, polygons, and lines\n\n\ntm_lines()\nlines\nlines\n\n\ntm_raster()\nraster\nraster\n\n\ntm_text()\ntext\npoints, polygons, and lines\n\n\ntm_basemap()\ntile\n\n\n\ntm_tiles()\ntile\n\n\n\nDerived functions\n\n\ntm_borders()\npolygons (borders)\npolygons\n\n\ntm_fill()\npolygons (fill)\npolygons\n\n\ntm_bubbles()\nbubbles\npoints, polygons, and lines\n\n\ntm_dots()\ndots\npoints, polygons, and lines\n\n\ntm_markers()\nmarker symbols\npoints, polygons, and lines\n\n\ntm_square()\nsquares\npoints, polygons, and lines\n\n\ntm_iso()\nlines with text labels\nlines\n\n\ntm_rgb()/tm_rgba()\nraster (RGB image)\nraster\n\n\n\n\n\n\n\n\nPolygons\nThe main function to visualize polygons is tm_polygons(). By default, it plots the internal area of the polygon in light grey and the polygon borders in slightly darker grey.\n\ntm_shape(world_vector) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nWe modify the colors useing the col and border.col arguments and other arguments borrowed from ggplot2.\n\ntm_shape(world_vector) +\n  tm_polygons(col = \"lightblue\",\n              border.col = \"black\",\n              lwd = 0.5,\n              lty = \"dashed\")\n\n\n\n\n\n\n\n\nBut, you may have noticed in the previous table that tm_polygons isn’t the only function we can use to plot polygon data. In fact, tm_polygons is a combination of two separate functions - tm_fill() and tm_borders().\nThe tm_borders() function plots just the borders and the tm_fill() function fills polygons with a fixed color or a color palette representing a selected variable.\n\n# plot just borders\n\ntm_shape(world_vector) +\n  tm_borders(col = \"red\")\n\n\n\n\n\n\n\n\n\n# fill polygons with fixed color\n\ntm_shape(world_vector) +\n  tm_fill(col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n# fill polygons with a color palette representing a variable\n\ntm_shape(world_vector) +\n  tm_fill(\"CO2_emissions\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax differences\n\n\n\nNote that to change the border color in tm_polygons() we used the border.col argument, but in tm_borders() we used the col argument. This is necessary in tm_polygons() to differentiate between the settings for the polygons fill and borders.\n\n\n\n\nSymbols\nSymbols are a very flexible layer type. They typically represent point data, but can also be used for lines and polygons (in this case located at the centroid of each feature). Symbols are also highly flexible in how they can be visualized. They can show the values of a given variable by the color, size, and shape of the symbol.\ntm_symbols() is the main function in tmap to display and modify symbol elements. By default, this function draws a gray circle symbol with a black border for each element of an input feature.\n\ntm_shape(world_cities) +\n  tm_symbols()\n\n\n\n\n\n\n\n\ntm_symbols() has a large number of arguments to flexibly adjust how elements are displayed. While this allows adjusting its results to almost any need, it also makes this function complicated. Therefore, four additional layers are implemented in tmap: tm_squares(), tm_bubbles(), tm_dots(), tm_markers(). All of them use tm_symbols(), but with different default values.\n\ntm_squares(): uses square symbols (shape = 22)\ntm_bubbles(): uses large circle symbols\ntm_dots(): uses small circle symbols (good for displaying many locations)\ntm_markers(): uses marker icons\n\n\ntm_shape(world_cities) +\n  tm_squares()\n\n\n\n\n\n\n\ntm_shape(world_cities) +\n  tm_bubbles()\n\n\n\n\n\n\n\ntm_shape(world_cities) +\n  tm_dots()"
  },
  {
    "objectID": "course-materials/labs/week1.html#visual-variables",
    "href": "course-materials/labs/week1.html#visual-variables",
    "title": "Week 1: Lab",
    "section": "5. Visual variables",
    "text": "5. Visual variables\nFollowing ggplot2 yet again, tmap uses the basic visual variables of color, size, and shape to represent data. Which variables can be applied depends on the type of the map layer.\n\nSymbols: color, size, and shape\nLines: color and size\nPolygons: color\n\nThe type of data (quantitative or qualitative) also determines which visual variables can be used.\n\nColor: quantitative or qualitative\nSize: quantitative\nShape: qualitative\n\n\nColor\ntmap uses the many ways that colors can be specified in R:\n\nbuilt-in color names (e.g. “red”)\nhexadecimal (e.g. #00FF00)\npalettes\n\nThere are dozens of packages that contain hundreds of color palettes. The most popular are RColorBrewer and viridis. By default, tmap attempts to identify the type of the data being plotted and selects on of the built-in palettes.\ntmap offers three main ways to specify color palettes using the palette argument:\n\na vector of colors\na palette function\none of the built-in names\n\nA vector of colors can be specified by name or hexidecimal. Importantly, the number of colors provided does not need to match the number of colors in the map legend. tmap automatically interpolates new colors in the case when a smaller number of colors is provided.\n\n\n\n\n\n\nUpdating legend titles\n\n\n\nJust like updating axis labels, we always need to update legend titles. In tmap we can do that directly by using the title argument in the attribute layer.\n\n\n\n# vector of colors\n\ntm_shape(world_vector) +\n  tm_polygons(\"life_expectancy\", \n              palette = c(\"yellow\", \"darkgreen\"),\n              title = \"Life Expectancy (years)\")\n\n\n\n\n\n\n\n\nAnother approach is to provide the output of a palette function. When using a palette function, you can specify the number of colors to use. Below we use the viridis palette from the viridisLite package.\n\n# palette function\n\n#install.packages(\"viridisLite\")\nlibrary(viridisLite)\n\ntm_shape(world_vector) +\n  tm_polygons(\"life_expectancy\", \n              palette = viridis(8),\n              title = \"Life Expectancy (years)\")\n\n\n\n\n\n\n\n\nFinally, the last approach is to use the name of one of the built-in color palettes.\n\n# built-in color palette\n\ntm_shape(world_vector) +\n  tm_polygons(\"life_expectancy\", \n              palette = \"YlGn\",\n              title = \"Life Expectancy (years)\")\n\n\n\n\n\n\n\n\n\n\nSize\nSizes can be used for points, lines (line widths), or text to represent quantitative (numerical) variables. By default, tmap represents points, lines, or text objects as the same size. The size of objects can be changed by using the size argument.\n\ntm_shape(world_vector) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(world_cities) +\n  tm_symbols(size = \"pop2020\",\n             legend.size.is.portrait = TRUE)"
  },
  {
    "objectID": "course-materials/labs/week1.html#layout",
    "href": "course-materials/labs/week1.html#layout",
    "title": "Week 1: Lab",
    "section": "6. Layout",
    "text": "6. Layout\nJust like in standard data visualizations, maps have elements that need to be provided in order to interpret them correctly. Maps need to contain either a scale bar and north arrow OR grid lines or graticules. tmap provides these elements (and others) as the following additional attribute layers.\n\n\n\nAttribute layers.\n\n\nFunction\nDescription\n\n\n\n\ntm_grid()\ndraws coordinate grid lines of the coordinate system of the main shape object\n\n\ntm_graticules()\ndraws latitude and longitude graticules\n\n\ntm_scale_bar()\nadds a scale bar\n\n\ntm_compass()\nadds a compass rose\n\n\ntm_credits()\nadds a text annotation\n\n\ntm_logo()\nadds a logo\n\n\ntm_xlab()\nadds an x axis labels\n\n\ntm_ylab()\nadds an y axis labels\n\n\ntm_minimap()\nadds minimap in the view mode only\n\n\n\n\n\n\n\n\nGrid lines\nThe tmap package offers two ways to draw coordinate lines - tm_grid() and tm_graticules(). tm_grid() represents the input data’s coordinates.\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_grid()\n\n\n\n\n\n\n\n\ntm_graticules() shows latitude and longitude lines, with degrees as units\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_graticules()\n\n\n\n\n\n\n\n\nBoth tm_grid() and tm_graticules() can be placed above or below other map layers.\n\ntm_shape(world_vector) +\n  tm_graticules() + # graticules below tm_fill()\n  tm_fill()\n\n\n\n\n\n\n\n\n\n\nScale bar and north arrow\nA scale bar is a graphic indicator of the relation between a distance on a map and the corresponding distance in the real world. A north arrow, or a map compass or compass rose, indicates the orientation of the map. North arrows can be added to every map, but are not necessary on maps of large areas (e.g. global maps) where the orientation is obvious.\n\ntm_shape(world_vector) +\n  tm_fill() +\n  tm_scale_bar() +\n  tm_compass(position = c(\"left\", \"top\"))\n\n\n\n\n\n\n\n\n\n\nLayout options\nSimilar to the theme() function in ggplot2, the tm_layout() function in tmap controls many of the map elements of the map layout.\n\ntm_shape(world_vector) +\n  tm_fill(col = \"wb_income_region\",\n          palette = viridisLite::plasma(5),\n          title = \"Regional Income\") +\n  tm_layout(bg.color = \"grey95\",\n            main.title = \"Global income\",\n            frame = FALSE)"
  },
  {
    "objectID": "course-materials/labs/week1.html#interactive-options",
    "href": "course-materials/labs/week1.html#interactive-options",
    "title": "Week 1: Lab",
    "section": "7. Interactive options",
    "text": "7. Interactive options\nOne of the most powerful aspects of tmap is the ease of creating interactive maps. tmap has two modes \"plot\" which creates static maps and \"view\" which creates interactive maps that can be easily embedded in quarto docs. It’s as easy as using the tmap_mode()!\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(world_vector) +\n  tm_fill(col = \"gdp_per_cap\",\n          title = \"GDP per capita\") \n\n\n\n\n\nTo return to regular plotting mode, simply reset tmap_mode.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "course-materials/labs/week1.html#saving-maps",
    "href": "course-materials/labs/week1.html#saving-maps",
    "title": "Week 1: Lab",
    "section": "8. Saving maps",
    "text": "8. Saving maps\nMaps can be stored as objects for for adding additional layers and saving programmatically. Maps can be saved directly in tmap using the tm_save() function.\n\nmap1 &lt;- tm_shape(world_vector) +\n  tm_fill(col = \"gdp_per_cap\",\n          palette = viridisLite::plasma(10),\n          title = \"GDP per capita\") +\n  tm_layout(main.title = \"Global gross domesic product\")\n\ntmap_save(map1, here(\"tmap-example.png\"))"
  },
  {
    "objectID": "course-materials/labs/week8.html",
    "href": "course-materials/labs/week8.html",
    "title": "Week 8: Lab",
    "section": "",
    "text": "Phenology is the timing of life history events. Important phenological events for plants involve the growth of leaves, flowering, and senescence (death of leaves). Plants species adapt the timing of these events to local climate conditions to ensure successful reproduction. Subsequently, animal species often adapt their phenology to take advantage of food availability. As the climate shifts this synchronization is being thrown out of whack. Shifts in phenology are therefore a common yardstick of understanding how and if ecosystems are adjusting to climate change.\nPlant species may employ the following phenological strategies:\nThis lab is based on a materials developed by Chris Kibler."
  },
  {
    "objectID": "course-materials/labs/week8.html#task",
    "href": "course-materials/labs/week8.html#task",
    "title": "Week 8: Lab",
    "section": "Task",
    "text": "Task\nIn this lab we are analyzing plant phenology near the Santa Clara River which flows from Santa Clarita to Ventura. We will investigate the phenology of the following plant communities:\n\nRiparian forests: grow along the river, dominated by winter deciduous cottonwood and willow trees\nGrasslands: grow in openspaces, dominated by drought deciduous grasses\nChaparral shrublands: grow in more arid habitats, dominated by evergreen shrubs\n\nTo investigate the phenology of these plant communities we will a time series of Landsat imagery and polygons identifying the locations of study sites within each plant community.\nOur goals in this lab are:\n\nConvert spectral reflectance into a measure of vegetation productivity (NDVI)\nCalculate NDVI throughout the year\nSummarize NDVI values within vegetation communities\nVisualize changes in NDVI within vegetation communities"
  },
  {
    "objectID": "course-materials/labs/week8.html#data",
    "href": "course-materials/labs/week8.html#data",
    "title": "Week 8: Lab",
    "section": "Data",
    "text": "Data\nLandsat’s Operational Land Imager (OLI)\n\n8 pre-processed scenes\n\nLevel 2 surface reflectance products\nErroneous values set to NA\nScale factor set to 100\nBands 2-7\nDates in filenname\n\n\nStudy sites\n\nPolygons representing sites\n\nstudy_site: character string with plant type"
  },
  {
    "objectID": "course-materials/labs/week8.html#prerequisites",
    "href": "course-materials/labs/week8.html#prerequisites",
    "title": "Week 8: Lab",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s load all necessary packages:\n\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(tmap)\nlibrary(cowplot)"
  },
  {
    "objectID": "course-materials/labs/week8.html#create-ndvi-function",
    "href": "course-materials/labs/week8.html#create-ndvi-function",
    "title": "Week 8: Lab",
    "section": "Create NDVI Function",
    "text": "Create NDVI Function\nLet’s start by defining a function to compute the NDVI.\n\nNDVI computes the difference in reflectance in the near infrared and red bands, normalized by their sum.\n\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}"
  },
  {
    "objectID": "course-materials/labs/week8.html#compute-ndvi-for-a-single-scene",
    "href": "course-materials/labs/week8.html#compute-ndvi-for-a-single-scene",
    "title": "Week 8: Lab",
    "section": "Compute NDVI for a Single Scene",
    "text": "Compute NDVI for a Single Scene\nWe have 8 scenes collected by Landsat’s OLI sensor on 8 different days throughout the year.\nLet’s start by loading in the first scene collected on June 12, 2018:\n\nlandsat_20180612 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20180612.tif\"))\nlandsat_20180612\n\nNow let’s update the names of the layers to match the spectral bands they correspond to:\n\nnames(landsat_20180612) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nlandsat_20180612\n\nNow we can apply the NDVI function we created to compute NDVI for this scene using the lapp() function.\n\nThe lapp() function applies a function to each cell using layers as arguments.\nTherefore, we need to tell lapp() which layers (or bands) to pass into the function.\n\nThe NIR band is the 4th layer and the red band is the 3rd layer in our raster. In this case, because we defined the NIR band as the first argument and the red band as the second argument in our function, we tell lapp() to use the 4th layer first and 3rd layer second.\n\nndvi_20180612 &lt;- lapp(landsat_20180612[[c(4, 3)]], fun = ndvi_fun)\nndvi_20180612"
  },
  {
    "objectID": "course-materials/labs/week8.html#attempt-1-compute-ndvi-for-all-scences",
    "href": "course-materials/labs/week8.html#attempt-1-compute-ndvi-for-all-scences",
    "title": "Week 8: Lab",
    "section": "Attempt 1: Compute NDVI for All Scences",
    "text": "Attempt 1: Compute NDVI for All Scences\nNow we want to repeat the same operations for all 8 scenes. Below is a possible solution, but it’s pretty clunky.\nLet’s load each layer:\n\nlandsat_20180612 &lt;-rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20180612.tif\"))\nlandsat_20180815 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20180815.tif\"))\nlandsat_20181018 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20181018.tif\"))\nlandsat_20181103 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20181103.tif\"))\nlandsat_20190122 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190122.tif\"))\nlandsat_20190223 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190223.tif\"))\nlandsat_20190412 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190412.tif\"))\nlandsat_20190701 &lt;- rast(here(\"course-materials\", \"data\", \"week8\", \"landsat_20190701.tif\"))\n\nAnd rename each layer:\n\nnames(landsat_20180612) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20180815) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20181018) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20181103) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190122) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190223) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190412) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(landsat_20190701) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\nNext, compute NDVI for each layer:\n\nndvi_20180612 &lt;- lapp(landsat_20180612[[c(4, 3)]], fun = ndvi_fun)\nndvi_20180815 &lt;- lapp(landsat_20180815[[c(4, 3)]], fun = ndvi_fun)\nndvi_20181018 &lt;- lapp(landsat_20181018[[c(4, 3)]], fun = ndvi_fun)\nndvi_20181103 &lt;- lapp(landsat_20181103[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190122 &lt;- lapp(landsat_20190122[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190223 &lt;- lapp(landsat_20190223[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190412 &lt;- lapp(landsat_20190412[[c(4, 3)]], fun = ndvi_fun)\nndvi_20190701 &lt;- lapp(landsat_20190701[[c(4, 3)]], fun = ndvi_fun)\n\nLet’s combine NDVI layers into a single raster stack.\n\nall_ndvi &lt;- c(ndvi_20180612, ndvi_20180815, ndvi_20181018, ndvi_20181103, ndvi_20190122, ndvi_20190223, ndvi_20190412, ndvi_20190701)\n\nNow, update the names of each layer to match the date of each image:\n\nnames(all_ndvi) &lt;- c(\"2018-06-12\", \"2018-08-15\", \"2018-10-18\", \"2018-11-03\", \"2019-01-22\", \"2019-02-23\", \"2019-04-12\", \"2019-07-01\")"
  },
  {
    "objectID": "course-materials/labs/week8.html#attempt-2-compute-ndvi-for-all-scenes",
    "href": "course-materials/labs/week8.html#attempt-2-compute-ndvi-for-all-scenes",
    "title": "Week 8: Lab",
    "section": "Attempt 2: Compute NDVI for All Scenes",
    "text": "Attempt 2: Compute NDVI for All Scenes\nThe first attempt was pretty clunky and required a lot of copy/pasting. Because we’re performing the same operations over and over again, this is a good opportunity to generalize our workflow into a function!\nLet’s start over and see how we could do this more efficiently.\nWe’ll clear our environment and redefine our function for NDVI:\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\nNext, let’s first sketch out what operations we want to perform so we can figure out what our function needs:\n\n# Note: this code is not meant to run! \n# We're just outlining the function we want to create\n\ncreate_ndvi_layer &lt;- function(){\n  # Read scene\n  landsat &lt;- rast(file)\n  # Rename layer\n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n  # Compute NDVI\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n}\n\n# What do we notice as what we need to pass into our function?\n\nWe want a list of the scenes so that we can tell our function to compute NDVI for each. To do that we look in our data folder for the relevant file.\n\nAsk for the names of all the files in the week8 folder\nSet the “pattern” option to return the names that end in .tif (\n\n.tif is the file extension for the landsat scenes\n\nSet the “full.names” option returns the full file path for each scene\n\n\nfiles &lt;- list.files(\n  here(\"course-materials\", \"data\", \"week8\"), pattern = \"*.tif\", \n  full.names = TRUE)\n\nNow let’s update our function to work with list of file names we created:\n\nPass function a number that will correspond to the index in the list of file names\n\n\ncreate_ndvi_layer &lt;- function(i){\n  landsat &lt;- rast(files[i])\n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n}\n\nLet’s test our function by asking it to read in the first file:\n\ntest &lt;- create_ndvi_layer(1)\n\nNow we can use our function to create a NDVI layer for each scene and stack them into a single rasterstack. And then update layer names to match date:\n\nall_ndvi &lt;- c(create_ndvi_layer(1), create_ndvi_layer(2), create_ndvi_layer(3), create_ndvi_layer(4), create_ndvi_layer(5), create_ndvi_layer(6), create_ndvi_layer(7), create_ndvi_layer(8))\n\nnames(all_ndvi) &lt;- c(\"2018-06-12\", \"2018-08-15\", \"2018-10-18\", \"2018-11-03\", \"2019-01-22\", \"2019-02-23\", \"2019-04-12\", \"2019-07-01\")"
  },
  {
    "objectID": "course-materials/labs/week8.html#compare-ndvi-across-vegetation-communities",
    "href": "course-materials/labs/week8.html#compare-ndvi-across-vegetation-communities",
    "title": "Week 8: Lab",
    "section": "Compare NDVI Across Vegetation Communities",
    "text": "Compare NDVI Across Vegetation Communities\nNow that we have computed NDVI for each of our scenes (days) we want to compare changes in NDVI values across different vegetation communities.\nFirst, we’ll read in a shapefile of study sites:\n\nsites &lt;- st_read(here(\"course-materials\", \"data\",\"week8\",\"study_sites.shp\"))\n\nAnd plot study sites on a single NDVI layer:\n\ntm_shape(all_ndvi[[1]]) +\n  tm_raster() +\n  tm_shape(sites) +\n  tm_polygons()\n\n\nExtract NDVI at Study Sites\nHere, we find the average NDVI within each study site. The output of extract is a data frame with rows that match the study site dataset, so we bind the results to the original dataset.\n\nsites_ndvi &lt;- terra::extract(all_ndvi, sites, fun = \"mean\")\n\nsites_annotated &lt;- cbind(sites, sites_ndvi)\n\nWe’re done! Except our data is very untidy… Let’s tidy it up!\n\nConvert to data frame\nTurn from wide to long format\nTurn layer names into date format\n\n\nsites_clean &lt;- sites_annotated %&gt;%\n  st_drop_geometry() %&gt;%\n  select(-ID) %&gt;%\n  pivot_longer(!study_site) %&gt;%\n  rename(\"NDVI\" = value) %&gt;%\n  mutate(\"year\" = str_sub(name, 2, 5),\n         \"month\" = str_sub(name, 7, 8),\n         \"day\" = str_sub(name, -2, -1)) %&gt;%\n  unite(\"date\", 4:6, sep = \"-\") %&gt;%\n  mutate(\"date\" = lubridate::as_date(date))"
  },
  {
    "objectID": "course-materials/labs/week8.html#plot-results",
    "href": "course-materials/labs/week8.html#plot-results",
    "title": "Week 8: Lab",
    "section": "Plot Results",
    "text": "Plot Results\nLet’s plot the results:\n\nggplot(sites_clean,\n       aes(x = date, y = NDVI,\n           group = study_site, col = study_site)) +\n  scale_color_manual(values = c(\"#EAAC8B\", \"#315C2B\", \"#315C2B\", \"#315C2B\",\"#9EA93F\")) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Normalized Difference Vegetation Index (NDVI)\", col = \"Vegetation type\",\n       title = \"Seasonal cycles of vegetation productivity\")"
  },
  {
    "objectID": "assignments/PR.html#assignment-overview",
    "href": "assignments/PR.html#assignment-overview",
    "title": "Data Science Portfolio",
    "section": "Assignment overview",
    "text": "Assignment overview\nThe purpose of this assignment is to create an online portfolio to showcase your spatial data science skills.\nTo do so, please:\n\nCreate two presentation-worthy GitHub repositories based on assignments 2-4 or labs 8 & 9\n\n\n\n\n\n\n\nTip\n\n\n\nPick the assignments or labs that highlight the skills you are most interested in showcasing to potential employers."
  },
  {
    "objectID": "assignments/PR.html#guidance-for-creating-repositories",
    "href": "assignments/PR.html#guidance-for-creating-repositories",
    "title": "Data Science Portfolio",
    "section": "Guidance for creating repositories",
    "text": "Guidance for creating repositories\nThe goal of creating new repositories is to transform your existing repositories from homework assignments or labs into organized and well-documented projects that showcase an environmental data science workflow. The audience for a repository is a potential collaborator(s) or employer(s) who wants to understand your workflow. Therefore, it is appropriate intermediate outputs in addition to final results as well as custom print statements and warning messages to check results.\nEach repository should have:\n\nA brief, but informative name (e.g. “assignment-2” is not acceptable)\nA clear and organized structure\n\nShould include folders for data and scripts, where appropriate\n\nA README\n\nContents should follow the guidance from EDS 296\nAdd a section articulating the skills that you are using in this repo. The idea is to make it easy for potential employers to understand your skills\n\nA .gitignore\n\nThis should include any files that you don’t want git to track (i.e. files that you don’t want pushed to GitHub)\n\nYour revised homework assignment, as a rendered Quarto document (both .qmd and .html files). Include the following updates:\n\nGive it a brief, but informative file name (e.g. hw2.qmd is not acceptable)\nDouble check that all contents meet the standard of professional output\nAny other updates you want to make! Including updating the theme\n\n\n\n\n\n\n\n\nChanging repository ownership\n\n\n\nYour homework submissions are technically owned by our GitHub Classroom organization. To have them appear on your GitHub profile (and be visible to others), you need to create a new repository directly from your GitHub account and copy your code over."
  },
  {
    "objectID": "assignments/PR.html#rubric",
    "href": "assignments/PR.html#rubric",
    "title": "Data Science Portfolio",
    "section": "Rubric",
    "text": "Rubric\nTo receive a “Satisfactory”, both repositories must include the elements described above."
  },
  {
    "objectID": "assignments/PR.html#submitting-assignment",
    "href": "assignments/PR.html#submitting-assignment",
    "title": "Data Science Portfolio",
    "section": "Submitting assignment",
    "text": "Submitting assignment\nPlease submit your PR to this Google Form."
  },
  {
    "objectID": "assignments/HW2.html#learning-outcomes",
    "href": "assignments/HW2.html#learning-outcomes",
    "title": "Homework Assignment 2",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nbuild effective, responsible, accessible and aesthetically-pleasing maps\npractice manipulating vector and raster data to build multi-layer maps\npractice making maps in R, specifically using tmap"
  },
  {
    "objectID": "assignments/HW2.html#instructions",
    "href": "assignments/HW2.html#instructions",
    "title": "Homework Assignment 2",
    "section": "Instructions",
    "text": "Instructions\n\nClone repository from GitHub Classrooms\nDownload data from here\nUnzip data and place in repository\nEdit Quarto document with responses\nPush final edits before deadline\n\nYour repository should have the following structure:\n\nEDS223-HW2\n│   README.md\n│   HW2.qmd\n│   Rmd/Proj files    \n│\n└───data\n     └───ejscreen\n     └───gbif-birds-LA\n     └───mapping-inequality"
  },
  {
    "objectID": "assignments/HW2.html#background",
    "href": "assignments/HW2.html#background",
    "title": "Homework Assignment 2",
    "section": "Background",
    "text": "Background\nPresent-day environmental justice may reflect legacies of injustice in the past. The United States has a long history of racial segregation which is still visible. During the 1930’s the Home Owners’ Loan Corporation (HOLC), as part of the New Deal, rated neighborhoods based on their perceived safety for real estate investment. Their ranking system, (A (green), B (blue), C (yellow), D (red)) was then used to block access to loans for home ownership. Colloquially known as “redlining”, this practice has had widely-documented consequences not only for community wealth, but also health.1 Redlined neighborhoods have less greenery2 and are hotter than other neighborhoods.3\nCheck out coverage by the New York Times.\nA recent study found that redlining has not only affected the environments communities are exposed to, it has also shaped our observations of biodiversity.4 Community or citizen science, whereby individuals share observations of species, is generating an enormous volume of data. Ellis-Soto and co-authors found that redlined neighborhoods remain the most undersampled areas across 195 US cities. This gap is highly concerning, because conservation decisions are made based on these data.\nCheck out coverage by EOS."
  },
  {
    "objectID": "assignments/HW2.html#data-details",
    "href": "assignments/HW2.html#data-details",
    "title": "Homework Assignment 2",
    "section": "Data details",
    "text": "Data details\n\nEJScreen\nData file: ejscreen/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\nWe will be working with data from the United States Environmental Protection Agency’s EJScreen: Environmental Justice Screening and Mapping Tool.\nAccording to the US EPA website:\n\nThis screening tool and data may be of interest to community residents or other stakeholders as they search for environmental or demographic information. It can also support a wide range of research and policy goals. The public has used EJScreen in many different locations and in many different ways.\nEPA is sharing EJScreen with the public:\n- to be more transparent about how we consider environmental justice in our work,\n- to assist our stakeholders in making informed decisions about pursuing environmental justice and,\n- to create a common starting point between the agency and the public when looking at issues related to environmental justice.\n\nEJScreen provides on environmental and demographic information for the US at the Census tract and block group levels. You will be working with data at the block group level that has been downloaded from the EPA site. To understand the associated data columns, you will need to explore the following in the data folder:\n\nTechnical documentation: ejscreen-tech-doc-version-2-2.pdf\nColumn descriptions: EJSCREEN_2023_BG_Columns.xlsx\n\nYou should also explore the limitations and caveats of the data.\n\n\nHOLC Redlining\nData file: mapping-inequality/mapping-inequality-los-angeles.json\nA team of researchers, led by the Digital Scholarship Lab at the University of Richmond have digitized maps and information from the HOLC as part of the Mapping Inequality project.\nWe will be working with maps of HOLC grade designations for Los Angeles. Information on the data can be found here.5\n\n\nBiodiversity observations\nData file: gbif-birds-LA.shp\nThe Global Biodiversity Information Facility is the largest aggregator of biodiversity observations in the world. Observations typically include a location and date that a species was observed.\nWe will be working observations of birds from 2021 onward.\n\n\n\n\n\n\nTip\n\n\n\nMake sure to check that these datasets have the same coordinate reference systems! If not, transform them to match."
  },
  {
    "objectID": "assignments/HW2.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "href": "assignments/HW2.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "title": "Homework Assignment 2",
    "section": "Part 1: Legacy of redlining in current environmental (in)justice",
    "text": "Part 1: Legacy of redlining in current environmental (in)justice\nYour first task is to explore historical redlining in Los Angeles and its legacy on present-day environmental justice.\n\nDescription\nFor this assignment, you must produce the following:\n\na map of historical redlining neighborhoods, including:\n\nneighborhoods colored by HOLC grade\nan appropriate base map\n\na table summarizing the percent of current census block groups within each HOLC grade (or none)\na set of figures summarizing current conditions (from the EJScreen data) within HOLC grades using the mean of the following variables:\n\n% low income\npercentile for Particulate Matter 2.5\npercentile for low life expectancy\n\na brief paragraph reflecting on these results"
  },
  {
    "objectID": "assignments/HW2.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "href": "assignments/HW2.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "title": "Homework Assignment 2",
    "section": "Part 2: Legacy of redlining in biodiversity observations",
    "text": "Part 2: Legacy of redlining in biodiversity observations\nYour second task is to explore the legacy of historical redlining in Los Angeles on the collection of bird observations.\n\nDescription\nFor this assignment, you must produce the following based on observations from 2022:\n\na figure summarizing the percent of observations within redlined neighborhoods within each HOLC grade\na brief paragraph explaining whether these results match the findings from Ellis-Soto et al. 2023"
  },
  {
    "objectID": "assignments/HW2.html#rubric-specifications",
    "href": "assignments/HW2.html#rubric-specifications",
    "title": "Homework Assignment 2",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nAssignments will be deemed “Satisfactory” based on the following criteria:\n\n“Correct” answers are not sufficient. Analysis must demonstrate critical interrogation of each step by showing justification and verification of intermediate steps using the following:\n\ncustom warning and error message (e.g. warning() and stop(); resources from EDS 221)\nunit tests (e.g. using {testthat}; resources from EDS 221)\ninformative comments (resource from EDS 220)\n\nAll maps and figures must include the following elements:\n\nan informative title\nlegends with legible titles, including units\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\nfor maps: indication of scale and orientation (i.e. graticules/gridlines or scale bar and compass)\nfor figures: axes with legible titles, including units\n\nThe rendered Quarto doc must show all required elements in a professional style with explanation of each step of analysis\n\nsee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW2.html#footnotes",
    "href": "assignments/HW2.html#footnotes",
    "title": "Homework Assignment 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGee, G. C. (2008). A multilevel analysis of the relationship between institutional and individual racial discrimination and health status. American journal of public health, 98(Supplement_1), S48-S56.↩︎\nNardone, A., Rudolph, K. E., Morello-Frosch, R., & Casey, J. A. (2021). Redlines and greenspace: the relationship between historical redlining and 2010 greenspace across the United States. Environmental health perspectives, 129(1), 017006.↩︎\nHoffman, J. S., Shandas, V., & Pendleton, N. (2020). The effects of historical housing policies on resident exposure to intra-urban heat: a study of 108 US urban areas. Climate, 8(1), 12.↩︎\nEllis-Soto, D., Chapman, M., & Locke, D. H. (2023). Historical redlining is associated with increasing geographical disparities in bird biodiversity sampling in the United States. Nature Human Behaviour, 1-9.↩︎\nRobert K. Nelson, LaDale Winling, Richard Marciano, Nathan Connolly, et al., “Mapping Inequality,” American Panorama, ed. Robert K. Nelson and Edward L. Ayers, accessed October 17, 2023, https://dsl.richmond.edu/panorama/redlining/↩︎"
  },
  {
    "objectID": "assignments/SR3.html",
    "href": "assignments/SR3.html",
    "title": "Final Self Reflection (SR #3)",
    "section": "",
    "text": "In this assignment you’ll reflect on your learning through the course. Reread your response to the Pre-Course Self Reflection (SR #1) and Mid-Course Self Reflection (SR #2) and answer the following questions:\n\n\n\nSince SR #2, have you improved any of your existing skills or learned new ones?\nHave you accomplished the learning goals you outlined for this course in SR #1 (and potentially expanded on SR #2)? If you did, how did you do it? If you didn’t why do you think you didn’t?\nDid you learn new strategies for being accomplishing your goals? Will you revise your plan for future quarters?\nWhat are you proudest of accomplishing in this course?\n\n\n\n\n\nWhat are some transferable skills that you developed in this course? How might you apply them to other courses or deliverables in your degree or to jobs in the future?\nWhat is one thing you really liked about this course, and why? What is one thing you think could be improved about this course, and how?\nIs there anything else you’d like the instructors to know about your experience in this course?"
  },
  {
    "objectID": "assignments/SR3.html#description",
    "href": "assignments/SR3.html#description",
    "title": "Final Self Reflection (SR #3)",
    "section": "",
    "text": "In this assignment you’ll reflect on your learning through the course. Reread your response to the Pre-Course Self Reflection (SR #1) and Mid-Course Self Reflection (SR #2) and answer the following questions:\n\n\n\nSince SR #2, have you improved any of your existing skills or learned new ones?\nHave you accomplished the learning goals you outlined for this course in SR #1 (and potentially expanded on SR #2)? If you did, how did you do it? If you didn’t why do you think you didn’t?\nDid you learn new strategies for being accomplishing your goals? Will you revise your plan for future quarters?\nWhat are you proudest of accomplishing in this course?\n\n\n\n\n\nWhat are some transferable skills that you developed in this course? How might you apply them to other courses or deliverables in your degree or to jobs in the future?\nWhat is one thing you really liked about this course, and why? What is one thing you think could be improved about this course, and how?\nIs there anything else you’d like the instructors to know about your experience in this course?"
  },
  {
    "objectID": "assignments/SR3.html#rubric-specifications",
    "href": "assignments/SR3.html#rubric-specifications",
    "title": "Final Self Reflection (SR #3)",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nTo receive a “Satisfactory” score, you must adhere to the following:\n\nSubmit your response here by 11:59 PM on the due date. Extensions can be requested by redeeming tokens.\nYour response must include answers to each question listed in the Description section.\nYou should aim to answer each question in 2-4 sentences.\nYour responses must demonstrate genuine reflection. Obviously low effort responses will receive a “Not Yet”.\n\n\n\n\n\n\n\nTips for a “Satisfactory”\n\n\n\nBy clicking the boxes above, you can keep track of which items you’ve completed! I suggest drafting your response in a word document before copy/pasting into the submission form."
  },
  {
    "objectID": "assignments/SR1.html",
    "href": "assignments/SR1.html",
    "title": "Pre-Course Self Reflection (SR #1)",
    "section": "",
    "text": "In this assignment you’ll introduce yourself, define your learning goals for this course, and outline a plan for how to meet them. You’ll revisit these goals and plans throughout the quarter, so it’s good to be reflective from the start!\n\n\nEveryone brings their own unique background and experiences to the course. So I can best understand your specific context, please address the following:\n\nDo you have a particular career goal in mind? If so, how does this course apply to your future career, if at all?\nHow confident do you feel in your coding skills in R? In other languages? Why?\nHow confident do you feel in your geospatial analysis skills? Why? Have you worked with a GIS before?\nIs there anything you wish your instructors knew about you, but don’t (e.g. responsibilities outside of school)?\n\n\n\n\n\nWhat skills or knowledge do you hope to gain from this course? Why?\nWhat learning objectives from the course are you most excited about? Why?\nMost importantly, how do you plan to accomplish your learning goals for this course? Be specific! Instead of stating “I will complete assignments on time”, reflect on what strategies you will employ. For example, creating a learning schedule, structured collaboration with peers, participating in online learning communities, etc."
  },
  {
    "objectID": "assignments/SR1.html#description",
    "href": "assignments/SR1.html#description",
    "title": "Pre-Course Self Reflection (SR #1)",
    "section": "",
    "text": "In this assignment you’ll introduce yourself, define your learning goals for this course, and outline a plan for how to meet them. You’ll revisit these goals and plans throughout the quarter, so it’s good to be reflective from the start!\n\n\nEveryone brings their own unique background and experiences to the course. So I can best understand your specific context, please address the following:\n\nDo you have a particular career goal in mind? If so, how does this course apply to your future career, if at all?\nHow confident do you feel in your coding skills in R? In other languages? Why?\nHow confident do you feel in your geospatial analysis skills? Why? Have you worked with a GIS before?\nIs there anything you wish your instructors knew about you, but don’t (e.g. responsibilities outside of school)?\n\n\n\n\n\nWhat skills or knowledge do you hope to gain from this course? Why?\nWhat learning objectives from the course are you most excited about? Why?\nMost importantly, how do you plan to accomplish your learning goals for this course? Be specific! Instead of stating “I will complete assignments on time”, reflect on what strategies you will employ. For example, creating a learning schedule, structured collaboration with peers, participating in online learning communities, etc."
  },
  {
    "objectID": "assignments/SR1.html#rubric-specifications",
    "href": "assignments/SR1.html#rubric-specifications",
    "title": "Pre-Course Self Reflection (SR #1)",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nTo receive a “Satisfactory” score, you must adhere to the following:\n\nSubmit your response here by 11:59 PM on the due date. Extensions are not allowed on this first assignment.\nYour response must include answers to each question listed in the Description section.\nYou should aim to answer each question in 2-4 sentences.\nYour responses must demonstrate genuine reflection. Obviously low effort responses will receive a “Not Yet”.\n\n\n\n\n\n\n\nTips for a “Satisfactory”\n\n\n\nBy clicking the boxes above, you can keep track of which items you’ve completed! I suggest drafting your response in a word document before copy/pasting into the submission form."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Important\n\n\n\nAll assignments are due at 11:59 PM on the date listed. Homework Assignments (HWs) and Self-reflections (SRs) are always due on Saturdays to ensure that you have at least one day a week with no course obligations."
  },
  {
    "objectID": "assignments.html#calendar",
    "href": "assignments.html#calendar",
    "title": "Assignments",
    "section": "",
    "text": "Important\n\n\n\nAll assignments are due at 11:59 PM on the date listed. Homework Assignments (HWs) and Self-reflections (SRs) are always due on Saturdays to ensure that you have at least one day a week with no course obligations."
  },
  {
    "objectID": "assignments.html#assignments",
    "href": "assignments.html#assignments",
    "title": "Assignments",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n\n\n\nImportant\n\n\n\nEarning “Satisfactory” marks on Self-reflections (SRs), Homework Assignments (HWs), and the Portfolio Repository (PR) will determine your letter grade (e.g. A, B, etc.) for this course. See Grader Tracker below.\n\n\nLinks to assignments will become available as they are assigned.\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nSR\nPre-course Self Reflection (SR#1)\n09/30/2024\n10/05/2024\n\n\nHW\nHomework Assignment #1\n09/30/2024\n10/05/2024\n\n\nHW\nHomework Assignment #2\n10/07/2024\n10/19/2024\n\n\nHW\nHomework Assignment #3\n10/21/2024\n11/09/2024\n\n\nSR\nMid-course Self Reflection (SR#2)\n10/28/2024\n11/02/2024\n\n\nHW\nHomework Assignment #4\n11/11/2024\n11/30/2024\n\n\nPR\nPortfolio Repository\n11/11/2024\n12/07/2024\n\n\nSR\nFinal Self Reflection (SR#3)\n12/02/2024\n12/07/2024"
  },
  {
    "objectID": "assignments.html#end-of-class-surveys",
    "href": "assignments.html#end-of-class-surveys",
    "title": "Assignments",
    "section": "End-of-class surveys",
    "text": "End-of-class surveys\n\n\n\n\n\n\nImportant\n\n\n\nEnd-of-class surveys (EOCs) will become available at the end of each class (Mondays) and are due by end-of-day (11:59 PM). Completing EOCs by the due dates / times will determine whether you earn a +/- on your course grade. See Grade Tracker below.\n\n\nLinks to surveys will become available as they are assigned.\n\n\n\nEOC link\nDate Assigned\nDate Due\n\n\n\n\nEOC (week 1)\nMon 09/30/2024\nMon 09/30/2024\n\n\nEOC (week 2)\nMon 10/07/2024\nMon 10/07/2024\n\n\nEOC (week 3)\nMon 10/14/2024\nMon 10/14/2024\n\n\nEOC (week 4)\nMon 10/21/2024\nMon 10/21/2024\n\n\nEOC (week 5)\nMon 10/28/2024\nMon 10/28/2024\n\n\nEOC (week 6)\nMon 11/04/2024\nMon 11/04/2024\n\n\nNo lecture or EOC (week 7)\nNA\nNA\n\n\nEOC (week 8)\nMon 11/18/2024\nMon 11/18/2024\n\n\nEOC (week 9)\nMon 11/25/2024\nMon 11/25/2024\n\n\nEOC (week 10)\nMon 12/02/2024\nMon 12/02/2024"
  },
  {
    "objectID": "assignments.html#grade-tracker",
    "href": "assignments.html#grade-tracker",
    "title": "Assignments",
    "section": "Grade Tracker",
    "text": "Grade Tracker\nUse the Grade Tracker, below, to determine your course grade:\n\n\n\n\n\n\n\n\n\nRedeem tokens in exchange for assignment extensions, missing class, or to revise / resubmit an assignment that received a “Not Yet” mark."
  },
  {
    "objectID": "assignments.html#rubric",
    "href": "assignments.html#rubric",
    "title": "Assignments",
    "section": "Rubric",
    "text": "Rubric\nEach Homework Assignment (HWs) will include an individual rubric. However, to earn a “Satisfactory” assignments must adhere to best practices for producing professional output. Below are examples of professional and unprofessional outputs for guidance.\nExamples of Professional Output:\n\nGood Example with sourced functions\nBad Example"
  },
  {
    "objectID": "assignments.html#getting-unstuck",
    "href": "assignments.html#getting-unstuck",
    "title": "Assignments",
    "section": "Getting unstuck",
    "text": "Getting unstuck\n\nWhere to find help\nBeing a great data scientist isn’t about writing perfect code; it’s about learning how to teach yourself and when to ask for help. The only way to get better at this process is to practice by taking the time to troubleshoot on our own, so you should always plan to start there! The graphic below shows the order in which you should approach different resources for help:\n\n\n\n\n\n\n\n\n\n\n\nRoadblock checklist\nIf you hit a roadblock, run through this checklist to make sure you’ve done your due diligence before bringing your question(s) to a peer, TA, or instructor.\n\nrevisit the course materials - your question may already be answered in the slides, textbook, or discussion section materials\nread the documentation - you can do so directly from RStudio by typing ?function_name in the console\nread the package’s vignette, if available - these are often linked on CRAN under the Documents sections (e.g. see {dplyr} on CRAN) or can be found through the command vignette(package = \"package-name\") and vignette(\"vignette-name\")\ntry Googling! - don’t forget to look back at our suggested troubleshooting and Googling tips (Teach Me How to Google)\n\n\n\nHow to ask questions\nWhen you decide to ask a question to a peer, TA, or instructor be sure to:\n\nProvide context. For example, “I’m trying to do this…” or “I’m working on the task where we do this…”\nShare the specific challenge. “I’m specifically trying to [insert function / package] to do this thing.”\nShare what happens and what you’ve learned. “I repeatedly get an error message that says [this]. I’ve tried [this] and [this]”\nShow your code ideally with a reprex that they can run / test.\nValue and expect the Socratic method, especially in classes and workshops – our goal is to provide critical thinking that is transferable, not just to provide a quick fix for a single error."
  },
  {
    "objectID": "assignments/SR2.html",
    "href": "assignments/SR2.html",
    "title": "Mid-Course Self Reflection (SR #2)",
    "section": "",
    "text": "In this assignment you’ll reflect on your progress in the course so far. Reread your response to the Pre-Course Self Reflection (SR #1) and answer the following questions:\n\n\n\nHow have you progressed towards your learning goals for this course?\nLooking back, have your learning goals changed? If so, how?\nLooking forward, do you have new goals?\nWhat skills are you proud of developing?\n\n\n\n\n\nLooking back at your plan for achieving your learning goals, have your strategies been effective?\nHave you discussed your plans with your classmates?\nLooking forward, will you adopt new strategies? If so, what will they be?\nIf you’ve needed help, how have you sought it out? If you haven’t sought out help, why not?\n\n\n\n\n\nWhat topics have excited you the most so far? Have you spent time outside of class diving deeper into any of these topics? If so, what have you learned?\nIs there anything about this course that you are really enjoying? Anything that isn’t working for you?\nWhat else would you like me to know about your experience so far?"
  },
  {
    "objectID": "assignments/SR2.html#description",
    "href": "assignments/SR2.html#description",
    "title": "Mid-Course Self Reflection (SR #2)",
    "section": "",
    "text": "In this assignment you’ll reflect on your progress in the course so far. Reread your response to the Pre-Course Self Reflection (SR #1) and answer the following questions:\n\n\n\nHow have you progressed towards your learning goals for this course?\nLooking back, have your learning goals changed? If so, how?\nLooking forward, do you have new goals?\nWhat skills are you proud of developing?\n\n\n\n\n\nLooking back at your plan for achieving your learning goals, have your strategies been effective?\nHave you discussed your plans with your classmates?\nLooking forward, will you adopt new strategies? If so, what will they be?\nIf you’ve needed help, how have you sought it out? If you haven’t sought out help, why not?\n\n\n\n\n\nWhat topics have excited you the most so far? Have you spent time outside of class diving deeper into any of these topics? If so, what have you learned?\nIs there anything about this course that you are really enjoying? Anything that isn’t working for you?\nWhat else would you like me to know about your experience so far?"
  },
  {
    "objectID": "assignments/SR2.html#rubric-specifications",
    "href": "assignments/SR2.html#rubric-specifications",
    "title": "Mid-Course Self Reflection (SR #2)",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nTo receive a “Satisfactory” score, you must adhere to the following:\n\nSubmit your response here by 11:59 PM on the due date. Extensions can be requested by redeeming tokens.\nYour response must include answers to each question listed in the Description section.\nYou should aim to answer each question in 2-4 sentences.\nYour responses must demonstrate genuine reflection. Obviously low effort responses will receive a “Not Yet”.\n\n\n\n\n\n\n\nTips for a “Satisfactory”\n\n\n\nBy clicking the boxes above, you can keep track of which items you’ve completed! I suggest drafting your response in a word document before copy/pasting into the submission form."
  },
  {
    "objectID": "assignments/HW1.html",
    "href": "assignments/HW1.html",
    "title": "Homework Assignment 1",
    "section": "",
    "text": "Important\n\n\n\nYou must earn a “Satisfactory” on all parts of the assignment to earn a “Satisfactory” on the assignment.\nThe assignment must be submitted through GitHub Classrooms. Each student receives one “free pass” for not submitting assignments via specified channels, after which you will receive a “Not Yet” mark.\nRead each part of the assignment carefully, and use the check boxes to ensure you’ve addressed all elements of the assignment!"
  },
  {
    "objectID": "assignments/HW1.html#learning-outcomes",
    "href": "assignments/HW1.html#learning-outcomes",
    "title": "Homework Assignment 1",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nbuild effective, responsible, accessible and aesthetically-pleasing maps\npractice manipulating vector and raster data to build multi-layer maps\npractice making maps in R, specifically using tmap"
  },
  {
    "objectID": "assignments/HW1.html#instructions",
    "href": "assignments/HW1.html#instructions",
    "title": "Homework Assignment 1",
    "section": "Instructions",
    "text": "Instructions\n\nClone repository from GitHub Classrooms\nDownload data from here\nUnzip data and place in repository\nEdit Quarto document with responses\nPush final edits before deadline\n\nYour repository should have the following structure:\n\nEDS223-HW1\n│   README.md\n│   HW1.qmd\n│   Rmd/Proj files    \n│\n└───data\n     └───easter-island\n     └───ejscreen"
  },
  {
    "objectID": "assignments/HW1.html#part-1-easter-island-from-land-to-sea",
    "href": "assignments/HW1.html#part-1-easter-island-from-land-to-sea",
    "title": "Homework Assignment 1",
    "section": "Part 1: Easter Island from land to sea",
    "text": "Part 1: Easter Island from land to sea\nIn this week’s discussion section, you practiced making a map of Easter Island. We will continue by extending our map into the sea by including bathymetry and seamounts.\n\nDescription\nCreate a map of Easter Island, including the following:\n\nthe island’s border and road network\nelevation of the island (indicated in meters above sea level)\nlocation of the island’s volcanoes (indicating their height in meters)\nbathymetry surrounding the island (indicated in meters below sea level)\nsurrounding seamounts (indicating their height in meters)\n\n\n\nData details\nYou will use the following new datasets:\n\nSeamounts from the Pacific Data Hub\nBathymetry (or seadepth) from NOAA’s marmap API\n\nThe following code is provided to read in each dataset:\n\nlibrary(sf)\nlibrary(here)\nlibrary(terra)\n\n# read in major points on the island\nei_points &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_points.gpkg\"))\n# subset points to volcanoes\nvolcanoes &lt;- subset(ei_points, type == \"volcano\")\n\n# read in island elevation\nei_elev &lt;- terra::rast(here::here(\"data\", \"easter_island\", \"ei_elev.tif\"))\n\n# read in island border\nei_borders &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\n\n# read in island road network\nei_roads &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))\n\n# read in bathymetry\nei_bathymetry &lt;- terra::rast(here::here(\"data\", \"easter_island\", \"ei_bathy.tif\"))\n\n# read in seamounts\nei_seamounts &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_seamounts.gpkg\"))\n\n\n\n\n\n\n\nSetting a bounding box for the map\n\n\n\nTo make sure that all the bathymetry and seamount data are included in the map, create a bounding box that includes both.\n\n# define larger plot bbox that is buffered around both of \n# the two largest layers to display all 4 seamounts in view \n\nbbox_seamount &lt;- st_bbox(ei_seamounts) # seamount bounding box\nbbox_bathymetry &lt;- st_bbox(ei_bathymetry) # bathymetry bounding box\nbbox_largest &lt;- st_bbox(c(xmin = min(bbox_bathymetry[1], bbox_seamount[1]),\n                         ymin = min(bbox_bathymetry[2], bbox_seamount[2]),\n                         xmax = max(bbox_bathymetry[3], bbox_seamount[3]),\n                         ymax = max(bbox_bathymetry[4], bbox_seamount[4])))\n\nNow that you’ve defined the bounding box, make sure to use it!"
  },
  {
    "objectID": "assignments/HW1.html#part-2-exploring-environmental-injustice",
    "href": "assignments/HW1.html#part-2-exploring-environmental-injustice",
    "title": "Homework Assignment 1",
    "section": "Part 2: Exploring environmental (in)justice",
    "text": "Part 2: Exploring environmental (in)justice\nAs many of us are aware, environmental degradation, pollution, and hazards is not felt equally by all people. Environmental justice has many definitions, but the United States Environmental Protection Agency defines it as follows:\n\n“Environmental justice” means the just treatment and meaningful involvement of all people, regardless of income, race, color, national origin, Tribal affiliation, or disability, in agency decision-making and other Federal activities that affect human health and the environment so that people:\n\nare fully protected from disproportionate and adverse human health and environmental effects (including risks) and hazards, including those related to climate change, the cumulative impacts of environmental and other burdens, and the legacy of racism or other structural or systemic barriers; and\nhave equitable access to a healthy, sustainable, and resilient environment in which to live, play, work, learn, grow, worship, and engage in cultural and subsistence practices\n\n\nFor far too long, environmental inequities have been invisible, and thus ignored. Mapping environmental inequities can be a powerful tool for revealing injustices. We will be working with data from the United States Environmental Protection Agency’s EJScreen: Environmental Justice Screening and Mapping Tool.\nAccording to the US EPA website:\n\nThis screening tool and data may be of interest to community residents or other stakeholders as they search for environmental or demographic information. It can also support a wide range of research and policy goals. The public has used EJScreen in many different locations and in many different ways.\nEPA is sharing EJScreen with the public:\n\nto be more transparent about how we consider environmental justice in our work,\n\nto assist our stakeholders in making informed decisions about pursuing environmental justice and,\n\nto create a common starting point between the agency and the public when looking at issues related to environmental justice.\n\n\n\nDescription\nFor this assignment, you will explore an environmental justice topic of your choosing. You should select a region, community, or environmental issue that matters to you.\nYou must complete the following:\n\ncreate two maps that communicate an environmental justice issue\nwrite a brief paragraph explaining what your maps communicate\n\n\n\nData details\nEJScreen provides on environmental and demographic information for the US at the Census tract and block group levels. You will be working with data at the block group level that has been downloaded from the EPA site. To understand the associated data columns, you will need to explore the following in the data folder:\n\nTechnical documentation: ejscreen-tech-doc-version-2-2.pdf\nColumn descriptions: EJSCREEN_2023_BG_Columns.xlsx\n\nYou should also explore the limitations and caveats of the data.\nThe following code provides examples for reading and manipulating the EJScreen data. You MUST update this code to suite your problem of interest.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# read in geodatabase of EJScreen data at the Census Block Group level\nejscreen &lt;- sf::st_read(here::here(\"data\", \"ejscreen\",\"EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\")) \n\n# filter to a state you are interested in\ncalifornia &lt;- ejscreen %&gt;%\n  dplyr::filter(ST_ABBREV == \"CA\") \n\n# filter to a county you are interested in\nsanta_barbara &lt;- ejscreen %&gt;%\n  dplyr::filter(CNTY_NAME %in% c(\"Santa Barbara County\"))\n\n# find the average values for all variables within counties\ncalifornia_counties &lt;- aggregate(california, by = list(california$CNTY_NAME), FUN = mean)"
  },
  {
    "objectID": "assignments/HW1.html#rubric-specifications",
    "href": "assignments/HW1.html#rubric-specifications",
    "title": "Homework Assignment 1",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nYou must complete the following to receive a “Satisfactory” on the assignment:\n\nAll maps must include the following elements:\n\nan informative title\nlegends with legible titles, including units\nindication of scale and orientation (i.e. graticules/gridlines or scale bar and compass)\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\n\nEJScreen maps and text must communicate about an issue\n\nit is not sufficient to make two unrelated maps\n\nThe rendered Quarto doc must show all required elements in a professional style\n\nsee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW3.html#learning-outcomes",
    "href": "assignments/HW3.html#learning-outcomes",
    "title": "Homework Assignment 3",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThis assignment will reinforce key concepts in geospatial analysis by practicing the following:\n\nload vector/raster data\n\nsimple raster operations\n\nsimple vector operations\n\nspatial joins"
  },
  {
    "objectID": "assignments/HW3.html#instructions",
    "href": "assignments/HW3.html#instructions",
    "title": "Homework Assignment 3",
    "section": "Instructions",
    "text": "Instructions\n\nClone repository from GitHub Classrooms\nDownload data from here\nUnzip data and place in repository\nEdit Quarto document with responses\nPush final edits before deadline\n\nYour repository should have the following structure:\n\nEDS223-HW3\n│   README.md\n│   Rmd/Proj files    \n│\n└───data\n    │   gis_osm_buildings_a_free_1.gpkg\n    │   gis_osm_roads_free_1.gpkg\n    │\n    └───ACS_2019_5YR_TRACT_48_TEXAS.gdb\n    |   │   census tract gdb files\n    |\n    └───VNP46A1\n    |   │   VIIRS data files"
  },
  {
    "objectID": "assignments/HW3.html#background",
    "href": "assignments/HW3.html#background",
    "title": "Homework Assignment 3",
    "section": "Background",
    "text": "Background\nClimate change is increasing the frequency and intensity of extreme weather events, with devastating impacts. “In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1 For more background, check out these engineering and political perspectives.\nIn this assignment, you will identify the impacts of these series of extreme winter storms by estimating the number of homes in the Houston metropolitan area that lost power and investigate whether not these impacts were disproportionately felt.\nYour analysis will be based on remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. In particular, you will use the VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nTo determine the number of homes that lost power, you link (spatially join) these areas with OpenStreetMap data on buildings and roads.\nTo investigate potential socioeconomic factors that influenced recovery, you will link your analysis with data from the US Census Bureau."
  },
  {
    "objectID": "assignments/HW3.html#description",
    "href": "assignments/HW3.html#description",
    "title": "Homework Assignment 3",
    "section": "Description",
    "text": "Description\nFor this assignment, you must produce the following:\n\na set of maps comparing night light intensities before and after the first to storms\na map of the homes in in Houston that lost power\nan estimate of the number of homes in Houston that lost power\na map of the census tracts in Houston that lost power\na plot comparing the distributions of median household income for census tracts that did and did not experience blackouts\na brief reflection (approx. 100 words) summarizing your results and discussing any limitations to this study"
  },
  {
    "objectID": "assignments/HW3.html#data-details",
    "href": "assignments/HW3.html#data-details",
    "title": "Homework Assignment 3",
    "section": "Data details",
    "text": "Data details\n\nNight lights\nUse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06. We therefore need to download two tiles per date.\nAs you’re learning in EDS 220, accessing, downloading, and preparing remote sensing data is a skill in it’s own right! To prevent this assignment from being a large data wrangling challenge, we have downloaded and prepped the following files for you to work with, stored in the VNP46A1 folder.\n\nData files:\n\nVNP46A1.A2021038.h08v05.001.2021039064328.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.tif: tile h08v06, collected on 2021-02-16\n\n\n\nRoads\nData file: gis_osm_roads_free_1.gpkg Typically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, we will ignore areas near highways.\nOpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. We used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \n\n\nHouses\nData file: gis_osm_buildings_a_free_1.gpkg\nWe can also obtain building data from OpenStreetMap. We again downloaded from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\n\n\n\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file.\n\nYou can use st_layers() to explore the contents of the geodatabase. Each layer contains a subset of the fields documents in the ACS metadata.\n\nThe geodatabase contains a layer holding the geometry information (ACS_2019_5YR_TRACT_48_TEXAS), separate from the layers holding the ACS attributes. You have to combine the geometry with the attributes to get a feature layer that sf can use.\n\n\n\n\n\n\nTip\n\n\n\nMake sure to check that these datasets have the same coordinate reference systems! If not, transform them to match."
  },
  {
    "objectID": "assignments/HW3.html#workflow-outline",
    "href": "assignments/HW3.html#workflow-outline",
    "title": "Homework Assignment 3",
    "section": "Workflow outline",
    "text": "Workflow outline\nTo complete complete the tasks of this assignment, you will need to break your analysis into the following key steps:\n\nfind locations that experienced a blackout by creating a mask\nexclude highways from analysis\nidentify homes that experienced blackouts by combining the locations of homes and blackouts\nidentify the census tracts likely impacted by blackout\n\nBelow is guidance and suggestions for each of these steps.\n\n\n\n\n\n\nTip\n\n\n\nFor improved computational efficiency and easier interoperability with sf, I recommend using the stars package for raster handling.\n\n\n\nCreate blackout mask\nTo identify places that experienced a blackout, you should create a “mask” that indicates for each cell whether or not it experienced a blackout.\n\nfind the change in night lights intensity (presumably) caused by the storm\n\nhint: this will require creating a raster object for each day (2021-02-07 and 2021-02-16)\n\nreclassify the difference raster, assuming that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout\nassign NA to all locations that experienced a drop of less than 200 nW cm-2sr-1 change\nvectorize the blackout mask\n\nhint: use st_as_sf() to convert from a raster to a vector and fix any invalid geometries with st_make_valid()\n\ncrop (spatially subset) the blackout mask to the Houston area as defined by the following coordinates:\n\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\n\nre-project the cropped blackout dataset to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area)\n\n\n\nExclude highways from the cropped blackout mask\nHighways may have experienced changes in their night light intensities that are unrelated to the storm. Therefore, you should excluded any locations within 200 meters of all highways in the Houston area.\n\nidentify areas within 200m of all highways\n\nhint: you may need to use st_union\n\nfind areas that experienced blackouts that are further than 200m from a highway\n\n\n\n\n\n\n\nTip\n\n\n\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\nBelow is a SQL query that can be used as an argument in st_read:\n\n\"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n\n\n\n\nIdentify homes likely impacted by blackouts\n\nidentify homes that overlap with areas that experienced blackouts\n\n\n\n\n\n\n\nTip\n\n\n\nThe buildings geopackage includes data on many types of buildings. As with the roads data, we can avoid reading in data we don’t need.\nBelow is a SQL query that can be used as an argument in st_read:\n\n\"SELECT *\nFROM gis_osm_buildings_a_free_1`\nWHERE (type IS NULL AND name IS NULL)`\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n\n\n\n\nIdentify the census tracts likely impacted by blackout\n\njoin the median household income from the previous 12 months to the census tract geometries\n\nhint: make sure to join by geometry ID\n\nidentify census tracts that that contained homes that experienced blackouts"
  },
  {
    "objectID": "assignments/HW3.html#rubric-specifications",
    "href": "assignments/HW3.html#rubric-specifications",
    "title": "Homework Assignment 3",
    "section": "Rubric (specifications)",
    "text": "Rubric (specifications)\nYour output should serve as a stand-alone item that someone unfamiliar with the assignment would be able to understand your analysis, including the decisions made in selecting your approach and interpretation of the results.\nAssignments will be deemed “Satisfactory” based on the following criteria:\n\nData analysis\n\nCode must produce expected output based on correct data manipulation\nMultiple approaches can be used to reach the correct output. Analysis must demonstrate critical interrogation of the approach used by showing justification and verification of intermediate steps. “Correct” answers are not sufficient. Justification and verification of approach should be demonstrated using the following:\n\ncustom warning and error message (e.g. warning() and stop(); resources from EDS 221)\ninformative comments (resource from EDS 220)\nBONUS: unit tests (e.g. using {testthat}; resources from EDS 221)\n\n\n\n\nPlots, tables, and maps\n\nAll plots, tables, and maps must be clear, accurate, and effectively convey the intended information\nAll plots and maps must include the following elements:\n\nan informative title\nlegends with legible titles, including units\ncolor scales that are accessible (i.e. make intuitive sense) and appropriate to the data (i.e. discrete vs. continuous)\nfor maps: indication of scale and orientation (i.e. graticules/gridlines or scale bar and compass)\nfor figures: axes with legible labels and titles, including units\n\nAll tables should be rendered with legible titles and stypling (e.g. {kableExtra})\n\n\n\nWritten reflections\n\nReflections must be clear, accurate, and demonstrate a deep understanding of the analysis performed.\n\n\n\nProfessional output\n\nQuarto document must be rendered to html\nThe rendered output must include the following elements:\n\ndocument header with title, name, and date (ideally the date rendered)\nall packages are loaded together at the top of the document\nall unnecessary/distracting warnings and messages are suppressed\ninclude informative code comments when appropriate (resource from EDS 220)\nfolding code or sourcing separate scripts when appropriate to direct reader’s attention\nsuccinct documentation between steps, such as section headers, descriptions for an analysis and map/plot interpretation\ncomplete and detailed data citations\nall defined variables are used in analysis (no variables are defined that are not used)\n\n\n\n\n\n\n\n\nGuide to professional output\n\n\n\nSee examples of professional and unprofessional output on the Assignments page"
  },
  {
    "objectID": "assignments/HW3.html#footnotes",
    "href": "assignments/HW3.html#footnotes",
    "title": "Homework Assignment 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the spatial modeling and analytic techniques of geographic information science to data science students. The emphasis is on deep understanding of spatial data models and the analytic operations they enable. Recognizing remotely sensed data as a key data type within environmental data science, this course will also introduce fundamental concepts and applications of remote sensing. In addition to this theoretical background, students will become familiar with libraries, packages, and APIs that support spatial analysis in R."
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nLecture: Monday 12:30-3:15 PM at Bren Hall 1424\nDiscussion Section 1: Wednesday 2:00-2:50 PM in Bren Hall 3022\nDiscussion Section 2: Wednesday 3:00-3:50 PM in Bren Hall 3022"
  },
  {
    "objectID": "index.html#readings-and-references",
    "href": "index.html#readings-and-references",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Readings and References",
    "text": "Readings and References\n\nGeocompuation with R\nSpatial Data Science with Applications in R\nA Gentle Introduction to GIS"
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nMeet minimum MEDS device requirements\nInstall or update to R version 4.40\nInstall or update RStudio\nCreate a GitHub account"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n\nRuth Oliver\nEmail: rutholiver@ucsb.edu\nLearn more: ryoliver-lab.github.io\n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\n\n\nAlessandra Vidal Meza\nEmail: avidalmeza@ucsb.edu\nLearn more: avidalmeza.github.io"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Geospatial Analysis & Remote Sensing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis website was designed by Sam Csik."
  },
  {
    "objectID": "course-materials/labs/week9.html",
    "href": "course-materials/labs/week9.html",
    "title": "Week 9: Lab",
    "section": "",
    "text": "Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\nClassifying remotely sensed imagery into land cover classes enables us to understand the distribution and change in land cover types over large areas.\nThere are many approaches for performing land cover classification:\nThis lab is based on a materials developed by Chris Kibler."
  },
  {
    "objectID": "course-materials/labs/week9.html#task",
    "href": "course-materials/labs/week9.html#task",
    "title": "Week 9: Lab",
    "section": "Task",
    "text": "Task\nIn this lab, we are using a form of supervised classification, a decision tree classifier.\nDecision trees classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data.\nIn this lab, we will create a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types: (1) green vegetation; (2) dry grass or soil; (3) urban; and (4) water.\nOur goals in this lab are:\n\nLoad and process Landsat scene\nCrop and mask Landsat data to study area\nExtract spectral data at training sites\nTrain and apply decision tree classifier\nPlot results"
  },
  {
    "objectID": "course-materials/labs/week9.html#data",
    "href": "course-materials/labs/week9.html#data",
    "title": "Week 9: Lab",
    "section": "Data",
    "text": "Data\nLandsat 5 Thematic Mapper\n\nLandsat 5\n1 scene from September 25, 2007\n\nBands: 1, 2, 3, 4, 5, 7\nCollection 2 surface reflectance product\n\nStudy area and training data\n\nPolygon representing southern Santa Barbara county\nPolygons representing training sites\n\ntype: character string with land cover type"
  },
  {
    "objectID": "course-materials/labs/week9.html#set-up",
    "href": "course-materials/labs/week9.html#set-up",
    "title": "Week 9: Lab",
    "section": "Set Up",
    "text": "Set Up\nWe’ll be working with vector and raster data, so will need both sf and terra. To train our classification algorithm and plot the results, we’ll use the rpart and rpart.plot packages.\nSet your working directory to the folder that holds the data for this lab.\n- Note: my filepaths may look different than yours!\nLet’s load all necessary packages:\n\nlibrary(sf)\nlibrary(terra)\nlibrary(here)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tmap)"
  },
  {
    "objectID": "course-materials/labs/week9.html#load-landsat-data",
    "href": "course-materials/labs/week9.html#load-landsat-data",
    "title": "Week 9: Lab",
    "section": "Load Landsat Data",
    "text": "Load Landsat Data\nLet’s create a raster stack. Each file name ends with the band number (e.g. B1.tif).\n\nNotice that we are missing a file for band 6\nBand 6 corresponds to thermal data, which we will not be working with for this lab\n\nTo create a raster stack, we will create a list of the files that we would like to work with and read them all in at once using the rast function. We’ll then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n# list files for each band, including the full file path\nfilelist &lt;- list.files(here::here(\"course-materials\", \"data\", \"week9\", \"landsat-data\"), full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat_20070925 &lt;- rast(filelist)\n\n# update layer names to match band\nnames(landsat_20070925) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat_20070925, r = 3, g = 2, b = 1, stretch = \"lin\")"
  },
  {
    "objectID": "course-materials/labs/week9.html#load-study-area",
    "href": "course-materials/labs/week9.html#load-study-area",
    "title": "Week 9: Lab",
    "section": "Load Study Area",
    "text": "Load Study Area\nWe want to constrain our analysis to the southern portion of the county where we have training data, so we’ll read in a file that defines the area we would like to study.\n\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9\", \"SB_county_south.shp\"))\n\n# project to match the Landsat data\nSB_county_south &lt;- st_transform(SB_county_south, crs = crs(landsat_20070925))"
  },
  {
    "objectID": "course-materials/labs/week9.html#crop-and-mask-landsat-data-to-study-area",
    "href": "course-materials/labs/week9.html#crop-and-mask-landsat-data-to-study-area",
    "title": "Week 9: Lab",
    "section": "Crop and Mask Landsat Data to Study Area",
    "text": "Crop and Mask Landsat Data to Study Area\nNow, we can crop and mask the Landsat data to our study area.\n\nWhy? This reduces the amount of data we’ll be working with and therefore saves computational time\nBonus: We can also remove any objects we’re no longer working with to save space\n\n\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat_20070925, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat_20070925, SB_county_south, landsat_cropped)"
  },
  {
    "objectID": "course-materials/labs/week9.html#convert-landsat-values-to-reflectance",
    "href": "course-materials/labs/week9.html#convert-landsat-values-to-reflectance",
    "title": "Week 9: Lab",
    "section": "Convert Landsat Values to Reflectance",
    "text": "Convert Landsat Values to Reflectance\nNow we need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\nIn this case, we are working with Landsat Collection 2.\n\nThe valid range of pixel values for this collection goes from 7,273 to 43,636…\n\nwith a multiplicative scale factor of 0.0000275\nwith an additive scale factor of -0.2\n\n\nLet’s reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%!\n\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat &lt;- (landsat * 0.0000275 - 0.2) * 100\n\n# plot true color image to check results\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n# check values are 0 - 100\nsummary(landsat)"
  },
  {
    "objectID": "course-materials/labs/week9.html#classify-image",
    "href": "course-materials/labs/week9.html#classify-image",
    "title": "Week 9: Lab",
    "section": "Classify Image",
    "text": "Classify Image\nLet’s begin by extracting reflectance values for training data!\n\nWe will load the shapefile identifying different locations within our study area as containing one of our 4 land cover types.\nWe can then extract the spectral values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\n# read in and transform training data\ntraining_data &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9\", \"trainingdata.shp\")) %&gt;%\n  st_transform(., crs = crs(landsat))\n\n# extract reflectance values at training sites\ntraining_data_values &lt;- extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %&gt;%\n  mutate(type = as.factor(type)) # convert landcover type to factor\n\nNext, let’s train the decision tree classifier!\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are).\n\nThe rpart function implements the CART algorithm\nThe rpart function needs to know the model formula and training data you would like to use\nBecause we are performing a classification, we set method = \"class\"\nWe also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\nTo understand how our decision tree will classify pixels, we can plot the results!\n\nNote: The decision tree is comprised of a hierarchy of binary decisions. Each decision rule has 2 outcomes based on a conditional statement pertaining to values in each spectral band.\n\n\n# establish model formula\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n                          data = SB_training_data,\n                          method = \"class\",\n                          na.action = na.omit)\n\n# plot decision tree\nprp(SB_decision_tree)\n\n…and apply the decision tree!\nThe terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n# classify image based on decision tree\nSB_classification &lt;- predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)"
  },
  {
    "objectID": "course-materials/labs/week9.html#plot-results",
    "href": "course-materials/labs/week9.html#plot-results",
    "title": "Week 9: Lab",
    "section": "Plot Results",
    "text": "Plot Results\nNow we can plot the results and check out our land cover map!\n\n# plot results\ntm_shape(SB_classification) +\n  tm_raster(col.scale = tm_scale_categorical(values = c(\"#8DB580\", \"#F2DDA4\", \"#7E8987\", \"#6A8EAE\")),\n            col.legend = tm_legend(labels = c(\"green vegetation\", \"soil/dead grass\", \"urban\", \"water\"),\n                                   title = \"Landcover type\")) +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "course-materials/labs/week2.html",
    "href": "course-materials/labs/week2.html",
    "title": "Week 2 Lab",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from Chapter 3 of Geocomputation with R and the tmap book.\nIn this lab, we’ll explore the basics of manipulating vector data in R using the sf package."
  },
  {
    "objectID": "course-materials/labs/week2.html#set-up",
    "href": "course-materials/labs/week2.html#set-up",
    "title": "Week 2 Lab",
    "section": "1. Set up",
    "text": "1. Set up\nInstall a new package to take advantage of some preloaded data.\n\ninstall.packages(\"spData\")\n\nLet’s load all necessary packages:\n\nrm(list = ls())\nlibrary(sf) # for handling vector data\nlibrary(tmap) # for making maps\nlibrary(tidyverse) # because we love the tidyverse\nlibrary(spData) # preloaded spatial data"
  },
  {
    "objectID": "course-materials/labs/week2.html#simple-features-in-sf",
    "href": "course-materials/labs/week2.html#simple-features-in-sf",
    "title": "Week 2 Lab",
    "section": "2. Simple features in sf",
    "text": "2. Simple features in sf\nSimple features is a hierarchical data model that represents a wide range of geometry types. The sf package can represent all common vector geometry types:\n\npoints\nlines\npolygons\nand their respective ‘multi’ versions\n\nsfprovides the same functionality that the sp, rgdal, and rgeos packages provided, but is more intuitive because it builds on the tidy data model and works well with the tidyverse. sf represents spatial objects as “simple feature” objects by storing them as a data frame with the geographic data stored in a special column (usually named geom or geometry).\n\nSimple features from scratch\nLet’s start by looking at how we can construct a sf object. Typically we will load sf objects by reading in data. However, it can be helpful to see how sf objects are created from scratch.\nFirst, we create a geometry for London by supplying a point and coordinate reference system.\n\n# create st_point with longitude and latitude for London\n# simple feature geometry\nlondon_point &lt;- st_point(c(0.1, 51.5))\n\n# add coordinate reference system\n# simple feature collection\nlondon_geom &lt;- st_sfc(london_point, crs = 4326)\n\nThen, we supply some non-geographic attributes by creating a data frame with attributes about London.\n\n# create data frame of attributes about London\nlondon_attrib &lt;- data.frame(\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nAnd we attach the simple feature collection and data frame to create a sf object. Check out the class of the new object we created.\n\n# combine geometry and data frame\n# simple feature object\nlondon_sf &lt;- st_sf(london_attrib, geometry = london_geom)\n\n# check class\nclass(london_sf)\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe can also check out what the CRS looks like:\n\nst_crs(london_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\nst_crs(london_sf)$IsGeographic\n\n[1] TRUE\n\nst_crs(london_sf)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n\n\nExisting sf object\nNow let’s look at an existing sf object representing countries of the world:\n\nworld &lt;- spData::world\nclass(world)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(world)\n\n[1] 177  11\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nWe can see that this object contains both spatial data (geom column) and attributes about those geometries. We can perform operations on the attribute data, just like we would with a normal data frame.\n\nsummary(world$lifeExp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  50.62   64.96   72.87   70.85   76.78   83.59      10 \n\n\nThe geometry column is “sticky”, meaning it will stick around unless we explicitly get rid of it. For example, dplyr’s select() function won’t get rid of it.\n\nworld_df &lt;- world %&gt;%\n  select(-geom) #doesn't actually remove the geom column\n\ncolnames(world_df) # geom still shows up as a column\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n\nTo drop the geom column and convert this sf object into a data frame, we need to drop the geometry column using the st_drop_geometry().\n\nworld_df &lt;- st_drop_geometry(world)\nclass(world_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_df)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\"\n\nncol(world)\n\n[1] 11\n\nncol(world_df)\n\n[1] 10\n\n\n\n\n\n\n\n\nsf syntax\n\n\n\nNote that all functions in the sf package start with the prefix st_ NOT sf_. Why? st_ stands for “spatiotemporal” as in data that varies in space and time."
  },
  {
    "objectID": "course-materials/labs/week2.html#coordinate-reference-systems-and-projections",
    "href": "course-materials/labs/week2.html#coordinate-reference-systems-and-projections",
    "title": "Week 2 Lab",
    "section": "3. Coordinate reference systems and projections",
    "text": "3. Coordinate reference systems and projections\nR handles coordinate reference systems using multiple formats:\n\nan identifying string specifying the authority and code such as EPSG:4325\n\nthese need to be passed as strings\nsf will accept the four digit code as an integer\n\nproj4strings are now outdated, but you might see them around\n\nfor example, +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\n\nReprojecting data\nIn some cases we will be working with data which is represented with different coordinate reference systems (CRS). Whenever we work with multiple spatial data objects, we need to check that the CRSs match.\nLet’s create another sf object for London, but now represented with a project coordinate system.\n\nlondon_proj = data.frame(x = 530000, y = 180000) %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = \"EPSG:27700\")\n\nWe can check the CRS of any data using the st_crs() function.\n\nst_crs(london_proj)\n\nCoordinate Reference System:\n  User input: EPSG:27700 \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThis is a lot of information to read, so if we wanted to use this point with our other London point, we need to check to see if they are using the same CRS.\n\nst_crs(london_proj) == st_crs(london_sf)\n\n[1] FALSE\n\n\nTo transform the CRS of a dataset, we use the st_transform() function. In the crs argument, we need to specify the coordinate reference system. We can do this by either supplying a CRS code or specifying the CRS of another dataset using the st_crs() function.\n\nlondon_sf_transform &lt;- st_transform(london_sf, crs = st_crs(london_proj))\n\nNow if we check, the CRS between the two datasets should match\n\nif(st_crs(london_sf_transform) == st_crs(london_proj)){\n  print(\"it's a match!\")\n} else {\n  print(\"still not a match\")\n}\n\n[1] \"it's a match!\"\n\n\n\n\n\n\n\n\nBuilding beautiful workflows\n\n\n\nHopefully we’re already thinking about how we could build checking coordinate reference systems into our workflows.\nFor example, we could add code like the following that transforms the CRS of dataset2 to match dataset1 and prints out a warning message.\n\nif(st_crs(dataset1) != st_crs(dataset2)){\n  warning(\"coordinate refrence systems do not match\")\n  dataset2 &lt;- st_transform(dataset1, crs = st_crs(dataset1))\n}\n\n\n\n\n\nChanging map projections\nRemember that whenever we make a map we are trying to display three dimensional data with only two dimensions. To display 3D data in 2D, we use projections. Which projection you use can have big implications for how you display information.\nTo the projection of our data, we could:\n\nreproject the underlying data\nor in tmap we can specify the projection we want the map to use\n\nLet’s compare global maps using two different projections:\n\nEqual Earth is an equal-area pseudocylindrical projection (EPSG 8857)\nMercator is a conformal cylindrical map that preserves angles (EPSG 3395)\n\n\ntm_shape(world, projection = 8857) +\n  tm_fill(col = \"area_km2\")\n\n\n\n\n\n\n\ntm_shape(world, projection = 3395) +\n  tm_fill(col = \"area_km2\")"
  },
  {
    "objectID": "course-materials/labs/week2.html#vector-attribute-subsetting",
    "href": "course-materials/labs/week2.html#vector-attribute-subsetting",
    "title": "Week 2 Lab",
    "section": "4. Vector attribute subsetting",
    "text": "4. Vector attribute subsetting\nOften we’ll want to manipulate sf objects in the same ways as we might with tabular data in data frames. The great thing about the simple features data model, is we can largely treat spatial objects the same as data frames.\n\ndplyr functions!\nThis means that we can use all of our favorite dplyr functions on sf objects – yay!\nWe can select columns…\n\nworld %&gt;%\n  select(name_long, pop)\n\nOr remove columns…\n\nworld %&gt;%\n  select(-subregion, -area_km2)\n\nOr select AND rename columns\n\nworld %&gt;%\n  select(name = name_long, population = pop)\n\nOr filter observations based on variables\n\nworld1 &lt;- world %&gt;%\n  filter(area_km2 &lt; 10000)\n\nsummary(world1$area_km2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2417    4412    6207    5986    7614    9225 \n\nworld2 &lt;- world %&gt;%\n  filter(lifeExp &gt;= 80)\n\nnrow(world2)\n\n[1] 24\n\n\n\n\nChaining commands with pipes\nBecause we can use dplyr functions with sf objects, we can chain together commands using the pipe operator.\nLet’s try to find the country in Asia with the highest life expectancy\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;%\n  select(name_long, continent, lifeExp) %&gt;%\n  slice_max(lifeExp) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 1 × 3\n  name_long continent lifeExp\n* &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 Japan     Asia         83.6\n\n\n\n\nVector attribute aggregation\nAggregation is the process of summarizing data with one or more ‘grouping’ variables. For example, using the ‘world’ which provides information on countries of the world, we might want to aggregate to the level of continents. It is important to note that aggregating data attributes is a different process from aggregating geographic data, which we will cover later.\nLet’s try to find the total population within each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE)) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 8 × 2\n  continent               population\n* &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811\n\n\nLet’s also find the total area and number of countries in each continent:\n\nworld %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  st_drop_geometry()\n\n# A tibble: 8 × 4\n  continent               population  area_km2 n_countries\n* &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;\n1 Africa                  1154946633 29946198.          51\n2 Antarctica                       0 12335956.           1\n3 Asia                    4311408059 31252459.          47\n4 Europe                   669036256 23065219.          39\n5 North America            565028684 24484309.          18\n6 Oceania                   37757833  8504489.           7\n7 Seven seas (open ocean)          0    11603.           1\n8 South America            412060811 17762592.          13\n\n\nBuilding on this, let’s find the population density of each continent, find the continents with highest density and arrange by the number of countries. We’ll drop the geometry column to speed things up.\n\nworld %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(continent) %&gt;%\n  summarize(population = sum(pop, na.rm = TRUE),\n            area_km2 = sum(area_km2, na.rm = TRUE),\n            n_countries = n()) %&gt;%\n  mutate(density = round(population/area_km2)) %&gt;%\n  slice_max(density, n = 3) %&gt;%\n  arrange(desc(n_countries))\n\n# A tibble: 3 × 5\n  continent population  area_km2 n_countries density\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;   &lt;dbl&gt;\n1 Africa    1154946633 29946198.          51      39\n2 Asia      4311408059 31252459.          47     138\n3 Europe     669036256 23065219.          39      29"
  },
  {
    "objectID": "course-materials/labs/week2.html#joins-with-vector-attributes",
    "href": "course-materials/labs/week2.html#joins-with-vector-attributes",
    "title": "Week 2 Lab",
    "section": "5. Joins with vector attributes",
    "text": "5. Joins with vector attributes\nA critical part of many data science workflows is combining data sets based on common attributes. In R, we do this using multiple join functions, which follow SQL conventions.\nLet’s start by looking a data set on national coffee production from the spData package:\n\ncoffee_data &lt;- spData::coffee_data\nhead(coffee_data)\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\nIt appears that coffee_data contains information on the amount of coffee produced in 2016 and 2017 from a subset of countries.\n\nnrow(coffee_data)\n\n[1] 47\n\nnrow(world)\n\n[1] 177\n\n\nThe coffee production dataset does not include any spatial information, so If we wanted to make a map of coffee production, we would need to combine coffee_data with the world dataset. We do this by joining based on countries’ names.\n\nworld_coffee &lt;- left_join(world, coffee_data, by = \"name_long\")\n\nnames(world_coffee)\n\n [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n [4] \"region_un\"              \"subregion\"              \"type\"                  \n [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n[10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n[13] \"coffee_production_2017\"\n\n\nAnd plot what this looks like…\n\ntm_shape(world_coffee) +\n  tm_fill(col = \"coffee_production_2017\",\n          title = \"Coffee production (2017)\")\n\n\n\n\n\n\n\n\nBy using a left join, our previous result added the coffee production information onto all countries of the world. If we just wanted to keep countries that do have coffee data, we could use an inner join:\n\nworld_coffee_inner &lt;- inner_join(world, coffee_data, by = \"name_long\")\n\nLet’s build ourselves a warning message to make sure we don’t lose any data because of incomplete matches.\n\nif (nrow(world_coffee_inner) != nrow(coffee_data)) {\n  warning(\"inner join does not match original data. potential data loss during join\")\n}\n\nWarning: inner join does not match original data. potential data loss during\njoin\n\n\nIt looks like we lost some countries with coffee data, so let’s figure out what’s going on. We can find rows that didn’t match using the setdiff() function.\n\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\"             \n\n\nWe see that one of the issues is that the two data sets use different naming conventions for the Democratic Republic of the Congo. We can use a string matching function to figure out what the DRC is called in the world data set.\n\n# search for the DRC in the world dataset\ndrc &lt;- stringr::str_subset(world$name_long, \"Dem*.+Congo\")\n\nNow we can update the coffee data set with the matching name for the DRC:\n\ncoffee_data$name_long[stringr::str_detect(coffee_data$name_long, \"Congo\")] &lt;- drc\n\nAnd we can try the inner join again and hopefully the DRC now matches:\n\nworld_coffee_inner &lt;- inner_join(world, coffee_data , by = \"name_long\")\n\n# update warning message conditional to include the mismatch for \"others\"\nif (nrow(world_coffee_inner) != nrow(coffee_data) & setdiff(coffee_data$name_long, world_coffee_inner$name_long) != \"Others\") {\n  warning(\"inner join does not match original data. potential data loss during join\")\n}\n\nLet’s visualize what a the inner join did to our spatial object.\n\ntm_shape(world_coffee_inner) +\n  tm_polygons(fill = \"coffee_production_2017\",\n              title = \"Coffee production (2017)\") +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCritical thinking question\n\n\n\nWhat happens if we left join a sf object onto a data frame?\n\ncoffee_world &lt;- left_join(coffee_data, world, by = \"name_long\")\nclass(coffee_world)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(coffee_world)\n\n [1] \"name_long\"              \"coffee_production_2016\" \"coffee_production_2017\"\n [4] \"iso_a2\"                 \"continent\"              \"region_un\"             \n [7] \"subregion\"              \"type\"                   \"area_km2\"              \n[10] \"pop\"                    \"lifeExp\"                \"gdpPercap\"             \n[13] \"geom\"                  \n\n\nWe end up with a data frame!"
  },
  {
    "objectID": "course-materials/labs/week4.html",
    "href": "course-materials/labs/week4.html",
    "title": "Week 4: Lab",
    "section": "",
    "text": "In this lab we’ll be exploring the basics of raster data, including spatial data and geometry operations. Raster data represents continuous surfaces, as opposed to the discrete features represented in the vector data model. We’ll primarily be working with data from Zion National Park in Utah."
  },
  {
    "objectID": "course-materials/labs/week4.html#set-up",
    "href": "course-materials/labs/week4.html#set-up",
    "title": "Week 4: Lab",
    "section": "1. Set Up",
    "text": "1. Set Up\nFirst, let’s install the {geoData} package which we’ll use later in the lab to get access to example datasets.\n\ninstall.packages(\"geoData\")\n\nNow let’s load all the necessary packages. R has several packages for handling raster data. In this lab, we’ll use the {terra} package.\n\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(kableExtra) # table formatting\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data\nlibrary(geodata) # spatial data"
  },
  {
    "objectID": "course-materials/labs/week4.html#raster-objects",
    "href": "course-materials/labs/week4.html#raster-objects",
    "title": "Week 4: Lab",
    "section": "2. Raster objects",
    "text": "2. Raster objects\nIn this section we’ll learn how to create raster data objects by reading in data and how to do basic data manipulations.\n\nCreating raster objects\nThe {terra} package represents raster objects using the SpatRaster class. The easiest way to create SpatRaster objects is to read them in using the rast() function. Raster objects can handle both continuous and categorical data.\nWe’ll start with an example of two datasets for Zion National Park from the spDataLarge package:\n\nsrtm.tif: remotely sensed elevation estimates (continuous data)\nnlcd.tif: simplified version of the National Land Cover Database 2011 product (categorical data)\n\n\n# create raster objects\nzion_elevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion_land &lt;- rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\n\n# test class of raster object\nclass(zion_elevation)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(legend.outside = TRUE)\n\nmap2 &lt;- tm_shape(zion_land) +\n  tm_raster(title = \"Land cover\") +\n  tm_layout(legend.outside = TRUE)\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nThe SpatRaster class can also handle multiple “layers”. Layers can store different variables for the same region in one object. This is similar to attributes (or columns) in data.frames. Later in the course when we discuss multispectral data, we’ll learn more about why remotely-sensed data will often contain multiple “bands” or layers.\nAs an example, we’ll load a dataset from spDataLarge containing the four bands of the Landsat 8 image for Zion National Park.\n\nlandsat &lt;- rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\n\nnlyr(landsat) # test number of layers in raster object\n\n[1] 4\n\n\n\n\nCode\ntm_shape(landsat) +\n  tm_raster(title = \"Unscaled reflectance\")\n\n\n\n\n\n\n\n\n\nWe can subset layers using either the layer number or name:\n\nlandsat3 &lt;- subset(landsat, 3)\nlandsat4 &lt;- subset(landsat, \"landsat_4\")\n\nWe can combine SpatRaster objects into one, using c():\n\nlandsat34 &lt;- c(landsat3, landsat4)\n\n\n\nMerging Rasters\nIn some cases, data for a region will be stored in multiple, contiguous files. To use them as a single raster, we need to merge them.\nIn this example, we download elevation data for Austria and Switzerland and merge the two rasters into one.\n\naustria &lt;- geodata::elevation_30s(country = \"AUT\", path = tempdir())\nswitzerland &lt;- geodata::elevation_30s(country = \"CHE\", path = tempdir())\n\nmerged &lt;- merge(austria, switzerland)\n\n\n\nCode\nmap1 &lt;- tm_shape(austria) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"Austria\")\n  \nmap2 &lt;- tm_shape(switzerland) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"Switzerland\")  \n\nmap3 &lt;- tm_shape(merged) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"Merged\")   \n  \ntmap_arrange(map1, map2, map3, nrow = 1) \n\n\n\n\n\n\n\n\n\n\n\nInspecting raster objects\nWe can get info on raster values just by typing the name or using the summary function.\n\nsummary(zion_elevation)\n\nWarning: [summary] used a sample\n\n\n      srtm     \n Min.   :1024  \n 1st Qu.:1535  \n Median :1836  \n Mean   :1843  \n 3rd Qu.:2114  \n Max.   :2892  \n\n\nWe can get global summaries, such as standard deviation.\n\nglobal(zion_elevation, sd)\n\n           sd\nsrtm 416.6776\n\n\nOr we can use freq() to get the counts with categories.\n\nfreq(zion_land)\n\n  layer      value  count\n1     1      Water   1209\n2     1  Developed  17517\n3     1     Barren 106070\n4     1     Forest 767537\n5     1  Shrubland 545771\n6     1 Herbaceous   4878\n7     1 Cultivated   8728\n8     1   Wetlands   6497\n\n\n\n\nIndexing\nWe can index rasters using row-column indexing or cell IDs.\n\n# row 1, column 1\nzion_elevation[1, 1]\n\n  srtm\n1 1728\n\n# cell ID 1\nzion_elevation[1]\n\n  srtm\n1 1728\n\n\nFor multi-layer rasters, subsetting returns the values in both layers.\n\nlandsat[1]\n\n  landsat_1 landsat_2 landsat_3 landsat_4\n1      9833      9579      9861     14114\n\n\nWe can also modify/overwrite cell values.\n\nzion_elevation[1, 1] &lt;- 0\nzion_elevation[1, 1]\n\n  srtm\n1    0\n\n\nReplacing values in multi-layer rasters requires a matrix with as many columns as layers and rows as replaceable cells.\n\nlandsat[1] &lt;- cbind(c(0), c(0),c(0), c(0))\nlandsat[1]\n\n  landsat_1 landsat_2 landsat_3 landsat_4\n1         0         0         0         0\n\n\nWe can also use a similar approach to replace values that we suspect are incorrect.\n\ntest_raster &lt;- zion_elevation\ntest_raster[test_raster &lt; 20] &lt;- NA"
  },
  {
    "objectID": "course-materials/labs/week4.html#spatial-subsetting",
    "href": "course-materials/labs/week4.html#spatial-subsetting",
    "title": "Week 4: Lab",
    "section": "3. Spatial subsetting",
    "text": "3. Spatial subsetting\nWe can move from subsetting based on specific cell IDs to extract info based on spatial objects.\nTo use coordinates for subsetting, we can “translate” coordinates into a cell ID with the functions terra::cellFromXY() or terra::extract().\n\n# create point within area covered by raster\npoint &lt;- matrix(c(-113, 37.5), ncol = 2)\n\n# approach 1\n# find cell ID for point\nid &lt;- cellFromXY(zion_elevation, xy = point)\n# index to cell\nzion_elevation[id]\n\n  srtm\n1 2398\n\n# approach 2\n# extract raster values at point\nterra::extract(zion_elevation, point)\n\n  srtm\n1 2398\n\n\nWe can also subset raster objects based on the extent another raster object. Here we extract the values of our elevation raster that fall within the extent of a clipping raster that we create.\n\n# create a raster with a smaller extent\nclip &lt;- rast(xmin = -113.3, xmax = -113, ymin = 37.2, ymax = 37.9,\n            resolution = 0.3,\n            vals = 1)\n\n# select values that fall within smaller extent\nzion_elevation_clip &lt;- zion_elevation[clip]\n\n# verify that output has fewer values than original\nif(ncell(zion_elevation) == nrow(zion_elevation_clip)) {\n  warning(\"clipping did not remove cells\")\n} else {\n  print(\"clipping removed cells\")\n}\n\n[1] \"clipping removed cells\"\n\n\nIn the previous example, we just got the values of the raster back (and lost the raster format). In some cases, we might want the output to be the raster cells themselves.\nWe can do this use the “[” operator and setting “drop = FALSE”.\n\nzion_elevation_clip &lt;- zion_elevation[clip, drop = FALSE]\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"original\")\n\nmap2 &lt;- tm_shape(zion_elevation_clip) +\n  tm_raster(title = \"Elevation (m)\") +\n    tm_layout(main.title = \"clipped\")\n\n\ntmap_arrange(map1, map2, nrow = 1)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWoops, is the clipped version really smaller?\n\n\n\nPlotting side-by-side, the clipped version appears to take up more space – does this mean the clipping didn’t work? How can we tell?\n\nVisually: If we look at the features represented, we can see that the clipped version doesn’t represent all the features present in the original version.\nQuantitatively: We can directly check whether the extents match using the ext() function!\n\n\nif(ext(zion_elevation) == ext(zion_elevation_clip)){\n  print(\"extents match\")\n} else{\n  print(\"extents do not match\")\n}\n\n[1] \"extents do not match\"\n\n\n\n\nIn the previous example, we subsetted the extent of the raster (removed cells). Another common use of spatial subsetting is to select cells based on their values. In this case we create a “masking” raster comprised of logicals or NAs that dictates the cells we would like to preserve.\n\n# create raster mask of the same resolution and extent\nrmask &lt;- zion_elevation\n\n# set all cells with elevation less than 2000 meters to NA\nrmask[rmask &lt; 2000] &lt;- NA\n \n# subset elevation raster based on mask\n\n# approach 1: bracket subsetting\nmasked1 &lt;- zion_elevation[rmask, drop = FALSE]   \n# approach 2: mask() function\nmasked2 &lt;- mask(zion_elevation, rmask)           \n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"original\")\n\nmap2 &lt;- tm_shape(masked1) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"bracket subsetting\")\n\nmap3 &lt;- tm_shape(masked2) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"mask()\")\n\ntmap_arrange(map1, map2, map3, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week4.html#map-algebra",
    "href": "course-materials/labs/week4.html#map-algebra",
    "title": "Week 4: Lab",
    "section": "4. Map algebra",
    "text": "4. Map algebra\n“Map algebra” is the set of operations that modify or summarize raster cell values with reference to surrounding cells, zones, or statistical functions that apply to every cell. Map algebra is typically categorized into local, focal, and zonal operations.\n\nLocal operations\nLocal operations are computed on each cell individually. For example, we can use ordinary arithmetic or logical statements.\n\nzion_elevation + zion_elevation # doubles each cells' value\nzion_elevation^2 # raises each cells' value to the power of 2\nlog(zion_elevation) # takes the log of each cells' value\nzion_elevation &gt; 5 # determines whether each cell has a value greater than 5\n\nWe can also classify intervals of values into groups. For example, we could classify elevation into low, middle, and high elevation cells.\nFirst, we need to construct a reclassification matrix:\n\nThe first column corresponds to the lower end of the class\nThe second column corresponds to the upper end of the class\nThe third column corresponds to the new value for the specified ranges in columns 1 and 2\n\n\n# create reclassification matrix\nrcl &lt;- matrix(c(1000, 1500, 1, # group 1 ranges from 1000 - 1500 m\n                1500, 2000, 2, # group 2 ranges from 1500 - 2000 m\n                2000, 2500, 3, # group 3 ranges from 2000 - 2500 m\n                2500, 3000, 4), # group 4 ranges from 2500 - 3000 m\n                ncol = 3, byrow = TRUE)\n\n# use reclassification matrix to reclassify elevation raster\nreclassified &lt;- classify(zion_elevation, rcl = rcl)\n\n# change reclassified values into factors\nvalues(reclassified) &lt;- as.factor(values(reclassified))\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"original\")\n\nmap2 &lt;- tm_shape(reclassified) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"reclassified\")\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\n\nFor more efficient processing, we can use a set of map algebra functions:\n\napp() applies a function to each cell of a raster to summarize the values of multiple layers into one layer\ntapp() is an extension of app() that allows us to apply on operation on a subset of layers\nlapp() allows us to apply a function to each cell using layers as arguments\n\nWe can use the lapp()function to compute the Normalized Difference Vegetation Index (NDVI). (More on this later in the quarter!) Let’s calculate NDVI for Zion National Park using multispectral satellite data.\nFirst, we need to define a function to calculate NDVI. Then, we can use lapp() to calculate NDVI in each raster cell. To do so, we just need the NIR and red bands.\n\n# define NDVI as the normalized difference between NIR and red bands\nndvi_fun &lt;- function(nir, red){\n  (nir - red) / (nir + red)\n}\n\n# apply NDVI function to Landsat bands 3 & 4\nndvi_rast &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun)\n\n\n\nCode\ntm_shape(ndvi_rast) +\n  tm_raster(title = \"NDVI\")\n\n\n\n\n\n\n\n\n\n\n\nFocal Operations\nLocal operations operate on one cell, though from multiple layers. Focal operations take into account a central (focal) cell and its neighbors. The neighborhood (or kernel, moving window, filter) can take any size or shape. A focal operation applies an aggregation function to all cells in the neighborhood and updates the value of the central cell before moving on to the next central cell.\nThe image below provides an example of using a moving window filter. The large orange square highlights the 8 cells that are considered “neighbors” to the central cell (value = 8). Using this approach, the value of the central cell will be updated to the minimum value of its neighboring cells (in this case 0). This process then repeats for each cell.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\n\n\n\n\n\n\nCritical thinking\n\n\n\n\n\nWhy are the cells on the border in the filtered raster now have values of NA?\nThe cells along the border do not have a complete set of “neighbors”, therefore the filtering operation returns a NA.\n\n\n\nWe can use the focal() function to perform spatial filtering. We define the size, shape, and weights of the moving window using a matrix. In the following example we’ll find the minimum value in 9x9 cell neighborhoods.\n\nelevation_focal &lt;- focal(zion_elevation, \n                         w = matrix(1, nrow = 9, ncol = 9), # create moving window\n                         fun = min) # function to map new values\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"original\")\n\nmap2 &lt;- tm_shape(elevation_focal) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"aggregated\")\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCritical thinking\n\n\n\n\n\nWhat should we expect to observe in the output for this spatial filtering example?\n\nOverall, we see more lower values because we are finding the minimum value in each neighborhood\nThe output looks “grainier” because many cells have the same values as their neighbors\n\n\n\n\n\n\nZonal Operations\nSimilar to focal operations, zonal operations apply an aggregation function to multiple cells. However, instead of applying operations to neighbors, zonal operations aggregate based on “zones”. Zones can are defined using a categorical raster and do not necessarily have to be neighbors\nFor example, we could find the average elevation for within the elevations zones we created.\n\nzonal(zion_elevation, reclassified, fun = \"mean\") %&gt;%\n  kable(col.names = c(\"Elevation zone\", \"Mean elevation (m)\")) %&gt;% \n  kable_styling(bootstrap_options = \"striped\")\n\n\n\n\nElevation zone\nMean elevation (m)\n\n\n\n\n0\n0.000\n\n\n1\n1292.281\n\n\n2\n1771.967\n\n\n3\n2222.776\n\n\n4\n2640.340"
  },
  {
    "objectID": "course-materials/labs/week4.html#changing-extent-origin-and-resolution",
    "href": "course-materials/labs/week4.html#changing-extent-origin-and-resolution",
    "title": "Week 4: Lab",
    "section": "1. Changing extent, origin, and resolution",
    "text": "1. Changing extent, origin, and resolution\n\nExtent\nIn the simplest case, two images differ only in their extent. Let’s start by increasing the extent of a elevation raster.\n\nelev_2 &lt;- extend(zion_elevation, c(1, 200)) # add one row and two columns\n\nPerforming algebraic operations on objects with different extents doesn’t work.\n\nelev + elev_2\n\nWe can align the extent of the 2 rasters using the extend() function. Here we extend the zion_elevation object to the extent of elev_2 by adding NAs.\n\nelev_3 &lt;- extend(zion_elevation, elev_2)\n\n\n\nOrigin\nThe origin function returns the coordinates of the cell corner closes to the coordinates (0,0).\n\norigin(zion_elevation)\n\n[1] -0.0004165537 -0.0004165677\n\n\n\n\nResolution\nRaster datasets can also differ in their resolution. To match resolutions we can decrease (or coarsen) the resolution by aggregating or increase (or sharpen) the resolution by disaggregating.\n\nAggregating\nWhen decreasing the resolution of rasters, we are effectively combining multiple celss into a single cell. Let’s start by coarsening the resolution of the Zion elevation data by a factor of 5, by taking the mean value of cells.\n\nzion_elevation_coarse &lt;-  aggregate(zion_elevation, fact = 5, fun = mean)\n\n\n\nCode\nmap1 &lt;- tm_shape(zion_elevation) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"original\")\n\nmap2 &lt;- tm_shape(zion_elevation_coarse) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"aggregated\")\n\ntmap_arrange(map1, map2, nrow = 1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCritical thinking\n\n\n\n\n\nWhat should we observe in the output?\nThe aggregated raster appears “grainier” because the cells are now larger. This is the same concept as having a more pixelated image.\n\n\n\n\n\nDisaggregating\nTo increase the resolution of a raster, we need to break a single cell into multiple cells. There are many ways to do this and the appropriate method will often depend on our specific purpose. However, most approaches define the values of the new (smaller) cells based on not only the value of the original cell they came from, but also neighboring cells.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\nIn the example below, we use the bilinear method to disaggregate the elevation raster we aggregated in the previous example.\n\n\n\n\n\n\nCritical thinking\n\n\n\n\n\nDoes disaggregating the aggregated version get us back to the original raster?\nNo! There is no way for us to exactly recover the original data from the aggregated version.\n\n\n\n\n# disaggregate the aggregated raster\nzion_elevation_disagg &lt;- disagg(zion_elevation_coarse, fact = 5, method = \"bilinear\")\n\n# check whether the disaggregated version matches the original\nif(identical(zion_elevation, zion_elevation_disagg)){\n  print(\"disaggregated data matches original\")\n} else {\n  warning(\"disaggregated data does not match original\")\n}\n\nWarning: disaggregated data does not match original\n\n\n\n\nCode\nmap3 &lt;- tm_shape(zion_elevation_disagg) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"disaggregated\")\n\ntmap_arrange(map1, map2, map3, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week4.html#resampling",
    "href": "course-materials/labs/week4.html#resampling",
    "title": "Week 4: Lab",
    "section": "2. Resampling",
    "text": "2. Resampling\nAggregation/disaggregation work when both rasters have the same origins.\nBut what do we do in the case where we have two or more rasters with different origins and resolutions? Resampling computes values for new pixel locations based on custom resolutions and origins.\nThe images below show that we are trying to find the values of the original raster within the cells defined by the new “target” raster.\n\n\n\n\n\n\n\n\n\n\nGeocomputation with R\n\nIn most cases, the target raster would be an object you are already working with, but here we define a target raster.\n\n# CODE NOT WORKING\ntarget_rast &lt;- rast(xmin = -113.2, xmax = -112.9,\n                   ymin = 37.14, ymax = 37.5,\n                   nrow = 450, ncol = 460, \n                   crs = crs(zion_elevation))\n\nzion_elevation_resample &lt;- resample(zion_elevation, y = target_rast, method = \"bilinear\")\n\n\n\nCode\nmap4 &lt;- tm_shape(zion_elevation_resample) +\n  tm_raster(title = \"Elevation (m)\") +\n  tm_layout(main.title = \"resampled\")\n\ntmap_arrange(map1, map4, nrow = 1)"
  },
  {
    "objectID": "course-materials/labs/week10.html",
    "href": "course-materials/labs/week10.html",
    "title": "Week 10: Lab",
    "section": "",
    "text": "The National Science Foundation’s National Ecological Observatory Network (NEON) collects standardized, open-access ecological data at 81 freshwater and terrestrial field sites across the country. In addition to an amazing array of on-the-ground surveys, they also periodically collect Lidar data at the sites. All data is publicly available through the NEON Data Portal.\nFor this exercise, we will imagine that we are interested in studying canopy structure (tree height) at the San Joaquin Experimental Range in California. We’re interested in figuring out if we can rely on the Lidar data NEON is collecting by comparing tree height estimates to on-the-ground field surveys. If the estimates between the two methods are similar, we could save ourselves a lot of time and effort measuring trees!\nThis lab is based on materials developed by Edmund Hart, Leah Wasser, and Donal O’Leary for NEON."
  },
  {
    "objectID": "course-materials/labs/week10.html#task",
    "href": "course-materials/labs/week10.html#task",
    "title": "Week 10: Lab",
    "section": "Task",
    "text": "Task\nTo estimate tree height from Lidar data, we will create a canopy height model (CHM) from Lidar-derived digital surface and terrain models. We will then extract tree height estimates within the locations of on-the-ground surveys and compare Lidar estimates to measured tree height in each plot."
  },
  {
    "objectID": "course-materials/labs/week10.html#data",
    "href": "course-materials/labs/week10.html#data",
    "title": "Week 10: Lab",
    "section": "Data",
    "text": "Data\nLidar data\n\nSJER2013_DSM.tif, digital surface model (DSM)\nSJER2013_DTM.tif, digital terrain model (DTM)\nDSMs represent the elevation of the top of all objects\nDTMs represent the elevation of the ground (or terrain)\n\nVegetation plot geometries\n\nSJERPlotCentroids_Buffer.shp\nContains locations of vegetation surveys\nPolygons representing 20m buffer around plot centroids\n\nVegetation surveys\n\nD17_2013_vegStr.csv\nMeasurements for individual trees in each plot\nMetadata available in D17_2013_vegStr_metadata_desc.csv"
  },
  {
    "objectID": "course-materials/labs/week10.html#prerequisites",
    "href": "course-materials/labs/week10.html#prerequisites",
    "title": "Week 10: Lab",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s load all necessary packages:\n\nlibrary(terra)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(here)"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-lidar-data",
    "href": "course-materials/labs/week10.html#load-lidar-data",
    "title": "Week 10: Lab",
    "section": "Load Lidar data",
    "text": "Load Lidar data\n\n# digital surface model (DSM)\ndsm &lt;- rast(here::here(\"course-materials\", \"data\", \"week10\", \"SJER2013_DSM.tif\"))\n\n# digital terrain model (DTM)\ndtm &lt;- rast(here::here(\"course-materials\", \"data\", \"week10\", \"SJER2013_DTM.tif\"))\n\nLet’s check if the DSM and DTM have the same resolution, position, and extent by creating a raster stack:\n\ntest_raster &lt;- c(dsm, dtm)\n\nCreate the canopy height model (CHM) or the height of all objects by finding the difference between the DSM and DTM:\n\nchm &lt;- dsm - dtm"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-vegetation-plot-geometries",
    "href": "course-materials/labs/week10.html#load-vegetation-plot-geometries",
    "title": "Week 10: Lab",
    "section": "Load vegetation plot geometries",
    "text": "Load vegetation plot geometries\nThis includes the locations of study plots and the surveys of individual trees in each plot.\n\n# read in plot centroids\nplot_centroids &lt;- st_read(here::here(\"course-materials\", \"data\", \"week10\", \"PlotCentroids\", \"SJERPlotCentroids_Buffer.shp\"))\n\n# test if the plot CRS matches the Lidar CRS\nst_crs(plot_centroids) == st_crs(chm)\n\n\ntm_shape(chm) +\n  tm_raster() +\n  tm_shape(plot_centroids) +\n  tm_polygons()"
  },
  {
    "objectID": "course-materials/labs/week10.html#load-vegetation-survey-data",
    "href": "course-materials/labs/week10.html#load-vegetation-survey-data",
    "title": "Week 10: Lab",
    "section": "Load vegetation survey data",
    "text": "Load vegetation survey data\nLet’s find the maximum tree height in each plot:\n\n# read in the vegetation surveys, which include the height of each tree\n\n# setting this option will keep all character strings as characters\noptions(stringsAsFactors=FALSE)\n\n# read in survey data and find the maximum tree height in each plot\nveg_surveys &lt;- read.csv(here::here(\"course-materials\", \"data\", \"week10\", \"VegetationData\", \"D17_2013_vegStr.csv\")) %&gt;%\n  group_by(plotid) %&gt;%\n  summarise(\"survey_height\" = max(stemheight, na.rm = TRUE))\n\nNow find the maximum tree height in each plot as determined by the CHM:\n\nextract_chm_height &lt;- terra::extract(chm, plot_centroids, fun = max) %&gt;%\n  rename(chm_height = SJER2013_DSM) %&gt;%\n  select(chm_height)\n\nCombine tree height estimates from the Lidar and plot surveys:\n\nplot_centroids &lt;- cbind(plot_centroids, extract_chm_height) %&gt;%\n  left_join(.,veg_surveys, by = c(\"Plot_ID\" = \"plotid\"))"
  },
  {
    "objectID": "course-materials/labs/week10.html#plot-results",
    "href": "course-materials/labs/week10.html#plot-results",
    "title": "Week 10: Lab",
    "section": "Plot results",
    "text": "Plot results\nLet’s compare the estimates between the two methods: Lidar and on-the-ground surveys\n\nTo make the comparison, we’ll add a 1:1 line\n\nIf all the points fall along this line it means that both methods give the same answer\n\nLet’s also add a regression line with confidence intervals to compare how the overall fit between methods compares to the 1:1 line\n\n\nggplot(plot_centroids, aes(y=chm_height, x= survey_height)) +\n  geom_abline(slope=1, intercept=0, alpha=.5, lty=2) + #plotting our \"1:1\" line\n  geom_point() +\n  geom_smooth(method = lm) + # add regression line and confidence interval\n  ggtitle(\"Validating Lidar measurements\") +\n  xlab(\"Maximum Measured Height (m)\") +\n  ylab(\"Maximum Lidar Height (m)\")\n\nWe’ve now compared Lidar estimates of tree height to on-the-ground measurements!\nIt looks like the Lidar estimates tend to underestimate tree height for shorter trees and overestimates tree height for taller trees. Or maybe human observers underestimate the height of tall trees because they’re challenging to measure? Or maybe the digital terrain model misjudged the elevation of the ground? There could be many reasons that the answers don’t line up! It’s then up to the researcher to figure out if the mismatch is important for their problem."
  },
  {
    "objectID": "course-materials/discussions/extra-discussion.html",
    "href": "course-materials/discussions/extra-discussion.html",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggspatial)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the ghm.tif files. ghm.tif Transform both to EPSG:4326.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")"
  },
  {
    "objectID": "course-materials/discussions/extra-discussion.html#prerequsites",
    "href": "course-materials/discussions/extra-discussion.html#prerequsites",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(sf)\nlibrary(terra)\nlibrary(stars)\nlibrary(ggspatial)\n\nLoad in the CPAD_2023a_SuperUnits.shp and the ghm.tif files. ghm.tif Transform both to EPSG:4326.\n\ncpad_super &lt;- st_read(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"CPAD_2023a_SuperUnits.shp\"), quiet = TRUE) %&gt;%\n  sf::st_transform(\"EPSG:4326\") %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(ID = row_number())\n\nghm &lt;- rast(here::here(\"course-materials\", \"data\", \"week9-discussion\", \"gHM_masked.tif\")) %&gt;%\n  project(\"EPSG:4326\")"
  },
  {
    "objectID": "course-materials/discussions/extra-discussion.html#exercises",
    "href": "course-materials/discussions/extra-discussion.html#exercises",
    "title": "Discssuion Week 10: Workflows and Maps",
    "section": "Exercises",
    "text": "Exercises\n\nLet’s make nice plots of the California Protected areas by access level\n\n\np1 &lt;- ggplot2::ggplot(data = cpad_super) +\n  geom_sf(aes(color = access_typ, fill = access_typ)) +\n  theme_bw() +\n  labs(\n    color = \"Access Type\",\n    fill = \"Access Type\"\n  ) +\n  annotation_scale(plot_unit = \"km\") +\n  annotation_north_arrow(\n    location = \"tr\",\n    pad_x = unit(0.2, \"in\"),\n    pad_y = unit(0.2, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill =\n        c(\"grey40\", \"white\"),\n      line_col = \"grey20\"\n    )\n  ) +\n  coord_sf() +\n  scale_color_viridis_d() +\n  scale_fill_viridis_d()\n\n\np1 +\n  facet_wrap(~access_typ) +\n  theme(strip.background = element_rect(fill = \"transparent\"))\n\n\nLet’s try plotting the ghm layers nicely too!\n\n\nggplot() +\n    geom_stars(data = st_as_stars(ghm)) +\n  coord_equal() +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"\",\n    fill = \"Global Human Modification\"\n  ) +\n  scale_fill_viridis_c() +\n  annotation_scale(plot_unit = \"km\") +\n  annotation_north_arrow(\n    location = \"tr\",\n    pad_x = unit(0.2, \"in\"),\n    pad_y = unit(0.2, \"in\"),\n    style = ggspatial::north_arrow_nautical(\n      fill =\n        c(\"grey40\", \"white\"),\n      line_col = \"grey20\"\n    )\n  )\n\n\nCreate a function to take 2 data sets (1 polygon and 1 raster) and create a boxplot of the values based on a specific layer\n\n\nsummary_boxplot &lt;- function(polygon, raster, my_layer, my_label) {\n  \n  # rasterize polygon by layer\n  id_rast &lt;- rasterize(polygon, raster, field = \"suid_nma\")\n  \n  #do mean zonal statistics\n  zonal_layer &lt;- zonal(raster, id_rast, fun = \"mean\", na.rm = TRUE)\n  \n  #join with polygon database\n  poly_join &lt;- full_join(polygon, zonal_layer) %&gt;% \n    select(suid_nma, gHM, my_layer)\n  \n  #create boxplot based on your layer\n  p1 &lt;- ggplot(poly_join) +\n    geom_boxplot(aes(gHM, .data[[my_layer]])) +\n    theme_bw() +\n    labs(x = \"Human Modification Index\", \n         y = my_label)\n  \n  return(p1)\n}\n\n\nLet’s select some layers and use our new function!\n\n\nnames(cpad_super)\n\n\naccess &lt;- summary_boxplot(cpad_super, ghm, \"access_typ\", \"Access Type\")\n\naccess\n\n\nlayer &lt;- summary_boxplot(cpad_super, ghm, \"layer\", \"Management Agency Type\")\n\nlayer"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html",
    "href": "course-materials/discussions/week4-discussion.html",
    "title": "Week 4: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from Geocomputation with R by Robin Lovelace."
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#get-started",
    "href": "course-materials/discussions/week4-discussion.html#get-started",
    "title": "Week 4: Discussion Section",
    "section": "1. Get Started",
    "text": "1. Get Started\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t have spDataLarge installed, make sure to run:  install.packages('spDataLarge', repos='https://nowosad.github.io/drat/', type='source')\n\n\n\nCreate an .Rproj as your version controlled project for Week 4\nCreate a Quarto document inside your .Rproj\nDownload this data folder from Google Drive and move it inside your .Rproj\nLoad all necessary packages and read spatial objects\n\n\nlibrary(tidyverse)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\n\n\ndem &lt;- terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nperu &lt;- stars::read_stars(here::here(\"week4-discussion\", \"PER_elv.tif\"))\nperu &lt;- rast(peru)"
  },
  {
    "objectID": "course-materials/discussions/week4-discussion.html#your-task",
    "href": "course-materials/discussions/week4-discussion.html#your-task",
    "title": "Week 4: Discussion Section",
    "section": "2. Your Task",
    "text": "2. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nPlot a histogram and boxplot of dem\nReclassify dem and compute the mean for the three classes:\n\n\nLow, where elevation is less than 300\nMedium\nHigh, where elevation is greater than 500\n\n\nCalculate the Normalized Difference Vegetation Index (NDVI) and Normalized Difference Water Index (NDWI) of landsat and find a correlation between NDVI and NDWI\n\n\nNote: \\(NDVI = (NIR - red)/(NIR + red)\\)\nNote: \\(NDWI = (green - NIR)/(green + NIR)\\)\n\nApply the functions to the appropriate Landsat 8 bands. Landsat 8 bands 2-5 correspond to bands 1-4 for this raster. Bands are as follows:\n\n\n\nBand\n\nColor\n\n\n\n\n1\nblue\n30 meter\n\n\n2\ngreen\n30 meter\n\n\n3\nred\n30 meter\n\n\n4\nnear-infrared\n30 meter\n\n\n\n\nFind the distance across all cells in peru to its nearest coastline\n\n\nHint: Use terra::distance() to find geographic distance for all cells\nNote: terra::distance() will calculate distance for all cells that are NA to the nearest cell that are not NA\nWeigh the distance raster with peru and visualize the difference between the raster created using the Euclidean distance (E7) and the raster weighted by elevation\n\nEvery 100 altitudinal meters should increase the distance to the coast by 10 km\n\n\n\nChange the resolution of srtm to 0.01 by 0.01 degrees\n\n\nUse all of the method available in the terra package\nNote: The srtm raster has a resolution of 0.00083 x 0.00083 degrees"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html",
    "href": "course-materials/discussions/week1-discussion.html",
    "title": "Week 1: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from the tmap book."
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#learning-objectives",
    "href": "course-materials/discussions/week1-discussion.html#learning-objectives",
    "title": "Week 1: Discussion Section",
    "section": "1. Learning Objectives",
    "text": "1. Learning Objectives\n\nRead in spatial objects\nCreate map with single spatial object\nCreate map with multiple spatial objects\nUse different types of tmap plotting formats (e.g. tm_polygons(), tm_fill(), tm_dots(), etc.)\nAdjust color palettes\nInclude essential map elements (e.g. scale bar & north arrow or graticules)\nCreate an interactive map\nBonus Challenge: Reproduce map using ggplot2 instead of tmap\n\n\n\n\n\n\n\nMore map making in R\n\n\n\nThere are MANY different ways to make maps in R, all with different pros/cons. Check out this resource for examples of what map making looks like in ggplot2, leaflet, and more!"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#get-started",
    "href": "course-materials/discussions/week1-discussion.html#get-started",
    "title": "Week 1: Discussion Section",
    "section": "2. Get Started",
    "text": "2. Get Started\n\nOpen your forked version of this repository and navigate to your version controlled project for Week 1\nCreate a Quarto document\nLoad all necessary packages\nRead in the spatial objects for Easter Island (Rapa Nui/Isla de Pascua):\n\nei_points: file contains several points on the island\nei_elev: raster with elevation data\nei_borders: polygon with the island outline\nei_roads: lines contains a road network for the island\n\n\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\nei_points &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_points.gpkg\"))\nvolcanoes &lt;- subset(ei_points, type == \"volcano\")\nei_elev &lt;- stars::read_stars(here::here(\"data\", \"easter_island\", \"ei_elev.tif\"))\nei_borders &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_border.gpkg\"))\nei_roads &lt;- sf::read_sf(here::here(\"data\", \"easter_island\", \"ei_roads.gpkg\"))"
  },
  {
    "objectID": "course-materials/discussions/week1-discussion.html#your-task",
    "href": "course-materials/discussions/week1-discussion.html#your-task",
    "title": "Week 1: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nCreate a map of Easter Island\nCreate a map of Easter Island and…\n\n\n…denote the island’s borders and continuous elevation\n…denote the island’s volcanoes and roads\n…play with the color palette and essential map elements\n\n\nCreate an interactive map of Easter Island"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(kableExtra)\n\n\ndem &lt;- terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nperu &lt;- stars::read_stars(here::here(\"data\", \"week4-discussion\", \"PER_elv.tif\"))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(kableExtra)\n\n\ndem &lt;- terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nlandsat &lt;- terra::rast(system.file(\"raster/landsat.tif\", package = \"spDataLarge\"))\nsrtm &lt;- terra::rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nperu &lt;- stars::read_stars(here::here(\"data\", \"week4-discussion\", \"PER_elv.tif\"))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem-1",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#plot-a-histogram-and-boxplot-of-dem-1",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Plot a histogram and boxplot of dem",
    "text": "Plot a histogram and boxplot of dem\n\nhist(dem,\n     main = \"Digital Elevation Model Raster Value Distribution\",\n     xlab = \"Value\")\n\n\n\n\n\n\n\nboxplot(dem,\n        main = \"Digital Elevation Model Raster Value Distribution\",\n        ylab = \"Value\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#reclassify-elevation-and-find-mean",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#reclassify-elevation-and-find-mean",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Reclassify Elevation and Find Mean",
    "text": "Reclassify Elevation and Find Mean\n\n# define a reclassification matrix\nrcl &lt;- matrix(c(-Inf, 300, 0, # values -Inf to 300 = 0\n                300, 500, 1,  # values 300 to 500 = 1\n                500, Inf, 2), # values 500 to Inf = 2\n              ncol = 3, byrow = TRUE)\n\n# apply the matrix to reclassify the raster, making all cells 0 or 1 or 2\ndem_rcl &lt;- terra::classify(dem, rcl = rcl)\n\n# assign labels to the numerical categories\nlevels(dem_rcl) &lt;- tibble::tibble(id = 0:2, \n                                  cats = c(\"low\", \"medium\", \"high\"))\n\n# calculate mean elevation for each category using original DEM values\nelevation_mean &lt;- terra::zonal(dem, dem_rcl, fun = \"mean\")\nelevation_mean\n\n    cats      dem\n1    low 274.3910\n2 medium 392.0486\n3   high 765.2197"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-correlation-between-ndwi-and-ndvi",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-correlation-between-ndwi-and-ndvi",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Find Correlation Between NDWI and NDVI",
    "text": "Find Correlation Between NDWI and NDVI\nDefine functions for calculating NDWI and NDVI\n\nndwi_fun &lt;- function(green, nir){\n    (green - nir)/(green + nir)\n}\n\nndvi_fun &lt;- function(nir, red){\n  (nir - red)/(nir + red)\n}\n\nApply the functions to the appropriate Landsat 8 bands. Landsat 8 bands 2-5 correspond to bands 1-4 for this raster. Bands are as follows:\n\n\n\nBand\n\nColor\n\n\n\n\n1\nblue\n30 meter\n\n\n2\ngreen\n30 meter\n\n\n3\nred\n30 meter\n\n\n4\nnear-infrared\n30 meter\n\n\n\n\nndwi_rast &lt;- terra::lapp(landsat[[c(2, 4)]],\n                         fun = ndwi_fun)\nplot(ndwi_rast,\n     main = \"Zion National Park NDWI\")\n\n\n\n\n\n\n\nndvi_rast &lt;- terra::lapp(landsat[[c(4, 3)]],\n                         fun = ndvi_fun)\n\n# stack rasters\ncombine &lt;- c(ndvi_rast, ndwi_rast)\n\nplot(combine, main = c(\"NDVI\", \"NDWI\"))\n\n\n\n\n\n\n\n# calculate the correlation between raster layers \nterra::layerCor(combine, fun = cor)\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.9132838\n[2,] -0.9132838  1.0000000"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-distances-across-all-peru-cells",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#find-distances-across-all-peru-cells",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Find Distances Across All peru Cells",
    "text": "Find Distances Across All peru Cells\n\n# Aggregate by a factor of 20 to reduce resolution and create new raster\nperu_agg &lt;- terra::aggregate(rast(peru), fact = 20)\nplot(peru_agg)\n\n\n\n\n\n\n\n# Create mask of ocean (NA values)\nwater_mask &lt;- is.na(peru_agg) # returns TRUE value for NA\n# Set all FALSE values to NA\nwater_mask[water_mask == 0] &lt;- NA\nplot(water_mask)\n\n\n\n\n\n\n\n# Find distance from each cell to ocean/coastline (default is unit = \"m\")\ndistance_to_coast &lt;- terra::distance(water_mask)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Convert from meters to kilometers \ndistance_to_coast_km &lt;- distance_to_coast/1000\n\nplot(distance_to_coast_km, main = \"Distance to the coast (km)\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#change-resolution-of-srtm",
    "href": "course-materials/discussions/answer-keys/week4-discussion-answerKey.html#change-resolution-of-srtm",
    "title": "Week 4: Discussion Section - Answer Key",
    "section": "Change Resolution of srtm",
    "text": "Change Resolution of srtm\n\nplot(srtm)\n\n\n\n\n\n\n\nrast_template &lt;- terra::rast(terra::ext(srtm), res = 0.01)\n\nsrtm_resampl1 &lt;- terra::resample(srtm, y = rast_template, method = \"bilinear\")\nsrtm_resampl2 &lt;- terra::resample(srtm, y = rast_template, method = \"near\")\nsrtm_resampl3 &lt;- terra::resample(srtm, y = rast_template, method = \"cubic\")\nsrtm_resampl4 &lt;- terra::resample(srtm, y = rast_template, method = \"cubicspline\")\nsrtm_resampl5 &lt;- terra::resample(srtm, y = rast_template, method = \"lanczos\")\n\nsrtm_resampl_all &lt;- c(srtm_resampl1, srtm_resampl2, srtm_resampl3, srtm_resampl4, srtm_resampl5)\nlabs &lt;- c(\"Bilinear\", \"Near\", \"Cubic\", \"Cubic Spline\", \"Lanczos\")\nplot(srtm_resampl_all, main = labs)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\n# Load raster data representing grain sizes with the three classes clay, silt and sand\ngrain &lt;- terra::rast(system.file(\"raster/grain.tif\", package = \"spData\"))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#subset-points-in-new-zealandaotearoa",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#subset-points-in-new-zealandaotearoa",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Subset Points in New Zealand/Aotearoa",
    "text": "Subset Points in New Zealand/Aotearoa\n\n# Subset New Zealand elevation points to &gt; 3100 meters\nnz_height3100 &lt;- nz_height %&gt;% \n  dplyr::filter(elevation &gt; 3100)\n\n# Create template: define the extent, resolution, and CRS based on nz_height3100\nnz_template &lt;- rast(terra::ext(nz_height3100), \n                    resolution = 3000, \n                    crs = terra::crs(nz_height3100))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#count-points-in-each-grid-cell",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#count-points-in-each-grid-cell",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Count Points in Each Grid Cell",
    "text": "Count Points in Each Grid Cell\n\n# Convert vector points to raster data\n# Function \"length\" returns a count of the elevation points per cell\nnz_raster &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = \"length\")\n\nplot(nz_raster, main = \"Number of Elevation Points &gt; 3100 in Each Grid Cell\")\nplot(st_geometry(nz_height3100), add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#find-maximum-elevation-in-each-grid-cell",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#find-maximum-elevation-in-each-grid-cell",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Find Maximum Elevation in Each Grid Cell",
    "text": "Find Maximum Elevation in Each Grid Cell\n\n# function \"max\" returns maximum elevation value per cell\nnz_raster2 &lt;- rasterize(nz_height3100, nz_template, field = \"elevation\", fun = max)\n\nplot(nz_raster2, main = \"Maximum Elevation in Each Grid Cell \")\nplot(st_geometry(nz_height3100), add = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#aggregate-and-resample-raster",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#aggregate-and-resample-raster",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Aggregate and Resample Raster",
    "text": "Aggregate and Resample Raster\n\n# Reduce the resolution by combining 2 cells in each direction into larger cells\n# Sum the values of all cells for the resulting elevation value\nnz_raster_low &lt;- aggregate(nz_raster, fact = 2, fun = sum, na.rm = TRUE)\n\n# Convert the new raster's resolution back to the 3kmx3km resolution of original raster\nnz_resample &lt;- resample(nz_raster_low, nz_raster)\n\nplots &lt;- c(nz_raster, nz_resample)\nlabs &lt;- c(\"Original 6 x 6 km\", \"Resample 6 x 6 km\")\nplot(plots, main = labs)\n\n\n\n\n\n\n\nplot(nz_raster_low, main = \"Resample 3 x 3 km\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#vectorize-raster",
    "href": "course-materials/discussions/answer-keys/week5-discussion-answerKey.html#vectorize-raster",
    "title": "Week 5: Discussion Section - Answer Key",
    "section": "Vectorize Raster",
    "text": "Vectorize Raster\n\n# Convert raster data to polygon vector data\ngrain_poly &lt;- as.polygons(grain) %&gt;% \n  st_as_sf()\n\nplot(grain, main = \"Grain (Raster)\")\n\n\n\n\n\n\n\nplot(grain_poly, main = \"Grain (Vector)\")\n\n\n\n\n\n\n\n# Subset polygons to only clay\nclay &lt;- grain_poly %&gt;% \n  dplyr::filter(grain == \"clay\")\n\nplot(clay, main = \"Clay\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\ncol &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"Colombia\", \"Colombia.shp\"))\n\nroads &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"RDLINE_colombia\", \"RDLINE_colombia.shp\"))\n\naves &lt;- readr::read_csv(here::here(\"data\", \"week2-discussion\", \"dataves.csv\")) %&gt;%\n  dplyr::as_tibble() %&gt;%\n  dplyr::rename(long = decimal_longitude) %&gt;%\n  dplyr::mutate(lat = decimal_latitude) %&gt;%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#read-in-vector-data",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#read-in-vector-data",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(sf)\nlibrary(tmap)\n\n\ncol &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"Colombia\", \"Colombia.shp\"))\n\nroads &lt;- st_read(here::here(\"data\", \"week2-discussion\", \"RDLINE_colombia\", \"RDLINE_colombia.shp\"))\n\naves &lt;- readr::read_csv(here::here(\"data\", \"week2-discussion\", \"dataves.csv\")) %&gt;%\n  dplyr::as_tibble() %&gt;%\n  dplyr::rename(long = decimal_longitude) %&gt;%\n  dplyr::mutate(lat = decimal_latitude) %&gt;%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#check-class-and-geometry-type",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#check-class-and-geometry-type",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Check Class and Geometry Type",
    "text": "Check Class and Geometry Type\n\nclass(col)\nclass(roads)\nclass(aves)\n\n\nunique(st_geometry_type(col))\nunique(st_geometry_type(roads))\nunique(st_geometry_type(aves))"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#select-macro-region-of-interest",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#select-macro-region-of-interest",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Select Macro Region of Interest",
    "text": "Select Macro Region of Interest\n\ncol_andean &lt;- col %&gt;%\n  # Set categorical \"levels\" in attribute N1_MacroBi (subregions of Colombia)\n  dplyr::mutate(N1_MacroBi = as.factor(N1_MacroBi)) %&gt;%\n  # Subset to Andean region of Colombia\n  dplyr::filter(N1_MacroBi == \"Andean\")\n\n\ntm_shape(col_andean) +\n  tm_polygons() +\n  tm_layout(main.title = \"Andean Region of Colombia\",\n            main.title.size = 1)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#play-with-coordinate-reference-system",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#play-with-coordinate-reference-system",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Play with Coordinate Reference System",
    "text": "Play with Coordinate Reference System\n\n# Print the CRS of each spatial object\nst_crs(col)\nst_crs(roads)\nst_crs(aves)\n\n# Print units of each CRS \nst_crs(col)$units\nst_crs(roads)$units\nst_crs(aves)$units\n\nThere are several ways to extract the longitude and latitude from the geometry column.\n\npurr Approach\n\naves_df_purrr &lt;- aves %&gt;%\n  # Extract lat & long from geometry column\n  mutate(lon = unlist(purrr::map(aves$geometry, 1)), # longitude = first component (x)\n         lat = unlist(purrr::map(aves$geometry, 2))) %&gt;% # latitude = second component (y)\n  st_drop_geometry() # Remove geometry column now that it's redundant\n\n\n\nst_coordinates Approach\n\naves_df_st_coords &lt;- aves %&gt;%\n  dplyr::mutate(lon = sf::st_coordinates(.)[,1],# Assign first matrix item to \"lon\"\n                lat = sf::st_coordinates(.)[,2]) %&gt;% # Assign second matrix item to \"lat\"\n  st_drop_geometry() # Remove geometry column now that it's redundant\n\nNext, convert to sf object again.\n\n\nst_as_sf Approach\n\naves_df_purrr &lt;- aves %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#bring-all-vector-data-types-together",
    "href": "course-materials/discussions/answer-keys/week2-discussion-answerKey.html#bring-all-vector-data-types-together",
    "title": "Week 2: Discussion Section - Answer Key",
    "section": "Bring All Vector Data Types Together",
    "text": "Bring All Vector Data Types Together\n\n# Boolen check if CRS match between 2 datasets\nst_crs(col) == st_crs(roads)\n\n# Transform bird data into same CRS as other Colombia data\naves &lt;- st_transform(aves, crs = st_crs(col))\n\n\n# Simple plot with all 3 data layers\ntm_shape(col) +\n  tm_polygons() +\n  tm_shape(roads) +\n  tm_lines() +\n  tm_shape(aves) +\n  tm_dots() +\n  tm_layout(main.title = \"Colombia ecoregions, roads,\\nand bird observations\",\n            main.title.size = 1)"
  },
  {
    "objectID": "course-materials/week8.html#class-materials",
    "href": "course-materials/week8.html#class-materials",
    "title": "Remote sensing of vegetation",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nSpectral properties of vegetation (leaf, canopy, and landscape)\n\n\n Lab\nNDVI and phenology\n\n\n Discussion\nPractice function making with raster time series data"
  },
  {
    "objectID": "course-materials/week8.html#assignment-reminders",
    "href": "course-materials/week8.html#assignment-reminders",
    "title": "Remote sensing of vegetation",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 8)\n11/18/2024\n11/18/2024\n\n\nHW\nHomework Assignment #4\n11/11/2024\n11/30/2024\n\n\nPR\nPortfolio Repository\n11/11/2024\n12/07/2024"
  },
  {
    "objectID": "course-materials/week8.html#background-reading",
    "href": "course-materials/week8.html#background-reading",
    "title": "Remote sensing of vegetation",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 10"
  },
  {
    "objectID": "course-materials/resources/good-bad-example/bad_example.html",
    "href": "course-materials/resources/good-bad-example/bad_example.html",
    "title": "Unprofessional Output Example",
    "section": "",
    "text": "Unprofessional Documents\nUnprofessional documents are messy and leave the reader struggling to follow the story of the analysis.\nExamples of components:\n\nmissing introduction for purpose of document, as well as section headers\nloading packages throughout document\nmissing comments, documentation between analysis steps, and formal and detailed data citation\nunnecessary outputs and intermediate checks retained in final version\nmessy code indentation for lists, parameters, functions within functions, etc.\nvery plain plot/map that is lacking important components such as legend, appropriate zoom level, name of study area, etc.\nmissing map explanation or general document conclusion\n\n\n\nElephant tracking in Krugar National Park\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nfp &lt;- list.files(path = here(\"course-materials\"), pattern = \"elephants.csv\", recursive = TRUE, full.names = TRUE)\n\nelephants &lt;- read_csv(fp) %&gt;% \n    sf::st_as_sf(coords = c(\"location-long\", \"location-lat\"), crs = st_crs(4326)) %&gt;%\n          filter(st_is_valid(.))\n\nRows: 283688 Columns: 11\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): sensor-type, individual-taxon-canonical-name, tag-local-identifier...\ndbl  (4): event-id, location-long, location-lat, external-temperature\nlgl  (1): visible\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nelephants\n\nSimple feature collection with 283688 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 31.06269 ymin: -25.37676 xmax: 32.00439 ymax: -23.97868\nGeodetic CRS:  WGS 84\n# A tibble: 283,688 × 10\n   `event-id` visible timestamp           `external-temperature` `sensor-type`\n *      &lt;dbl&gt; &lt;lgl&gt;   &lt;dttm&gt;                               &lt;dbl&gt; &lt;chr&gt;        \n 1 9421351127 TRUE    2007-08-13 00:30:00                     24 gps          \n 2 9421351128 TRUE    2007-08-13 02:00:00                     23 gps          \n 3 9421351129 TRUE    2007-08-13 03:31:00                     21 gps          \n 4 9421351130 TRUE    2007-08-13 04:00:00                     21 gps          \n 5 9421351131 TRUE    2007-08-13 06:00:00                     22 gps          \n 6 9421351132 TRUE    2007-08-13 07:30:00                     30 gps          \n 7 9421351133 TRUE    2007-08-13 08:00:00                     33 gps          \n 8 9421351134 TRUE    2007-08-13 11:30:00                     36 gps          \n 9 9421351135 TRUE    2007-08-13 12:00:00                     36 gps          \n10 9421351136 TRUE    2007-08-13 15:30:00                     33 gps          \n# ℹ 283,678 more rows\n# ℹ 5 more variables: `individual-taxon-canonical-name` &lt;chr&gt;,\n#   `tag-local-identifier` &lt;chr&gt;, `individual-local-identifier` &lt;chr&gt;,\n#   `study-name` &lt;chr&gt;, geometry &lt;POINT [°]&gt;\n\n\n\nmetadata_df &lt;- read_csv(list.files(path = here(\"course-materials\"),\n                                   pattern = \"elephants_metadata.csv\",\n                                     recursive = TRUE,\n                                     full.names = TRUE))\n\nRows: 14 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): tag-id, animal-id, animal-taxon, animal-comments, animal-life-sta...\ndbl   (4): deploy-off-latitude, deploy-off-longitude, deploy-on-latitude, de...\nlgl   (1): animal-sex\ndttm  (2): deploy-on-date, deploy-off-date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nsort(unique(elephants$\"individual-local-identifier\")) == sort(unique(metadata_df$\"animal-id\"))\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\nmin(metadata_df$\"deploy-on-date\")\n\n[1] \"2007-08-12 22:30:00 UTC\"\n\n\n\nmax(metadata_df$\"deploy-off-date\")\n\n[1] \"2009-08-12 21:30:00 UTC\"\n\n\n\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(rnaturalearth)\nlibrary(osmdata)\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright\n\n\n\nsouth_africa &lt;- rnaturalearth::ne_countries(scale = \"medium\",\n                                    returnclass = \"sf\") %&gt;%\n                                    filter(admin == \"South Africa\")\n\ntm_shape(south_africa) +\n tm_borders(col = \"black\", lwd = 0.5) +\n tm_fill(col = \"white\") +\n tm_shape(elephants) +\ntm_dots(col = \"individual-local-identifier\",\npalette = 'viridis',\nsize = 0.1,\nborder.col = \"black\",\ntitle = \"Individual\") +\n tm_layout(main.title = \"Elephant Observations\",\n   title.position = c(\"center\", \"top\"), title.size = 1.2,\n   legend.show = FALSE)\n\n\n\n\n\n\n\n\nData: https://datarepository.movebank.org"
  },
  {
    "objectID": "course-materials/resources/plotting.html",
    "href": "course-materials/resources/plotting.html",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "In R, various geospatial packages exist for visualizing and manipulating spatial data:\n\nsf\nggplot2\nmapview\nleaflet\ntmap\nterra\n\nVisualizing vector data, raster data, or both overlaid can be accomplished in various ways. This resource provides example code and explanations for which packages are recommended to get the most out of your visualizations.\n\n\n\n\n\n\n\n\n\nPreference\nPackage\n\n\n\n\nAre you already familiar with ggplot for plotting, and you’re interested in a static visualization?\nggplot2\n\n\nAre you interested in highly tailorable map features and options to create either static or interactive visualizations?\ntmap\n\n\nAre you interested in mapping raster data quickly with less code and less flexibility?\nterra::plot\n\n\nAre you interested creating interactive maps quickly with high flexibility?\nleaflet\n\n\nAre you interested in exploring spatial objects of any sort interactively?\nmapview\n\n\n\n\n\n\nsf, tmap and ggplot2 are great options for visualizing vector data, which is tabular data such as points, lines, and polygons.\nExamples of vector file formats:\n\nshapefile (.shp and auxillary files .shx, .dbf, .prj, etc.)\nGeoPackage (.gpkg)\nGeoParquet (.parquet)\nGeoJSON (.geojson)\n\n\n\nUse the gbif API to download species observations for polar bears from the Global Biodiversity Information Facility.\n\n\nShow the code\n# read in a list of items containing polar bear data, including metadata\npb_data &lt;- occ_search(scientificName = \"Ursus maritimus\", \n                      limit = 300)\n\n# subset the imported data to just the relevant dataframe and attributes\npb_obs &lt;- pb_data$data %&gt;% \n  select(decimalLongitude, \n         decimalLatitude, \n         year,\n         country) %&gt;%\n  mutate(year = as.factor(year)) # year = categorical\n  \n# remove rows with NA in any col\npb_obs &lt;- na.omit(pb_obs)\n\n\n\n\n\nSpatial data will not always already contain critical spatial metadata, so you may have to manually assign it using spatial operations before plotting. For example, point data may contain latitude and longitude coordinates into separate columns and may not come with a set coordinate reference system (CRS). sf can help create a geometry column from separate latitude and longitude columns and set the CRS to WGS84, EPSG:4326.\n\n\nShow the code\n# convert separate longitude and latitude columns into \n# cohesive point geometries, and set the CRS\npb_spatial &lt;- pb_obs %&gt;% \n  sf::st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\n\n\n\nStart with the basics: plot the point data using the native R function plot(), which does not include an interactive feature and is not highly tailorable.\n\n\nShow the code\nplot(st_geometry(pb_spatial),\n     main = \"Polar Bear Observations\",\n     col = \"black\",\n     pch = 16,\n     axes = TRUE,\n     xlab = \"Longitude\",\n     ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\nThat static plot is pretty bland, and it has little context without a palette or a basemap.\n\n\n\nMake an interactive map with the color of the points representing the year of the observation on a basemap. This can be done with either mapview or leaflet.\n\n\n\n\nShow the code\nmapview(pb_spatial,\n        zcol = \"year\",\n        map.types = \"Esri.NatGeoWorldMap\",\n        legend = TRUE,\n        layer.name = \"Polar Bear Observations\")\n\n\n\n\n\n\nThe points are clickable when this is rendered locally, and a metadata window pops up for each observation.\n\n\n\n\n\nShow the code\npalette &lt;- colorFactor(palette = 'viridis',\n                       domain = pb_spatial$year)\n\nleaflet(data = pb_spatial) %&gt;%\n  addProviderTiles(\"Esri.NatGeoWorldMap\") %&gt;% \n  addCircleMarkers(\n    radius = 5,\n    color = \"black\",  # point edges\n    fillColor = ~palette(year),\n    fillOpacity = 0.7,\n    stroke = TRUE,\n    weight = 1,  # point edge thickness\n    popup = ~paste(\"Year:\", year) # clickable points, show observation year\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = palette, \n    values = ~year,\n    title = \"Polar Bear Observations\",\n    opacity = 1\n  )\n\n\n\n\n\n\n\n\n\n\nsf and ggplot can be used in conjunction to plot the polar bear observations statically on a basemap. The default x and y gridlines are cohesive with latitude/longitude point data.\n\n\nShow the code\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\") \n\nggplot(data = world) +\n  geom_sf() +\n  geom_sf(data = pb_spatial, \n          aes(fill = year),  # point color based on 'year'\n          color = \"black\",  # point edges black\n          shape = 21,\n          size = 2, \n          alpha = 0.7) +  # transparency\n  labs(title = \"Polar Bear Observations\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  # limit map to polar bear habitat latitudes\n  coord_sf(xlim = c(-180, 180), ylim = c(45, 90), expand = FALSE) +\n  theme_minimal() + \n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nggplot can make more complex maps, too:\n\n\nShow the code\nggplot() +\n  geom_sf(data = world, fill = \"palegreen\", color = \"darkgreen\") +  # Gray land with dark borders\n  geom_sf(data = pb_spatial, \n          aes(fill = year),\n          color = \"black\",\n          shape = 21, \n          size = 2, \n          alpha = 0.7) +\n  # limit the map to polar bear habitat latitudes & add more gridlines\n  coord_sf(xlim = c(-180, 180),\n           ylim = c(45, 90),\n           expand = FALSE) +\n  scale_x_continuous(breaks = seq(-180, 180, by = 10)) +\n  scale_y_continuous(breaks = seq(45, 90, by = 10)) +\n  labs(title = \"Polar Bear Observations\",\n       subtitle = \"CRS EPSG:4326\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  theme(panel.background = element_rect(fill = \"lightblue\"), # blue ocean\n        plot.title = element_text(hjust = 0.5), # center the title\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"right\",\n        legend.box.background = element_rect(color = \"black\", \n                                             size = 0.5)) \n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\ntmap is specifically designed for mapping spatial data with many highly tailorable options, making it more customizable than ggplot. It recognizes spatial objects from sf, terra, and other geospatial packages. tmap can make both static and interactive maps, as it builds on ggplot and leaflet. More detailed basemaps, like those availble from leaflet and ESRI, are only an option in interactive mode for tmap. For large-scale static data, you can load in a simple world map to use as a basemap.\ntmap allows for fine control over the locations of the title and legend. You can choose inside or outside the map, with values between 0-1 specified for the x and y position.\nMake a static tmap:\n\n\nShow the code\n# clarify the default mode is static plot\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ndata(World)\n\ntm_shape(World) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"white\") +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = 'viridis',\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(\n    bg.color = \"lightblue\",\n    title = \"Polar Bear\\nObservations\",\n    frame = TRUE,\n    title.position = c(0.01, 0.5),\n    title.size = 1.2,\n    legend.frame = TRUE,\n    legend.position = c(0.01, 0.2)\n  )\n\n\n\n\n\n\n\n\n\nMake an interactive tmap:\n\n\nShow the code\n# set mode to interactive\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nShow the code\ntm_shape(World) +\n  tm_borders(col = \"black\", \n             lwd = 0.5) +\n  tm_fill(col = \"white\", \n          alpha = 0.5) +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = \"viridis\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(bg.color = \"lightblue\",\n            title = \"Polar Bear Observations\",\n            title.size = 1.2,\n            legend.frame = TRUE)\n\n\n\n\n\n\n\n\n\n\nPolygon data is composed of multiple points connected by lines to create closed shapes. Since there are many polar bears in Canada and Greenland, plot the polar bear observations on top of only Canada and Greenland polygons using both tmap and ggplot.\n\n\nShow the code\ncanada_greenland &lt;- rnaturalearth::ne_countries(scale = \"medium\", \n                                                returnclass = \"sf\") %&gt;% \n                                   filter(admin %in% c(\"Canada\", \"Greenland\"))\n\n\nWe only want to plot the polar bear points that fit within these polygons, so execute a spatial join.\n\n\nShow the code\npb_canada_greenland &lt;- st_join(pb_spatial, \n                               canada_greenland, \n                               join = st_within,\n                               left = FALSE)\n\n\n\n\n\n\nShow the code\nblue_palette &lt;- RColorBrewer::brewer.pal(n = 2, name = \"Blues\")\n\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n       ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# static map setting, this is the default, but\n# needs to be reset if previously set to \"interactive\"\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10, # number of gridlines on x and y axes\n          alpha = 0.5) +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\",\n            title.size = 1,\n            legend.title.size = 0.9,\n            title.fontface = \"bold\",\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\nAdd a polygon for polar bear habitat range from the International Union for Conservation of Nature (IUCN) Red List. This means we have 3 vector objects overlaid: polygons for country borders, points for polar bear observations, and 1 polygon for habitat.\n\n\nShow the code\n# search for file anywhere within \"course-materials\" dir\nhab_filename = \"data_0.shp\"\nhab_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = hab_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\npb_habitat = st_read(hab_fp)\n# convert from a \"simple feature collection\" with \n# 15 fields to just the geometry\npb_habitat_poly &lt;- st_geometry(pb_habitat)\n\n\n\n\nShow the code\nplot(pb_habitat_poly,\n     main = \"Polar Bear Habitat Range\",\n     col = \"lightyellow\",\n     axes = TRUE,\n     xlab = \"Latitude\",\n     ylab = \"Longitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  geom_sf(data = pb_habitat_poly,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          alpha = 0.2) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       subtitle = \"with habitat range\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n      ylab = \"Latitude\") +\n  # limit map window: zoom into Canada and Greenland\n  coord_sf(xlim = st_bbox(canada_greenland)[c(\"xmin\", \"xmax\")],\n           ylim = st_bbox(canada_greenland)[c(\"ymin\", \"ymax\")])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10,\n          alpha = 0.5) +\n  tm_shape(pb_habitat_poly) +\n  tm_fill(col = \"yellow\", \n          alpha = 0.2, \n          title = \"habitat\") +\n  tm_borders(col = \"darkgoldenrod1\") +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\\nwith Habitat Range\",\n            title.size = 1,\n            title.fontface = \"bold\",\n            legend.title.size = 0.9,\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nterra specializes in raster data processing, which are n-dimensional arrays.\nExamples of file formats:\n\nGeoTIFF (Tag Image File Format, .tif)\nnetCDF (Network Common Data Form, .nc)\nPNG or JPEG images (.png, .jpg)\n\n\n\nImport a raster of sea ice for the Northern Hemisphere and plot it as simply as possible with terra::plot\n\n\nShow the code\nice_filename = \"N_197812_concentration_v3.0.tif\"\nice_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = ice_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\narctic_ice &lt;- terra::rast(ice_fp)\n\nterra::plot(arctic_ice,\n            main = \"Arctic Sea Ice Concentration\")\n\n\n\n\n\n\n\n\n\nThis data has a default palette; each cell is color-coded in shades of blue to white, where dark blue is 0% ice (open ocean) and white is 100% ice. You can view the default palette (“color table”) with terra::coltab\nThe CRS is projected and in units of meters, with each raster cell representing 25 km x 25 km. See CRS metadata here.\nMake a histogram of the raster values using the base R hist to understand the numerical data distribution:\n\n\nShow the code\nhist(arctic_ice,\n     main = \"Arctic Sea Ice Concentration Raster Values\",\n     xlab = \"Values\",\n     ylab = \"Frequency\",\n     col = \"deepskyblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\n\nIn order to properly plot multiple spatial objects on top of one another, they must have the same CRS. Transform the CRS of the habitat polygon into the CRS of the raster for Arctic sea ice, then plot the habitat polygon onto the raster.\n\n\nShow the code\npb_habitat_arctic &lt;- st_transform(pb_habitat_poly, st_crs(arctic_ice))\n\nterra::plot(arctic_ice,\n            main = \"Sea Ice Concentration and Polar Bear Habitat\")\nterra::plot(pb_habitat_arctic, \n            add = TRUE, \n            border = \"darkgoldenrod1\", \n            col = adjustcolor(\"yellow\", alpha.f = 0.2))\n\n\n\n\n\n\n\n\n\nterra automatically defines the x and y axes ticks based on the spatial metadata of the raster.\n\n\n\nScale the data values between 0-100 and convert the array into a dataframe, because ggplot only accepts tabular data. Assign a new palette using RColorBrewer that is similar to the color table associated with the terra plot above.\n\n\nShow the code\n# reverse color palette to better match the default terra::plot palette values\nblue_palette &lt;- rev(RColorBrewer::brewer.pal(n = 9, name = \"Blues\"))\n\n# scale the data 0-100:\n# there are no NA values in this raster but na.rm = TRUE is good practice\nrange &lt;- range(values(arctic_ice), na.rm = TRUE)\narctic_ice_scaled &lt;- (arctic_ice-range[1]) / (range[2]-range[1]) * 100\n\narctic_ice_df &lt;- terra::as.data.frame(arctic_ice_scaled,\n                               cells = FALSE, # do not create index col\n                               xy = TRUE)  %&gt;% # include lat and long cols\n                 dplyr::rename(ice_concentration = N_197812_concentration_v3.0)\n\nggplot() +\n  geom_raster(data = arctic_ice_df,\n              aes(x = x, y = y,\n                  fill = ice_concentration)) +\n  scale_fill_gradientn(colors = blue_palette) +\n  geom_sf(data = pb_habitat_arctic,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          size = 0.2,\n          alpha = 0.1) +\n  coord_sf(default_crs = st_crs(pb_habitat_arctic)) +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill = \"grey\", color = NA),\n        plot.background = element_rect(fill = \"grey\", color = NA)) +\n  labs(title = \"Sea Ice Concentration\\nand Polar Bear Habitat Range\",\n       subtitle = \"Proj CRS: NSIDC Sea Ice Polar Stereographic North\",\n       fill = \"Sea Ice\\nConcentration\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n\n\n\n\n\nNote that ggplot does not automatically derive the units of meters from the projected CRS from the spatial metadata. Instead, it uses 4326 by default. As a result, we mask the axes ticks. Axes ticks can manually be defined with scale_x_continuous and scale_y_continuous\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntmap_options(max.categories = 9)\n\ntm_shape(arctic_ice_scaled) +\n  tm_raster(palette = blue_palette,\n            title = \"Sea Ice\\nConcentration\",\n            style = \"cont\",\n            breaks = seq(0, 100, length.out = 11),\n            midpoint = NA) +\ntm_shape(pb_habitat_arctic) +\n  tm_polygons(col = \"yellow\",\n              border.col = \"darkgoldenrod1\",\n              lwd = 1,\n              alpha = 0.1) +\ntm_graticules(n.x = 5, n.y = 5, \n              labels.show = TRUE, \n              labels.size = 0.6,\n              alpha = 0.3) +\ntm_layout(title = \"Sea Ice\\nConcentration\\nand Polar Bear\\nHabitat Range\",\n          main.title.size = 0.8,\n          title.fontface = \"bold\",\n          legend.outside = TRUE,\n          legend.title.size = 1,\n          legend.outside.position = \"right\",\n          inner.margins = c(0.1, 0.1, 0.1, 0.1)) +\ntm_scale_bar(position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nCitation\n\n\n\n\nGBIF, polar bear observation points\nGBIF.org (01 September 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.79778w\n\n\nIUCN Red List, polar bear range polygon\nIUCN. 2024. The IUCN Red List of Threatened Species. Version 2024-1. https://www.iucnredlist.org. Accessed on Septmeber 5, 2024.\n\n\nNSIDC, sea ice concentration raster\nFetterer, F., Knowles, K., Meier, W. N., Savoie, M. & Windnagel, A. K. (2017). Sea Ice Index. (G02135, Version 3). [Data Set]. Boulder, Colorado USA. National Snow and Ice Data Center. https://doi.org/10.7265/N5K072F8. [describe subset used if applicable]. Date Accessed 09-13-2024.\n\n\n\nNSIDC sea ice concentration metadata"
  },
  {
    "objectID": "course-materials/resources/plotting.html#which-mapping-package-should-i-use",
    "href": "course-materials/resources/plotting.html#which-mapping-package-should-i-use",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "Preference\nPackage\n\n\n\n\nAre you already familiar with ggplot for plotting, and you’re interested in a static visualization?\nggplot2\n\n\nAre you interested in highly tailorable map features and options to create either static or interactive visualizations?\ntmap\n\n\nAre you interested in mapping raster data quickly with less code and less flexibility?\nterra::plot\n\n\nAre you interested creating interactive maps quickly with high flexibility?\nleaflet\n\n\nAre you interested in exploring spatial objects of any sort interactively?\nmapview"
  },
  {
    "objectID": "course-materials/resources/plotting.html#vector-data-points",
    "href": "course-materials/resources/plotting.html#vector-data-points",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "sf, tmap and ggplot2 are great options for visualizing vector data, which is tabular data such as points, lines, and polygons.\nExamples of vector file formats:\n\nshapefile (.shp and auxillary files .shx, .dbf, .prj, etc.)\nGeoPackage (.gpkg)\nGeoParquet (.parquet)\nGeoJSON (.geojson)\n\n\n\nUse the gbif API to download species observations for polar bears from the Global Biodiversity Information Facility.\n\n\nShow the code\n# read in a list of items containing polar bear data, including metadata\npb_data &lt;- occ_search(scientificName = \"Ursus maritimus\", \n                      limit = 300)\n\n# subset the imported data to just the relevant dataframe and attributes\npb_obs &lt;- pb_data$data %&gt;% \n  select(decimalLongitude, \n         decimalLatitude, \n         year,\n         country) %&gt;%\n  mutate(year = as.factor(year)) # year = categorical\n  \n# remove rows with NA in any col\npb_obs &lt;- na.omit(pb_obs)\n\n\n\n\n\nSpatial data will not always already contain critical spatial metadata, so you may have to manually assign it using spatial operations before plotting. For example, point data may contain latitude and longitude coordinates into separate columns and may not come with a set coordinate reference system (CRS). sf can help create a geometry column from separate latitude and longitude columns and set the CRS to WGS84, EPSG:4326.\n\n\nShow the code\n# convert separate longitude and latitude columns into \n# cohesive point geometries, and set the CRS\npb_spatial &lt;- pb_obs %&gt;% \n  sf::st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\n\n\n\nStart with the basics: plot the point data using the native R function plot(), which does not include an interactive feature and is not highly tailorable.\n\n\nShow the code\nplot(st_geometry(pb_spatial),\n     main = \"Polar Bear Observations\",\n     col = \"black\",\n     pch = 16,\n     axes = TRUE,\n     xlab = \"Longitude\",\n     ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\nThat static plot is pretty bland, and it has little context without a palette or a basemap.\n\n\n\nMake an interactive map with the color of the points representing the year of the observation on a basemap. This can be done with either mapview or leaflet.\n\n\n\n\nShow the code\nmapview(pb_spatial,\n        zcol = \"year\",\n        map.types = \"Esri.NatGeoWorldMap\",\n        legend = TRUE,\n        layer.name = \"Polar Bear Observations\")\n\n\n\n\n\n\nThe points are clickable when this is rendered locally, and a metadata window pops up for each observation.\n\n\n\n\n\nShow the code\npalette &lt;- colorFactor(palette = 'viridis',\n                       domain = pb_spatial$year)\n\nleaflet(data = pb_spatial) %&gt;%\n  addProviderTiles(\"Esri.NatGeoWorldMap\") %&gt;% \n  addCircleMarkers(\n    radius = 5,\n    color = \"black\",  # point edges\n    fillColor = ~palette(year),\n    fillOpacity = 0.7,\n    stroke = TRUE,\n    weight = 1,  # point edge thickness\n    popup = ~paste(\"Year:\", year) # clickable points, show observation year\n  ) %&gt;%\n  addLegend(\n    \"bottomright\",\n    pal = palette, \n    values = ~year,\n    title = \"Polar Bear Observations\",\n    opacity = 1\n  )\n\n\n\n\n\n\n\n\n\n\nsf and ggplot can be used in conjunction to plot the polar bear observations statically on a basemap. The default x and y gridlines are cohesive with latitude/longitude point data.\n\n\nShow the code\nworld &lt;- rnaturalearth::ne_countries(scale = \"medium\", returnclass = \"sf\") \n\nggplot(data = world) +\n  geom_sf() +\n  geom_sf(data = pb_spatial, \n          aes(fill = year),  # point color based on 'year'\n          color = \"black\",  # point edges black\n          shape = 21,\n          size = 2, \n          alpha = 0.7) +  # transparency\n  labs(title = \"Polar Bear Observations\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  # limit map to polar bear habitat latitudes\n  coord_sf(xlim = c(-180, 180), ylim = c(45, 90), expand = FALSE) +\n  theme_minimal() + \n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nggplot can make more complex maps, too:\n\n\nShow the code\nggplot() +\n  geom_sf(data = world, fill = \"palegreen\", color = \"darkgreen\") +  # Gray land with dark borders\n  geom_sf(data = pb_spatial, \n          aes(fill = year),\n          color = \"black\",\n          shape = 21, \n          size = 2, \n          alpha = 0.7) +\n  # limit the map to polar bear habitat latitudes & add more gridlines\n  coord_sf(xlim = c(-180, 180),\n           ylim = c(45, 90),\n           expand = FALSE) +\n  scale_x_continuous(breaks = seq(-180, 180, by = 10)) +\n  scale_y_continuous(breaks = seq(45, 90, by = 10)) +\n  labs(title = \"Polar Bear Observations\",\n       subtitle = \"CRS EPSG:4326\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       fill = \"Year\") +\n  theme(panel.background = element_rect(fill = \"lightblue\"), # blue ocean\n        plot.title = element_text(hjust = 0.5), # center the title\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"right\",\n        legend.box.background = element_rect(color = \"black\", \n                                             size = 0.5)) \n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\ntmap is specifically designed for mapping spatial data with many highly tailorable options, making it more customizable than ggplot. It recognizes spatial objects from sf, terra, and other geospatial packages. tmap can make both static and interactive maps, as it builds on ggplot and leaflet. More detailed basemaps, like those availble from leaflet and ESRI, are only an option in interactive mode for tmap. For large-scale static data, you can load in a simple world map to use as a basemap.\ntmap allows for fine control over the locations of the title and legend. You can choose inside or outside the map, with values between 0-1 specified for the x and y position.\nMake a static tmap:\n\n\nShow the code\n# clarify the default mode is static plot\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ndata(World)\n\ntm_shape(World) +\n  tm_borders(col = \"black\", lwd = 0.5) +\n  tm_fill(col = \"white\") +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = 'viridis',\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(\n    bg.color = \"lightblue\",\n    title = \"Polar Bear\\nObservations\",\n    frame = TRUE,\n    title.position = c(0.01, 0.5),\n    title.size = 1.2,\n    legend.frame = TRUE,\n    legend.position = c(0.01, 0.2)\n  )\n\n\n\n\n\n\n\n\n\nMake an interactive tmap:\n\n\nShow the code\n# set mode to interactive\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nShow the code\ntm_shape(World) +\n  tm_borders(col = \"black\", \n             lwd = 0.5) +\n  tm_fill(col = \"white\", \n          alpha = 0.5) +\n  tm_shape(pb_spatial) +\n  tm_dots(col = \"year\",\n          palette = \"viridis\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Year\") +\n  tm_layout(bg.color = \"lightblue\",\n            title = \"Polar Bear Observations\",\n            title.size = 1.2,\n            legend.frame = TRUE)"
  },
  {
    "objectID": "course-materials/resources/plotting.html#vector-data-polygons",
    "href": "course-materials/resources/plotting.html#vector-data-polygons",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "Polygon data is composed of multiple points connected by lines to create closed shapes. Since there are many polar bears in Canada and Greenland, plot the polar bear observations on top of only Canada and Greenland polygons using both tmap and ggplot.\n\n\nShow the code\ncanada_greenland &lt;- rnaturalearth::ne_countries(scale = \"medium\", \n                                                returnclass = \"sf\") %&gt;% \n                                   filter(admin %in% c(\"Canada\", \"Greenland\"))\n\n\nWe only want to plot the polar bear points that fit within these polygons, so execute a spatial join.\n\n\nShow the code\npb_canada_greenland &lt;- st_join(pb_spatial, \n                               canada_greenland, \n                               join = st_within,\n                               left = FALSE)\n\n\n\n\n\n\nShow the code\nblue_palette &lt;- RColorBrewer::brewer.pal(n = 2, name = \"Blues\")\n\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n       ylab = \"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# static map setting, this is the default, but\n# needs to be reset if previously set to \"interactive\"\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10, # number of gridlines on x and y axes\n          alpha = 0.5) +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\",\n            title.size = 1,\n            legend.title.size = 0.9,\n            title.fontface = \"bold\",\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")\n\n\n\n\n\n\n\n\n\n\n\n\nAdd a polygon for polar bear habitat range from the International Union for Conservation of Nature (IUCN) Red List. This means we have 3 vector objects overlaid: polygons for country borders, points for polar bear observations, and 1 polygon for habitat.\n\n\nShow the code\n# search for file anywhere within \"course-materials\" dir\nhab_filename = \"data_0.shp\"\nhab_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = hab_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\npb_habitat = st_read(hab_fp)\n# convert from a \"simple feature collection\" with \n# 15 fields to just the geometry\npb_habitat_poly &lt;- st_geometry(pb_habitat)\n\n\n\n\nShow the code\nplot(pb_habitat_poly,\n     main = \"Polar Bear Habitat Range\",\n     col = \"lightyellow\",\n     axes = TRUE,\n     xlab = \"Latitude\",\n     ylab = \"Longitude\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data = canada_greenland) +\n  geom_sf(aes(fill = admin), \n          color = \"black\") +\n  scale_fill_manual(values = blue_palette) +\n  geom_sf(data = pb_canada_greenland,\n          color = \"red\",\n          size = 1) +\n  geom_sf(data = pb_habitat_poly,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          alpha = 0.2) +\n  theme_minimal() +\n  labs(title = \"Canada and Greenland Polar Bear Observations\",\n       subtitle = \"with habitat range\",\n       fill = \"Country\",\n       xlab = \"Longitude\",\n      ylab = \"Latitude\") +\n  # limit map window: zoom into Canada and Greenland\n  coord_sf(xlim = st_bbox(canada_greenland)[c(\"xmin\", \"xmax\")],\n           ylim = st_bbox(canada_greenland)[c(\"ymin\", \"ymax\")])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(canada_greenland) + \n  tm_fill(col = \"admin\", \n          palette = blue_palette, \n          title = \"Country\") +\n  tm_borders(col = \"black\") +\n  tm_shape(pb_canada_greenland) +\n  tm_dots(col = \"red\", size = 0.1) +\n  tm_grid(lines = TRUE,\n          col = \"gray\",\n          labels.inside.frame = TRUE,\n          n.x = 10, n.y = 10,\n          alpha = 0.5) +\n  tm_shape(pb_habitat_poly) +\n  tm_fill(col = \"yellow\", \n          alpha = 0.2, \n          title = \"habitat\") +\n  tm_borders(col = \"darkgoldenrod1\") +\n  tm_layout(title = \"Canada and Greenland\\nPolar Bear Observations\\nwith Habitat Range\",\n            title.size = 1,\n            title.fontface = \"bold\",\n            legend.title.size = 0.9,\n            legend.text.size = 0.8,\n            title.position = c(0.66, 0.31)) +\n  tm_xlab(\"Longitude\") +\n  tm_ylab(\"Latitude\")"
  },
  {
    "objectID": "course-materials/resources/plotting.html#raster-data",
    "href": "course-materials/resources/plotting.html#raster-data",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "terra specializes in raster data processing, which are n-dimensional arrays.\nExamples of file formats:\n\nGeoTIFF (Tag Image File Format, .tif)\nnetCDF (Network Common Data Form, .nc)\nPNG or JPEG images (.png, .jpg)\n\n\n\nImport a raster of sea ice for the Northern Hemisphere and plot it as simply as possible with terra::plot\n\n\nShow the code\nice_filename = \"N_197812_concentration_v3.0.tif\"\nice_fp &lt;- list.files(path = here(\"course-materials\"),\n                 pattern = ice_filename,\n                 recursive = TRUE,\n                 full.names = TRUE)\n\narctic_ice &lt;- terra::rast(ice_fp)\n\nterra::plot(arctic_ice,\n            main = \"Arctic Sea Ice Concentration\")\n\n\n\n\n\n\n\n\n\nThis data has a default palette; each cell is color-coded in shades of blue to white, where dark blue is 0% ice (open ocean) and white is 100% ice. You can view the default palette (“color table”) with terra::coltab\nThe CRS is projected and in units of meters, with each raster cell representing 25 km x 25 km. See CRS metadata here.\nMake a histogram of the raster values using the base R hist to understand the numerical data distribution:\n\n\nShow the code\nhist(arctic_ice,\n     main = \"Arctic Sea Ice Concentration Raster Values\",\n     xlab = \"Values\",\n     ylab = \"Frequency\",\n     col = \"deepskyblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\n\nIn order to properly plot multiple spatial objects on top of one another, they must have the same CRS. Transform the CRS of the habitat polygon into the CRS of the raster for Arctic sea ice, then plot the habitat polygon onto the raster.\n\n\nShow the code\npb_habitat_arctic &lt;- st_transform(pb_habitat_poly, st_crs(arctic_ice))\n\nterra::plot(arctic_ice,\n            main = \"Sea Ice Concentration and Polar Bear Habitat\")\nterra::plot(pb_habitat_arctic, \n            add = TRUE, \n            border = \"darkgoldenrod1\", \n            col = adjustcolor(\"yellow\", alpha.f = 0.2))\n\n\n\n\n\n\n\n\n\nterra automatically defines the x and y axes ticks based on the spatial metadata of the raster.\n\n\n\nScale the data values between 0-100 and convert the array into a dataframe, because ggplot only accepts tabular data. Assign a new palette using RColorBrewer that is similar to the color table associated with the terra plot above.\n\n\nShow the code\n# reverse color palette to better match the default terra::plot palette values\nblue_palette &lt;- rev(RColorBrewer::brewer.pal(n = 9, name = \"Blues\"))\n\n# scale the data 0-100:\n# there are no NA values in this raster but na.rm = TRUE is good practice\nrange &lt;- range(values(arctic_ice), na.rm = TRUE)\narctic_ice_scaled &lt;- (arctic_ice-range[1]) / (range[2]-range[1]) * 100\n\narctic_ice_df &lt;- terra::as.data.frame(arctic_ice_scaled,\n                               cells = FALSE, # do not create index col\n                               xy = TRUE)  %&gt;% # include lat and long cols\n                 dplyr::rename(ice_concentration = N_197812_concentration_v3.0)\n\nggplot() +\n  geom_raster(data = arctic_ice_df,\n              aes(x = x, y = y,\n                  fill = ice_concentration)) +\n  scale_fill_gradientn(colors = blue_palette) +\n  geom_sf(data = pb_habitat_arctic,\n          fill = \"yellow\",\n          color = \"darkgoldenrod1\",\n          size = 0.2,\n          alpha = 0.1) +\n  coord_sf(default_crs = st_crs(pb_habitat_arctic)) +\n  theme(panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.background = element_rect(fill = \"grey\", color = NA),\n        plot.background = element_rect(fill = \"grey\", color = NA)) +\n  labs(title = \"Sea Ice Concentration\\nand Polar Bear Habitat Range\",\n       subtitle = \"Proj CRS: NSIDC Sea Ice Polar Stereographic North\",\n       fill = \"Sea Ice\\nConcentration\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n\n\n\n\n\nNote that ggplot does not automatically derive the units of meters from the projected CRS from the spatial metadata. Instead, it uses 4326 by default. As a result, we mask the axes ticks. Axes ticks can manually be defined with scale_x_continuous and scale_y_continuous\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nShow the code\ntmap_options(max.categories = 9)\n\ntm_shape(arctic_ice_scaled) +\n  tm_raster(palette = blue_palette,\n            title = \"Sea Ice\\nConcentration\",\n            style = \"cont\",\n            breaks = seq(0, 100, length.out = 11),\n            midpoint = NA) +\ntm_shape(pb_habitat_arctic) +\n  tm_polygons(col = \"yellow\",\n              border.col = \"darkgoldenrod1\",\n              lwd = 1,\n              alpha = 0.1) +\ntm_graticules(n.x = 5, n.y = 5, \n              labels.show = TRUE, \n              labels.size = 0.6,\n              alpha = 0.3) +\ntm_layout(title = \"Sea Ice\\nConcentration\\nand Polar Bear\\nHabitat Range\",\n          main.title.size = 0.8,\n          title.fontface = \"bold\",\n          legend.outside = TRUE,\n          legend.title.size = 1,\n          legend.outside.position = \"right\",\n          inner.margins = c(0.1, 0.1, 0.1, 0.1)) +\ntm_scale_bar(position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "course-materials/resources/plotting.html#data-citations",
    "href": "course-materials/resources/plotting.html#data-citations",
    "title": "Plotting Geospatial Data: Vectors and Rasters",
    "section": "",
    "text": "Dataset\nCitation\n\n\n\n\nGBIF, polar bear observation points\nGBIF.org (01 September 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.79778w\n\n\nIUCN Red List, polar bear range polygon\nIUCN. 2024. The IUCN Red List of Threatened Species. Version 2024-1. https://www.iucnredlist.org. Accessed on Septmeber 5, 2024.\n\n\nNSIDC, sea ice concentration raster\nFetterer, F., Knowles, K., Meier, W. N., Savoie, M. & Windnagel, A. K. (2017). Sea Ice Index. (G02135, Version 3). [Data Set]. Boulder, Colorado USA. National Snow and Ice Data Center. https://doi.org/10.7265/N5K072F8. [describe subset used if applicable]. Date Accessed 09-13-2024.\n\n\n\nNSIDC sea ice concentration metadata"
  },
  {
    "objectID": "course-materials/template.html",
    "href": "course-materials/template.html",
    "title": "template {add topic}",
    "section": "",
    "text": "Session\nMaterials\n\n\n\n\n Lecture\ndescription\n\n\n Lab\ndescription\n\n\n Discussion\ndescription"
  },
  {
    "objectID": "course-materials/template.html#class-materials",
    "href": "course-materials/template.html#class-materials",
    "title": "template {add topic}",
    "section": "",
    "text": "Session\nMaterials\n\n\n\n\n Lecture\ndescription\n\n\n Lab\ndescription\n\n\n Discussion\ndescription"
  },
  {
    "objectID": "course-materials/template.html#assignment-reminders",
    "href": "course-materials/template.html#assignment-reminders",
    "title": "template {add topic}",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (Week 1)\nMon 01/08/2024\nMon 01/08/2024, 11:55pm PT\n\n\nSR\nSelf reflection (SR #1)\nMon 01/08/2024\nSat 01/13/2024, 11:59pm PT\n\n\nHW\nHomework Assignment #1\nMon 01/08/2024\nSat 01/20/2024, 11:59pm PT"
  },
  {
    "objectID": "course-materials/template.html#background-reading",
    "href": "course-materials/template.html#background-reading",
    "title": "template {add topic}",
    "section": " Background Reading",
    "text": "Background Reading"
  },
  {
    "objectID": "course-materials/template.html#additional-resources",
    "href": "course-materials/template.html#additional-resources",
    "title": "template {add topic}",
    "section": " Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/week3.html#class-materials",
    "href": "course-materials/week3.html#class-materials",
    "title": "Spatial and geometry operations with vector data",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nIntro to basic spatial data and geometry operations with vector data\n\n\n Lab\nSpatial joins, topological relationships, and distance relationships\n\n\n Discussion\nPractice vector operations (Answer Key)"
  },
  {
    "objectID": "course-materials/week3.html#assignment-reminders",
    "href": "course-materials/week3.html#assignment-reminders",
    "title": "Spatial and geometry operations with vector data",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 3)\n10/14/2024\n10/14/2024\n\n\nHW\nHomework Assignment #2\n10/07/2024\n10/19/2024"
  },
  {
    "objectID": "course-materials/week3.html#background-reading",
    "href": "course-materials/week3.html#background-reading",
    "title": "Spatial and geometry operations with vector data",
    "section": " Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 4\nGeocomputation with R, Chapter 5\nGIS Fundamentals, Chapter 9 Part 1\nDouglas–Peucker Algorithm (Cartography Playground)\nLine Simplification with Visvalingam–Whyatt Algorithm (Mike Bostok)"
  },
  {
    "objectID": "course-materials/week3.html#technical-background",
    "href": "course-materials/week3.html#technical-background",
    "title": "Spatial and geometry operations with vector data",
    "section": " Technical Background",
    "text": "Technical Background\n\nsf overview\nsf cheatsheet\nIntroduction to sf and stars"
  },
  {
    "objectID": "course-materials/week4.html#class-materials",
    "href": "course-materials/week4.html#class-materials",
    "title": "Raster spatial and geometry operations",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nIntro to raster data\n\n\n Lab\nBasics of raster operations with terra\n\n\n Discussion\nPractice raster manipulations (Answer Key)"
  },
  {
    "objectID": "course-materials/week4.html#assignment-reminders",
    "href": "course-materials/week4.html#assignment-reminders",
    "title": "Raster spatial and geometry operations",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 4)\n10/21/2024\n10/21/2024\n\n\nHW\nHomework Assignment #3\n10/21/2024\n11/09/2024"
  },
  {
    "objectID": "course-materials/week4.html#background-reading",
    "href": "course-materials/week4.html#background-reading",
    "title": "Raster spatial and geometry operations",
    "section": " Background Reading",
    "text": "Background Reading\n\nGeocomputation with R, Chapter 3\nGeocomputation with R, Chapter 4\nGeocomputation with R, Chapter 5"
  },
  {
    "objectID": "course-materials/week10.html#class-materials",
    "href": "course-materials/week10.html#class-materials",
    "title": "Intro to active remote sensing",
    "section": " Class Materials",
    "text": "Class Materials\n\n\n\nSession\nMaterials\n\n\n\n\n Lecture\nFundamentals of active remote sensing: LiDAR and RADAR\n\n\n Lab\nValidating LiDAR tree height estimates\n\n\n Discussion\nPractice gridding and spatial interpolation"
  },
  {
    "objectID": "course-materials/week10.html#assignment-reminders",
    "href": "course-materials/week10.html#assignment-reminders",
    "title": "Intro to active remote sensing",
    "section": " Assignment Reminders",
    "text": "Assignment Reminders\n\n\n\n\n\n\nImportant\n\n\n\nAll assignments are due at 11:59 PM on the date listed.\n\n\n\n\n\nAssignment Type\nAssignment Title\nDate Assigned\nDate Due\n\n\n\n\nEOC\nEnd-of-class survey (week 10)\n11/25/2024\n11/25/2024\n\n\nSR\nEnd-of-course reflection (SR#3)\n12/02/2024\n12/07/2023\n\n\nPR\nPortfolio Repository\n11/11/2024\n12/07/2024"
  },
  {
    "objectID": "course-materials/week10.html#background-reading",
    "href": "course-materials/week10.html#background-reading",
    "title": "Intro to active remote sensing",
    "section": " Background Reading",
    "text": "Background Reading\n\nRemote Sensing of the Environment, Chapter 9\nIntroduction to Interpreting Digital RADAR Images\nIntroduction to Light Detection and Ranging (Lidar) Remote Sensing Data (Earth Lab, CU Boulder)\nWhat is Synthetic Aperture Radar? (NASA)\nGet To Know SAR: Polarimetry (NASA)"
  },
  {
    "objectID": "course-materials/week10.html#additional-resources",
    "href": "course-materials/week10.html#additional-resources",
    "title": "Intro to active remote sensing",
    "section": " Additional Resources",
    "text": "Additional Resources\n\nExplore Lidar Points in Plas.io (Earth Lab, CU Boulder)"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html",
    "href": "course-materials/discussions/week6-discussion.html",
    "title": "Week 6: Discussion Section",
    "section": "",
    "text": "Source Materials\n\n\n\nThe following materials are modified from curriculum developed by Earth Lab and the Raster Analysis with terra book."
  },
  {
    "objectID": "course-materials/labs/week6.html",
    "href": "course-materials/labs/week6.html",
    "title": "Week 6: Lab",
    "section": "",
    "text": "In this lab we’ll continue to explore operations that rely on interactions between vector and raster data. Today, we’ll see how to convert raster data into vector data. We’ll also explore creating false color imagery."
  },
  {
    "objectID": "course-materials/labs/week6.html#set-up",
    "href": "course-materials/labs/week6.html#set-up",
    "title": "Week 6: Lab",
    "section": "1. Set up",
    "text": "1. Set up\nFirst, we’ll load all relevant packages.\n\nlibrary(sf) # vector handling\nlibrary(terra) # raster handling\nlibrary(tidyverse)\nlibrary(tmap) # map making\nlibrary(spData) # spatial data\nlibrary(spDataLarge) # spatial data"
  },
  {
    "objectID": "course-materials/labs/week6.html#rasterization",
    "href": "course-materials/labs/week6.html#rasterization",
    "title": "Week 6: Lab",
    "section": "2. Rasterization",
    "text": "2. Rasterization\n“Rasterization” is the process of representing vector objects as raster objects. You might consider “rasterizing” vector data for the following reasons:\n\nto use in an analysis that benefits from raster operations (e.g. map algebra)\nstandardize with other data used in analysis\nsimplify data to reduce computational load\naggregated data to standard grid\n\nTo “rasterize” data using the {terra} package, we use the rasterize() function. The first two arguments define the following:\n\nx: vector object to be “rasterized”\ny: a ‘template’ raster object defining the extent, resolution, and CRS of the output\n\n\n\n\n\n\n\nDefining the template raster\n\n\n\nThe geographic resolution of the input raster has a major impact on the results.\n\nIf it is too low (cell size is too large), the result may miss the full geographic variability of the vector data\nIf it is too high (cell size is too small), computational times may be excessive\n\nThere are no simple rules to follow when deciding an appropriate geographic resolution, which is heavily dependent on the intended use of the results. Often the target resolution is imposed on the user, for example when the output of rasterization needs to be aligned to some other existing raster.\n\n\n\nLine and polygon rasterization\nThe simplest case of rasterization is simply converting the geometries of vector objects to raster objects. In this case, all we are hoping to do is indicate within each raster cell whether or not the vector object is present there.\nIn most cases, the purpose behind rasterizing vector object is to make it more directly comparable to data that is represented as raster objects. In that case, you want to use the raster object that you would like to compare to as the ‘template’ raster.\nLet’s check out how this works by investigating an example from Zion National Park\n\n# load Zion park boundary (vector object to rasterize)\nboundary &lt;- read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")) \n\n# load elevation raster to use as template raster object\nelevation &lt;- rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n# check and transform coordinate reference systems\nif(crs(elevation) == crs(boundary)) {\n  print(\"Coordinate reference systems match\")\n} else{\n  warning(\"Updating coordinate reference systems to match\")\n  # transform data to match\n  boundary &lt;- st_transform(boundary, st_crs(elevation))\n}\n\nWarning: Updating coordinate reference systems to match\n\n\nRasterization gives different results for polygon versus line vector objects.\n\nRasterized polygons: all grid cells intersecting polygon (including cells inside polygon)\nRasterized lines: grid cells intersecting line (excluding cells potentially enclose by line)\n\nLet’s check out whether or not the park boundary is represented as polygon or line. We can always inspect the geometry type of a vector object using sf::st_geometry_type().\n\nif(sf::st_geometry_type(boundary) == \"POLYGON\"){\n  print(\"polygon data\")\n} else {\n  print(\"not polygon data\")\n}\n\n[1] \"polygon data\"\n\n\nThe park boundary is a polygon, so let’s make a version that just represents the park border using a line geometry.\n\n# update park boundary object name to clarify that it's a polygon\npark_polygon &lt;- boundary\n\n# create line version park boundary\npark_border &lt;- boundary %&gt;%\n  sf::st_cast(., \"MULTILINESTRING\")\n\n\n\nCode\nmap1 &lt;- tm_shape(park_polygon) +\n  tm_polygons() +\n  tm_layout(main.title = \"polygon\")\n\nmap2 &lt;- tm_shape(park_border) +\n  tm_lines() +\n  tm_layout(main.title = \"line\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nNow we can rasterize both the polygon and line representations of the park boundary.\n\npolygon_raster &lt;- terra::rasterize(park_polygon, elevation)\nborder_raster &lt;- terra::rasterize(park_border, elevation)\n\n\n\nCode\nmap1 &lt;- tm_shape(polygon_raster) +\n  tm_raster() +\n  tm_layout(main.title = \"rasterized polygon\")\n\nmap2 &lt;- tm_shape(border_raster) +\n  tm_raster() +\n  tm_layout(main.title = \"rasterized line\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\n\n\n\n\n\n\n\nIn the case where you are rasterizing without a pre-existing raster you would like to match to, you can create a template raster from scratch.\n\n\n\n\n\n\nImpact of resolution\n\n\n\nUsing a custom template raster is a also a great way to explore how the resolution impacts the results of rasterization!\n\n\n\n# create low spatial resolution template raster\ntemplate_raster_low &lt;- terra::rast(ext(park_polygon), resolution = 0.05,\n                        crs = st_crs(park_polygon))\n\n# create high spatial resolution template raster\ntemplate_raster_high &lt;- terra::rast(ext(park_polygon), resolution = 0.01,\n                        crs = st_crs(park_polygon))\n\n# rasterize to low resolution template raster\npolygon_raster_low &lt;- terra::rasterize(park_polygon, template_raster_low)\n\n# rasterize to high resolution template raster\npolygon_raster_high &lt;- terra::rasterize(park_polygon, template_raster_high)\n\n\n\nCode\nmap1 &lt;- tm_shape(polygon_raster_low) +\n  tm_raster() +\n  tm_layout(main.title = \"low resolution\")\n\nmap2 &lt;- tm_shape(polygon_raster_high) +\n  tm_raster() +\n  tm_layout(main.title = \"high resolution\")\n\ntmap_arrange(map1, map2, nrow = 1)\n\n\nWarning: Currect projection of shape polygon_raster_low unknown. Long lat (epsg\n4326) coordinates assumed.\n\n\nWarning: Currect projection of shape polygon_raster_high unknown. Long lat\n(epsg 4326) coordinates assumed.\n\n\n\n\n\n\n\n\n\n\n\nRasterizing point data\nWhen working with point data, we can use the same process as with line and polygon data. However, in some cases we might want to perform more complex operations. Instead of simply indicating whether or not the vector object falls within each grid cell, we might want to count the number of points in each grid cell or even summarize the attributes of points within each cell.\nLet’s try an example using data on cycle hire points in London from {spData}.\n\n# define point data\ncycle_hire_osm &lt;- spData::cycle_hire_osm\n\n# transform to projected CRS\ncycle_hire_osm_projected = sf::st_transform(cycle_hire_osm, \"EPSG:27700\")\n\n# define raster template\ntemplate_raster &lt;- terra::rast(ext(cycle_hire_osm_projected), resolution = 1000,\n                       crs = crs(cycle_hire_osm_projected))\n\nIn the following code chunk we’ll perform three versions of rasterization to produce the following:\n\nraster representing the presence/absence of cycle hire points\nraster representing the number of cycle hire points\nraster representing the total capacity of cycle hire points\n\nTo represent the presence/absence of cycle hire points, we use the same procedure as before.\n\nch_raster1 &lt;- terra::rasterize(cycle_hire_osm_projected, template_raster)\n\nTo represent the number of cycle hire points, we use the fun argument which allows for various summarizing functions. Setting the argument fun = \"length\" will return a count of the number of points in each cell\n\nch_raster2 &lt;- rasterize(cycle_hire_osm_projected, template_raster, \n                       fun = \"length\")\n\nTo represent the total capacity of cycle hire points, we need to take the sum of each points’ capacity. We can do this by defining which field we would like to aggregate (in this case capacity) and what aggregating function we would like to use (in this case fun = sum).\n\nch_raster3 &lt;- rasterize(cycle_hire_osm_projected, template_raster, \n                       field = \"capacity\", fun = sum, na.rm = TRUE)\n\n\n\nCode\nmap1 &lt;- tm_shape(cycle_hire_osm_projected) +\n  tm_symbols(col = \"capacity\") +\n  tm_layout(main.title = \"original points\")\n\nmap2 &lt;- tm_shape(ch_raster1) +\n  tm_raster(title = \"presence\") +\n  tm_layout(main.title = \"presence/absence\")\n\nmap3 &lt;- tm_shape(ch_raster2) +\n  tm_raster(title = \"Hire points (n)\") +\n  tm_layout(main.title = \"count of points\")\n\nmap4 &lt;- tm_shape(ch_raster3) +\n  tm_raster(title = \"Capacity (n bikes)\") +\n  tm_layout(main.title = \"sum of capacity\")\n\ntmap_arrange(map1, map2, map3, map4, nrow = 2)"
  },
  {
    "objectID": "course-materials/labs/week6.html#creating-false-color-images",
    "href": "course-materials/labs/week6.html#creating-false-color-images",
    "title": "Week 6: Lab",
    "section": "3. Creating false color images",
    "text": "3. Creating false color images\n\n\n\n\n\n\nSource Materials\n\n\n\nThe following materials are modified from Humboldt State Geospatial Online.\n\n\nEach band of a multispectral image can be displayed one band at a time as a grey scale image, or in a combination of three bands at a time as a color composite image. Computer screens can display an image in three different channels (red, green, blue) at a time, by using a different primary color for each band. When we combine these three images we get a color composite image.\nNatural or true color composite:\n\ndisplays red, green, blue RS bands in the red, green, blue channels, respectively\n\nFalse color composite:\n\ndisplays RS bands beyond the visible portion of the spectrum, or\ndisplays red, green, blue RS bands not necessarily in the red, green, and blue channels\n\nIn this example, we’ll work with a remote sensing image of Olinda, Brazil collected by the Landsat-7 Enhanced Thematic Mapper.\n\n\n\nLandsat-7 bands\n\n\nBand\nWavelengths\n\n\n\n\nBand 1\n0.45 - 0.52 micrometers (blue)\n\n\nBand 2\n0.52 - 0.60 micrometers (green)\n\n\nBand 3\n0.63 - 0.69 micrometers (red)\n\n\nBand 4\n0.77 - 0.90 micrometers (near-infrared)\n\n\nBand 5\n1.55 - 1.75 micrometers (short-wave infrared)\n\n\nBand 7\n2.08 - 2.35 micrometers (mid-infrared)\n\n\n\n\n\n\n\n\n# load Landsat image\nL7 &lt;- terra::rast(system.file(\"tif/L7_ETMs.tif\", package = \"stars\"))\n\nTo plot various band combinations, we can use tmap::tm_rgb() which allows us to define which band should be displayed in each channel.\nWe can explore various band combinations, but there are a few common false color composites:\n\nNear infrared (red channel), red (green channel), green (blue channel):\n\nPlants reflect near infrared and green light, while absorbing red\nPlants appear deep red and therefore helpful for gauging plant health\n\nShortwave infrared (red channel), near infrared (green channel), green (blue channel):\n\nWater absorbs all three wavelengths, so appears black\nWater and wet soil stand out and therefore helpful for monitoring floods\n\n\n\n\nCode\nmap1 &lt;- tm_shape(L7) +\n  tm_rgb(r = 1, g = 2, b = 3) +\n  tm_layout(main.title = \"true color\")\n\nmap2 &lt;- tm_shape(L7) +\n  tm_rgb(r = 4, g = 3, b = 2) +\n  tm_layout(main.title = \"NIR, red, green\")\n\nmap3 &lt;- tm_shape(L7) +\n  tm_rgb(r = 5, g = 4, b = 2) +\n  tm_layout(main.title = \"SWIR, NIR, green\")\n\ntmap_arrange(map1, map2, map3, nrow = 1)"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#background",
    "href": "course-materials/discussions/week6-discussion.html#background",
    "title": "Week 6: Discussion Section",
    "section": "1. Background",
    "text": "1. Background\nIn Lab #6, you explored various band combinations and plotted false color images with tm_rgb() from {tmap}. False color images allow you to visually highlight specific features in an image that may otherwise not be readily discernible. To add to your toolbelt, you will now be plotting false color images with plotRGB() from {terra}.\n\n\n\n\n\n\nMany Ways To Plot a False Color Image\n\n\n\n\n\nYou can plot false color composites with ggplot2 too! Check out this tutorial.\n\n\n\nThe plotRGB() function allows you to apply a stretch to normalize the colors in an image. You can either apply a linear stretch or histogram equalization. A linear stretch distributes the values across the entire histogram range defined by the max/min lower bounds of the original raster. A histogram equalization stretches dense parts of the histogram and condenses sparse parts.\nBut why would you want to normalize the colors in an image?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“When the range of pixel brightness values is closer to 0, a darker image is rendered by default. You can stretch the values to extend to the full 0-255 range of potential values to increase the visual contrast of the image. When the range of pixel brightness values is closer to 255, a lighter image is rendered by default. You can stretch the values to extend to the full 0-255 range of potential values to increase the visual contrast of the image.”\n\nYou are provided Landsat data for the site of the Cold Springs fire that occurred near Nederland, CO. The fire occurred from July 10-14, 2016 and the Landsat images are from June 5, 2016 (pre-fire) and August 8, 2016 (post-fire). The multispectral bands, wavelength range, and associated spatial resolution of the first 7 bands in the Landsat 8 sensor are listed below.\n\n\n\n\n\n\n\n\n\nBand\nWavelength range (nanometers)\nSpatial Resolution (m)\nSpectral Width (nm)\n\n\n\n\nBand 1 - Coastal aerosol\n430 - 450\n30\n2.0\n\n\nBand 2 - Blue\n450 - 510\n30\n6.0\n\n\nBand 3 - Green\n530 - 590\n30\n6.0\n\n\nBand 4 - Red\n640 - 670\n30\n0.03\n\n\nBand 5 - Near Infrared (NIR)\n850 - 880\n30\n3.0\n\n\nBand 6 - Short-Wave Infrared 1 (SWIR1)\n1570 - 1650\n30\n8.0\n\n\nBand 7 - Short-Wave Infrared 2 (SWIR2)\n2110 - 2290\n30\n18"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#get-started",
    "href": "course-materials/discussions/week6-discussion.html#get-started",
    "title": "Week 6: Discussion Section",
    "section": "2. Get Started",
    "text": "2. Get Started\n\nCreate an .Rproj as your version controlled project for Week 6\nCreate a Quarto document inside your .Rproj\nDownload this data folder from Google Drive and move it inside your .Rproj\nLoad all necessary packages, read spatial objects, and define nbr_fun function\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(terra)\n\n\n# Set directory for folder\npre_fire_dir &lt;- here::here(\"data\", \"LC80340322016189-SC20170128091153\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npre_fire_bands &lt;- list.files(pre_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npre_fire_rast &lt;- rast(pre_fire_bands)\n\n# Read mask raster\npre_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016189LGN00_cfmask_crop.tif\"))\n\n\n# Set directory for folder\npost_fire_dir &lt;- here::here(\"data\", \"LC80340322016205-SC20170127160728\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npost_fire_bands &lt;- list.files(post_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npost_fire_rast &lt;- rast(post_fire_bands)\n\n# Read mask raster\npost_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016205LGN00_cfmask_crop.tif\"))\n\n\nnbr_fun &lt;- function(nir, swir2){\n    (nir - swir2)/(nir + swir2)\n}"
  },
  {
    "objectID": "course-materials/discussions/week6-discussion.html#your-task",
    "href": "course-materials/discussions/week6-discussion.html#your-task",
    "title": "Week 6: Discussion Section",
    "section": "3. Your Task",
    "text": "3. Your Task\nNow, to meet this week’s learning objectives, your task:\n\nRename the bands of the pre_fire and post_fire rasters using names()\n\nNext, for each of the pre_fire and post_fire rasters…\n\nMask out clouds and shadows with the pre_mask and post_mask rasters\n\n\nHint: Set mask &gt; 0 to NA\n\n\n\n\n\n\n\nDealing with Clouds and Shadows\n\n\n\n\n\n“Extreme cloud cover and shadows can make the data in those areas, un-usable given reflectance values are either washed out (too bright - as the clouds scatter all light back to the sensor) or are too dark (shadows which represent blocked or absorbed light)” (Earth Lab)\n\n\n\n\nPlot a true color composite using plotRGB()\n\n\nMap the red band to the red channel, green to green, and blue to blue\nApply a linear stretch “lin” or histogram equalization “hist”\n\n\n\n\n\n\n\nHow To Decide How To “Stretch” Raster Imagery?\n\n\n\n\n\nTo make an informed choice about whether to apply a linear stretch or histogram equalization, check the distribution of reflectance values of the bands in each raster:\n\n# View histogram for each band\nhist(pre_fire_rast,\n     maxpixels = ncell(pre_fire_rast),\n     col = \"orange\")\n\n\n\n\n\nPlot two false color composite using plotRGB()\n\n\nMap the SWIR2 band to the red channel, NIR to green, and green to blue\nApply a linear stretch “lin” or histogram equalization “hist”\n\n\n\n\n\n\n\nWhat is the SWIR, NIR, Red false color scheme?\n\n\n\n“Combining SWIR, NIR, and Red bands highlights the presence of vegetation, clear-cut areas and bare soils, active fires, and smoke; in a false color image” (EOS Data Analytics)\n\n\n\nCalculate the normalized burn ratio (NBR)\n\n\nHint: Use lapp() like you previously did for NDVI and NDWI in Week 4\n\nLet’s bring it home!\n\nFind the difference NBR, where \\(dNBR = prefireNBR - postfireNBR\\)\nPlot the dnBR raster\n\n\nBonus Challenge: Use classify() to assign the severity levels below:\n\n\n\n\nSeverity Level\ndNBR Range\n\n\n\n\nEnhanced Regrowth\n&lt; -.1\n\n\nUnburned\n-.1 to +.1\n\n\nLow Severity\n+.1 to +.27\n\n\nModerate Severity\n+.27 to +.66\n\n\nHigh Severity\n&gt; .66"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(tmaptools)\n# Set directory for folder\npre_fire_dir &lt;- here::here(\"data\", \"LC80340322016189-SC20170128091153\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npre_fire_bands &lt;- list.files(pre_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npre_fire_rast &lt;- rast(pre_fire_bands)\n\n# Read mask raster\npre_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016189LGN00_cfmask_crop.tif\"))\n# Set directory for folder\npost_fire_dir &lt;- here::here(\"data\", \"LC80340322016205-SC20170127160728\")\n\n# Create a list of all images that have the extension .tif and contain the word band\npost_fire_bands &lt;- list.files(post_fire_dir,\n                             pattern = glob2rx(\"*band*.tif$\"),\n                             full.names = TRUE)\n# Create a raster stack\npost_fire_rast &lt;- rast(post_fire_bands)\n\n# Read mask raster\npost_mask &lt;- rast(here::here(\"data\", \"LC80340322016189-SC20170128091153\", \"LC80340322016205LGN00_cfmask_crop.tif\"))\nnbr_fun &lt;- function(nir, swir2){\n    (nir - swir2)/(nir + swir2)\n}"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#subset",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#subset",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Subset",
    "text": "Subset"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#rename-bands",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#rename-bands",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Rename Bands",
    "text": "Rename Bands\n\nbands &lt;- c(\"Aerosol\", \"Blue\", \"Green\", \"Red\", \"NIR\", \"SWIR1\", \"SWIR2\")\nnames(pre_fire_rast) &lt;- bands\nnames(post_fire_rast) &lt;- bands"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#mask-clouds-and-shadows",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#mask-clouds-and-shadows",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Mask Clouds and Shadows",
    "text": "Mask Clouds and Shadows\n\n# Set all cells with values greater than 0 to NA\npre_mask[pre_mask &gt; 0] &lt;- NA\n\n# Subset raster based on mask\npre_fire_rast &lt;- mask(pre_fire_rast, mask = pre_mask)\n\n# View raster\nplot(pre_fire_rast)\n\n\n\n\n\n\n\n\n\n# Set all cells with values greater than 0 to NA\npost_mask[post_mask &gt; 0] &lt;- NA\n\n# Subset raster based on mask\npost_fire_rast &lt;- mask(post_fire_rast, mask = post_mask)\n\n# View raster\nplot(post_fire_rast)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-true-color-composite",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-true-color-composite",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Plot True Color Composite",
    "text": "Plot True Color Composite\n\nplotRGB(pre_fire_rast, r = 4, g = 3, b = 2, stretch = \"lin\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\nplotRGB(post_fire_rast, r = 4, g = 3, b = 2, stretch = \"lin\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-false-color-composite",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#plot-false-color-composite",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Plot False Color Composite",
    "text": "Plot False Color Composite\n\nplotRGB(pre_fire_rast, r = 7, g = 5, b = 3, stretch = \"lin\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\nplotRGB(post_fire_rast, r = 7, g = 5, b = 3, stretch = \"lin\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-nbr-and-dnbr",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-nbr-and-dnbr",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Calculate NBR and dNBR",
    "text": "Calculate NBR and dNBR\n\npre_nbr_rast &lt;- terra::lapp(pre_fire_rast[[c(5, 7)]], fun = nbr_fun)\n\nplot(pre_nbr_rast, main = \"Cold Springs Pre-Fire NBR\", colNA = \"black\")\n\n\n\n\n\n\n\n\n\npost_nbr_rast &lt;- terra::lapp(post_fire_rast[[c(5, 7)]], fun = nbr_fun)\n\nplot(post_nbr_rast, main = \"Cold Springs Post-Fire NBR\", colNA = \"black\")"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-and-plot-dnbr",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#calculate-and-plot-dnbr",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Calculate and Plot dNBR",
    "text": "Calculate and Plot dNBR\n\ndiff_nbr &lt;- pre_nbr_rast - post_nbr_rast\n\ntm_shape(diff_nbr) +\n  tm_raster(style = \"equal\", n = 6, \n            palette = get_brewer_pal(\"YlOrRd\", n = 6, plot = FALSE),\n            title = \"Difference NBR (dNBR)\", colorNA = \"black\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#reclassification",
    "href": "course-materials/discussions/answer-keys/week6-discussion-answerKey.html#reclassification",
    "title": "Week 6: Discussion Section - Answer Key",
    "section": "Reclassification",
    "text": "Reclassification\n\n# Set categories for severity levels\ncategories &lt;- c(\"Enhanced Regrowth\", \"Unburned\", \"Low Severity\", \"Moderate Severity\", \"High Severity\")\n\n# Create reclassification matrix\nrcl &lt;- matrix(c(-Inf, -0.1, 1, # group 1 ranges for Enhanced Regrowth\n                -0.1, 0.1, 2, # group 2 ranges for Unburned\n                0.1, 0.27, 3, # group 3 ranges for Low Severity\n                0.27, 0.66, 4, # group 4 ranges for Moderity Severity\n                0.66, Inf, 5), # group 5 ranges for High Severity\n                ncol = 3, byrow = TRUE)\n\n# Use reclassification matrix to reclassify dNBR raster\nreclassified &lt;- classify(diff_nbr, rcl = rcl)\n\nreclassified[is.nan(reclassified)] &lt;- NA\n\n\ntm_shape(reclassified) +\n  tm_raster(style = \"cat\",\n            labels = c(categories, \"Missing\"),\n            palette = get_brewer_pal(\"YlOrRd\", n = 5, plot = FALSE),\n            title = \"Severity Level\", colorNA = \"black\")+\n  tm_layout(legend.outside = TRUE)"
  }
]