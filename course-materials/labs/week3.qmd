---
title: "Week 3: Lab"
subtitle: "EDS 223: Geospatial Analysis & Remote Sensing"
author: "Ruth Oliver"
date: last-modified
execute: 
  eval: true
format:
  html:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| eval: true
#| echo: false
#| out-width: "80%"
#| fig-align: "center"
knitr::include_graphics("images/horst-sf.png")
```

::: {.gray-text .center-text}
Artwork by [Allison Horst](https://allisonhorst.com/allison-horst){target="_blank"}
:::


In this lab, we'll explore the basics of spatial and geometry operations on vector data in R using the `sf` package.

::: {.callout-note icon=true}
# Source Materials
The following materials are modified from [Chapter 4](https://geocompr.robinlovelace.net/spatial-operations.html) and [Chapter 5](https://geocompr.robinlovelace.net/geometry-operations.html) of *Geocomputation with R* by Robin Lovelace.
:::

# Spatial data operations
[Last week](week2.qmd), we covered how the basics of the `sf` package. We saw that the magic of working with spatial data in the `tidyverse` means that we can leverage many of our favorite `dplyr` functions to manipulate the data attributes. So far, we've only worked with the `data.frame` component of `sf` objects. In this section, we'll see how we can perform analogous operations using the `geometry` column.

## 1. Set Up

Let's load all necessary packages:

```{r}
#| warning: false
#| message: false
library(sf)
library(tmap)
library(tidyverse)
library(spData)
```

## 2. Spatial subsetting (filtering)
When working with tabular data, we have frequently found it useful to subset the `data.frame` we are working with based on some condition using `dplyr::filter()`. 


```{r}
#| eval: true
#| echo: false
#| out-width: "80%"
#| fig-align: "center"
knitr::include_graphics("images/horst-dplyr-filter.png")
```

::: {.gray-text .center-text}
Artwork by [Allison Horst](https://allisonhorst.com/allison-horst){target="_blank"}
:::


For example, last week we saw how we could filter to countries whose average life expectancy is greater than 80 years old using the following code:

```{r}
#| eval: false
world %>%
  filter(lifeExp >= 80)
```

Similarly, we might want to filter data based on its *spatial* relationships. In this case, we use spatial subsetting which is the process of converting a spatial object into a new object containing only the spatial features that *relate* in space to another object. This is analogous the attribute subsetting that we covered last week (example above). 

### Topological relationships
When filtering based on attributes, we use conditions (for example, `lifeExp >= 80`). In spatial subsetting, we use the relationships of objects to each other in space (topological relationships). These relationships are based on mathematical relationships, but can be more easily understood from visualizing them. The figure below shows how each relationship is satisfied. 

```{r}
#| eval: true
#| echo: false
#| out-width: "80%"
#| fig-align: "center"
knitr::include_graphics("images/relations.png")
```

::: {.gray-text .center-text}
[*Geocomputation with R*](https://r.geocompx.org/spatial-operations#topological-relations){target="_blank"}
:::

:::{.callout-tip icon=true}
# `st_intersects()` and `st_disjoint()`

Note that `st_intersects()`is a "catch-all" that contains the following relationships:

- `st_touches()`
- `st_overlaps()`
- `st_contains()` and s`t_contains_properly()`
- `st_covers()` and `st_covered_by()`
- `st_within()`

`st_disjoint()` is the ***opposite*** of `st_intersects()`
:::


### Examples
There are many ways to spatially subset in R, so we will explore a few.

As an example we'll work with the following two datasets from the `spData` package:

- `nz`: polygons representing the [16 regions of New Zealand](https://en.wikipedia.org/wiki/Regions_of_New_Zealand)
- `nz_height`: top 101 heighest points in New Zealand

We'll explore by trying to find all the high points in the region of Canterbury (shown in dark grey). 
```{r}
#| echo: false

canterbury <- nz %>%
  filter(Name == "Canterbury")
```

```{r}
#| code-fold: true
tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) +
  tm_fill(col = "darkgrey") +
  tm_shape(nz_height) +
  tm_dots(col = "red")
```

#### Bracket subsetting
Like attribute subsetting, the command `x[y, ]` (equivalent to `nz_height[canterbury, ]`) subsets features of a target `x` using the contents of a source object `y`. Instead of `y` being a vector of class logical or integer, however, for spatial subsetting both `x` and `y` must be geographic objects. 
Specifically, objects used for spatial subsetting in this way must have the class `sf` or `sfc`: both `nz` and `nz_height` are geographic vector data frames and have the class `s`f, and the result of the operation returns another `sf` object representing the features in the target `nz_height` object that intersect with (in this case high points that are located within) the Canterbury region.

```{r}
# first filter to the region of Canterbury
canterbury <- nz %>%
  filter(Name == "Canterbury")

# subset nz_heights to just the features that intersect Canterbury
c_height1 <- nz_height[canterbury, ]

```

By default bracket subsetting will filter to features in `x` that intersect features in `y`. However, we can use other topological relationships by changing options.

```{r}
#| eval: false
nz_height[canterbury, , op = st_disjoint]
```


#### `st_filter()`
The `sf` package also includes the function `st_filter()` which is analogous to `dplyr::filter()`. Using `st_filter()` we can perform spatial subsetting in the same format as using `dplyr` commands. The `.predicate =` argument allows us to define which topological relationship we would like to filter by (e.g. `st_intersects()`, `st_disjoint()`).

The results from this method are the identical to the method above.

```{r}
# subset to the features in Cantebury
c_height2 <- nz_height %>%
  st_filter(y = canterbury, .predicate = st_intersects) # define the topological relationship

```

#### Topological operators (`st_intersects()`)

The previous two methods either by default or explicitly use the argument `st_intersects`. All topological relationships have their own topological operators which are functions that evaluate whether or not features meet the specified condition (e.g. `st_intersects()`). These operators can be used for spatial subsetting, but are more complicated to use.

The output of `st_intersects()` and other topological operators is a sparse geometry binary predicate list (yikes!) that's a list that defines whether or not each feature in `x` intersects `y`.

This can be converted into logical vector of `TRUE` and `FALSE` values which can then be used for filtering.

```{r}
# sparse binary predicate list
nz_height_sgbp <- st_intersects(x = nz_height, y = canterbury)
nz_height_sgbp

# convert to logical vector
nz_height_logical <- lengths(nz_height_sgbp) > 0

# filter based on logical vector
c_height3 = nz_height[nz_height_logical, ]

```

Now let's plot results from all three methods to confirm they gave the same results.
```{r}
#| code-fold: true

map1 <- tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) +
  tm_fill(col = "darkgrey") +
  tm_shape(c_height1) +
  tm_dots(col = "red") +
tm_layout(title = "Bracket subsetting")

map2 <- tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) +
  tm_fill(col = "darkgrey") +
  tm_shape(c_height1) +
  tm_dots(col = "red") +
tm_layout(title = "st_filter()")

map3 <- tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) +
  tm_fill(col = "darkgrey") +
  tm_shape(c_height3) +
  tm_dots(col = "red") +
tm_layout(title = "st_intersects()")

tmap_arrange(map1, map2, map3, nrow = 1)
```

### Distance relationships

The topological relationships we have been discussing are all binary (features either intersect or don't). In some cases, it might be helpful to subset based on a distance to a feature. In these cases we can use the `st_is_within_distance()` to filter features. By default `st_is_within_distance()` will return a sparse geometry binary predicate list as in `st_intersects()` above. Instead, we can return a logical by setting `sparse = FALSE`. 

```{r}
# find heights within 1000 km of Canterbury
nz_height_logical <- st_is_within_distance(nz_height, canterbury,
                      dist = units::set_units(1000, "km"), # set distance
                      sparse = FALSE) # return logical vector instead

c_height4 <- nz_height[nz_height_logical, ] # filter based on logical

```

Now, we should see points appear that do not *intersect* Caterbury, but are within 1000 km.
```{r}
#| code-fold: true
# additional high points should appear
tm_shape(nz) +
  tm_polygons() +
  tm_shape(canterbury) +
  tm_fill(col = "darkgrey") +
  tm_shape(c_height4) +
  tm_dots(col = "red")
```

## 3. Spatial joins
Joins are a common way to link different data sources. Up until now, we have been performing joins using common attributes between `data.frame`s. Last week, we saw that the same joins can be used on `sf` objects. However, we can also perform joins by using the spatial relationship of datasets.

First, let's remind ourselves of the different types of joins.

```{r}
#| eval: true
#| echo: false
#| out-width: "80%"
#| fig-align: "center"
knitr::include_graphics("images/dplyr-joins.png")
```

::: {.gray-text .center-text}
[Software Carpentry](https://data-lessons.github.io/gapminder-R/12-joins.html){target="_blank"}
:::

#### Toplogical relationships
With spatial data, we can join based on the `geometry` columns using topological relationships using the `st_join()` function. By default `st_join()` will join based on geometries that intersect, but can accommodate other topological relationships by changing the `join = ` argument. By default `st_join()` performs left joins, but can perform inner joins by setting `left = FALSE`.

```{r}
#| eval: false

# specify join based on geometries x within y
st_join(x, y, join = st_within)

# specify inner join
st_join(x, y, left = FALSE)
```


Let's consider the scenario where we would like to know which region each of the highest points is located in. We can left join the `nz` dataset (polygons of NZ's regions) onto the `nz_height` dataset (points of highest points in the county).

```{r}
nz_height_left_join <- st_join(nz_height, nz) %>%
  select(id = t50_fid, elevation, region = Name)

head(nz_height_left_join)
```

Now we could use this data to summarize the number of highest point in each region!

```{r}
nz_height_left_join %>%
  group_by(region) %>%
  summarise(n_points = n()) %>%
  st_drop_geometry()
```

#### Distance-based joins
Similar to filtering, in some cases we may want to join datasets based on their proximity. Let's see an example!

We'll use the following two datasets from the `spData` package:

- `cycle_hire`: points representing cycle hire points across London with information on number of bikes available
- `cycle_hire_osm`: dataset downloaded from OpenStreetMaps representing cycle hire points across London with information on the capacity of the hire point

In this example, we would like join the `capacity` attribute from the `cycle_hire_osm` dataset to the `cycle_hire` dataset. Unfortunately it appears that the points from the two datasets do not perfectly align.

```{r}
# check whether or not points overlap
if(any(st_intersects(cycle_hire, cycle_hire_osm, sparse = FALSE)) == TRUE){
  print("points overlap")
} else{
  warning("points don't overlap")
}
```

```{r}
#| message: false
#| code-fold: true

tmap_mode("view")

tm_shape(cycle_hire) +
  tm_symbols(col = "red", alpha = 0.2) +
tm_shape(cycle_hire_osm) +
  tm_symbols(col = "blue", alpha = 0.2)
```

We can join by again using `st_join()`, but this time including a distance threshold using `st_is_within_distance`.

```{r}
cycle_hire_join <- st_join(cycle_hire, cycle_hire_osm,
                           st_is_within_distance,
                           dist = units::set_units(20, "m")) %>%
                   select(id, capacity)

head(cycle_hire_join)
```

Let's build some checks to diagnose the output.

```{r}
if(nrow(cycle_hire) == nrow(cycle_hire_join)){
  print("join matches original data dimensions")
} else {
  warning("join does not match orginal data dimensions")
  print(paste("cycle_hire has", nrow(cycle_hire), "rows"))
  print(paste("cycle_hire_join has", nrow(cycle_hire_join), "rows"))
}
```

Note that the joined result has more rows than the target data. This is because some of the cycle hire stations in `cycle_hire` have multiple matches in `cycle_hire_osm`. Depending on your project, you would need to think about how to resolve this. In this case, we can aggregate the values for the overlapping points by taking the mean.

```{r}
# aggregate values for single points in cycle_hire
cycle_hire_join <- cycle_hire_join %>%
  group_by(id) %>%
  summarise(capacity = mean(capacity))

# check results
if(nrow(cycle_hire) == nrow(cycle_hire_join)){
  print("join matches original data dimensions")
} else {
  warning("join does not match orginal data dimensions")
  print(paste("cycle_hire has", nrow(cycle_hire), "rows"))
  print(paste("cycle_hire_join has", nrow(cycle_hire_join), "rows"))
}
```

### Spatial aggregation

As with aggregating attribute data, spatial data aggregation *condenses* data: outputs have few rows than inputs. Think about our friend `group_by() %>% summarise()`! Spatial aggregation is the same. 

Let's consider the example where we would like to make a map of the mean elevation of high points within each region of NZ. There are several ways to do this, but the first thing we should be thinking is that we will need to retain the `geometry` column of the NZ regions in order to make a map.

The first approach is by leveraging `st_join()` again. But in this example, we want the `nz` object to be the target to maintain the geometries we need.

```{r}
tmap_mode("plot")
nz_elevation <- st_join(x = nz, y = nz_height) %>%
  group_by(Name) %>%
  summarise(elevation = mean(elevation, na.rm = TRUE))
```

The second approah uses the `aggregate()` function. Although it doesn't follow the `dplyr` piping convention we're used to, `aggregate()` will come in handy later, so it's nice to see how it works.

The syntax looks slightly different, in this case the argument `x` is the data we would like to aggregate (`nz_height`) and the `by` argument specifies the geometry that you would like to group by. The `FUN` argument defines the function that you would like to use to aggregate, in this case `mean`.

```{r}
nz_elevation <- aggregate(x = nz_height, by = nz, FUN = mean)
```


```{r}
#| code-fold: true

map1 <- tm_shape(nz_elevation) +
  tm_polygons(col = "elevation",
              title = "Mean elevation (meters)") +
  tm_layout(title = "group_by()")

map2 <- tm_shape(nz_elevation) +
  tm_polygons(col = "elevation",
              title = "Mean elevation (meters)") +
  tm_layout(title = "aggregate()")

tmap_arrange(map1, map2, nrow = 1)

```

:::{.callout-note icon=true}
# Joining incongruent layers

An important consideration when aggregating spatial objects is that geometries are *congruent*, meaning aggregating zones align with the units being aggregating. This is often the case with administrative boundaries where units are sub-units of one another (e.g. countries > states > counties). 

However, in some cases the aggregating zones do not share common borders with the target. This is an issue because it's not clear how to aggregate underlying data. Areal interpolation overcomes this issue by transferring values to another using algorithms, including simple area weighted approaches.


```{r}
incongruent <- spData::incongruent
aggregating_zones <- spData::aggregating_zones

tm_shape(incongruent) +
  tm_polygons(col = "lightblue",
              border.col = "blue") +
  tm_shape(aggregating_zones) +
  tm_borders(col = "red")
  
```

The simplest useful method for this is area weighted spatial interpolation, which transfers values from the `incongruent` object to a new column in `aggregating_zones` in proportion with the area of overlap: the larger the spatial intersection between input and output features, the larger the corresponding value. This is implemented in `st_interpolate_aw()`, as demonstrated in the code chunk below.

```{r}
# select just the value to be aggregated
incongruent_2 <- incongruent %>%
  select(value)

# use area-weighted interpolation to aggregrate the "value" attribute
aggregating_zones_area_weighted <- st_interpolate_aw(incongruent_2, aggregating_zones, extensive = TRUE)

aggregating_zones_area_weighted$value
```

:::

# Geometry operations

## 1. Geometry unions (aggregating)

```{r}
#| message: false
us_states <- spData::us_states

regions1 <- us_states %>%
  group_by(REGION) %>%
  summarise(population = sum(total_pop_15, na.rm = TRUE))

regions2 <- aggregate(x = us_states[, "total_pop_15"], by = list(us_states$REGION),
                      FUN = sum, na.rm = TRUE)
```

```{r}
#| code-fold: true
#| message: false
#| warning: false

map1 <- tm_shape(us_states) +
  tm_polygons(col = "total_pop_15",
              title = "Total population") +
  tm_layout(title = "US States")

map2 <- tm_shape(regions1) +
  tm_polygons(col = "population",
              title = "Total population") +
  tm_layout(title = "group_by()")

map3 <- tm_shape(regions2) +
  tm_polygons(col = "total_pop_15",
              title = "Total population") +
  tm_layout(title = "aggregate()")

tmap_arrange(map1, map2, map3, nrow = 1)
```

What's going on behind the scenes...

```{r}
us_west = us_states[us_states$REGION == "West", ]
us_west_union = st_union(us_west)

texas = us_states[us_states$NAME == "Texas", ]
texas_union = st_union(us_west_union, texas)
```


## 2. Filtering

### Buffers

```{r}
seine_buffer_5km <- st_buffer(seine, dist = 5000)
seine_buffer_50km = st_buffer(seine, dist = 50000)
```

```{r}
#| code-fold: true

map1 <- tm_shape(seine_buffer_5km) +
  tm_polygons() +
  tm_shape(seine) +
  tm_lines() +
  tm_layout(title = "5km buffer")

map2 <- tm_shape(seine_buffer_50km) +
  tm_polygons() +
  tm_shape(seine) +
  tm_lines() +
  tm_layout(title = "50km buffer")

tmap_arrange(map1, map2, nrow = 1)
```


```{r}
seine_union <- st_union(seine_buffer_50km)
```


```{r}
#| code-fold: true

tm_shape(seine_union) +
  tm_polygons() +
  tm_shape(seine) +
  tm_lines() +
  tm_layout(title = "50km buffer")
```


```{r}
nz_height_buffer <- st_buffer(nz_height, dist = 1000000)

c_height5 <- nz_height_buffer %>%
  st_filter(y = canterbury, .predicate = st_intersects)

if(nrow(c_height4) == nrow(c_height5)){
  print("results from buffer approach match st_is_within_distance() approach")
} else{
  warning("approaches giving different results")
}
```

### Clipping


```{r}
x <- st_sfc(st_point(c(0, 1))) %>%
  st_buffer(., dist = 1) %>%
  st_as_sf()

y <- st_sfc(st_point(c(1, 1))) %>%
  st_buffer(., dist = 1) %>%
  st_as_sf()

intersection <- st_intersection(x, y)
difference_x_y <- st_difference(x, y)
difference_y_x <- st_difference(y, x)
union <- st_union(x, y)
sym_difference <- st_sym_difference(x, y)
```


```{r}
#| warning: false
#| code-fold: true

bbox <- st_union(x, y)

map1 <- tm_shape(x, bbox = bbox) +
  tm_borders(col = "red") +
  tm_shape(y) +
  tm_borders(col = "blue")

map2 <- map1 +
  tm_shape(intersection, bbox = bbox) +
  tm_fill(col = "purple") +
  tm_layout(title = "st_intersection()")

map3 <- map1 +
  tm_shape(difference_x_y, bbox = bbox) +
  tm_fill(col = "purple") +
  tm_layout(title = "st_difference(x,y)")

map4 <- map1 +
  tm_shape(difference_y_x, bbox = bbox) +
  tm_fill(col = "purple") +
  tm_layout(title = "st_difference(y,x)")

map5 <- map1 +
  tm_shape(union, bbox = bbox) +
  tm_fill(col = "purple") +
  tm_layout(title = "st_union()")

map6 <- map1 +
  tm_shape(sym_difference, bbox = bbox) +
  tm_fill(col = "purple") +
  tm_layout(title = "st_sym_diffference()")

tmap_arrange(map1, map2, map3, map4, map5, map6, nrow = 2)
```

```{r}
bb <- st_bbox(st_union(x, y))
box <- st_as_sfc(bb)
p <- st_sample(x = box, size = 100) %>%
  st_as_sf()

x_and_y = st_intersection(x, y)

# way #1
p_xy1 = p[x_and_y, ]
# way #2
p_xy2 = st_intersection(p, x_and_y)

p_xy3 <- p %>%
  st_filter(., x_and_y)
```

```{r}
#| warning: false
#| code-fold: true
map2 <- map1 +
  tm_shape(p) +
  tm_symbols(alpha = 0.5) +
  tm_layout(title = "original")

map3 <- map2 +
  tm_shape(p_xy1) +
  tm_symbols(col = "purple") +
  tm_layout(title = "method 1")

map4 <- map2 +
  tm_shape(p_xy2) +
  tm_symbols(col = "purple") +
  tm_layout(title = "method 2")

map5 <- map2 +
  tm_shape(p_xy3) +
  tm_symbols(col = "purple") +
  tm_layout(title = "method 3")

tmap_arrange(map2, map3, map4, map5, nrow = 2)

```

```{r}


```


## 3. Making life easier!

### Centroids

```{r}
#| warning: false

nz_centroid <- st_centroid(nz)
seine_centroid <- st_centroid(seine)

nz_pos <- st_point_on_surface(nz)
seine_pos <- st_point_on_surface(seine)
```

```{r}
#| code-fold: true

map1 <- tm_shape(nz) +
  tm_polygons() +
  tm_shape(nz_centroid) +
  tm_symbols(col = "red", alpha = 0.5) +
  tm_shape(nz_pos) +
  tm_symbols(col = "blue", alpha = 0.5)

map2 <- tm_shape(seine) +
  tm_lines() +
  tm_shape(seine_centroid) +
  tm_symbols(col = "red", alpha = 0.5) +
  tm_shape(seine_pos) +
  tm_symbols(col = "blue", alpha = 0.5)

tmap_arrange(map1, map2, nrow = 1)
```

### Simplification

```{r}
seine_simple <- st_simplify(seine, dTolerance = 2000)  # 2000 m
```

```{r}
#| code-fold: true

map1 <- tm_shape(seine) +
  tm_lines() +
  tm_layout("original")

map2 <- tm_shape(seine_simple) +
  tm_lines() +
  tm_layout("st_simplify()")

tmap_arrange(map1, map2, nrow = 1)
```

